{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_q():\n",
    "    q_values = {}\n",
    "    for s in range(0, env.observation_space.n):\n",
    "        for a in range(0, env.action_space.n):\n",
    "            q_values[(s, a)] = 0\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode_mc_control(q_values, epsilon):\n",
    "    # current state we are in\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    r_s = [] # rewards after each step of episode\n",
    "    s_a = [] # state|action pairs of episode\n",
    "    # apply policy\n",
    "    while not done:\n",
    "        action = choose_action(q_values, state, epsilon)\n",
    "\n",
    "        s_a.append((state, action))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        r_s.append(reward)\n",
    "    return s_a, r_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update Q-values with TD prediction after every step\n",
    "# TODO update policy after every step\n",
    "def play_episode_SARSA(q_values, epsilon):\n",
    "    # current state we are in\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    r_s = [] # rewards after each step of episode\n",
    "    s_a = [] # state|action pairs of episode\n",
    "    \n",
    "    # choose first action\n",
    "    action = choose_action(q_values, state, epsilon)\n",
    "    \n",
    "    while not done:\n",
    "        s_a.append((state, action))\n",
    "        state_bar, reward, done, _ = env.step(action)\n",
    "        r_s.append(reward)\n",
    "        \n",
    "        # get A' for S' to calculate q_values\n",
    "        action_bar = choose_action(q_values, state_bar, epsilon)\n",
    "        \n",
    "        # update q-values\n",
    "        q_values[(state, action)] += 0.3 * (reward + q_values[(state_bar, action_bar)] - q_values[(state, action)])\n",
    "        \n",
    "        state = state_bar\n",
    "        action = action_bar\n",
    "        \n",
    "    return s_a, r_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO update q-values with max(Q(S',A)) instead of Q(S',A')\n",
    "def play_episode_QLearning(q_values, epsilon):\n",
    "    # current state we are in\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    r_s = [] # rewards after each step of episode\n",
    "    s_a = [] # state|action pairs of episode\n",
    "    \n",
    "    # choose first action\n",
    "    action = choose_action(q_values, state, epsilon)\n",
    "    \n",
    "    while not done:\n",
    "        s_a.append((state, action))\n",
    "        state_bar, reward, done, _ = env.step(action)\n",
    "        r_s.append(reward)\n",
    "        \n",
    "        # get A' for S' to calculate q_values\n",
    "        action_bar = choose_action(q_values, state_bar, epsilon)\n",
    "        \n",
    "        # update q-values\n",
    "        q_values[(state, action)] += 0.3 * (reward + max([q_values[(state_bar, a)] for a in range(0, env.action_space.n)]) - q_values[(state, action)])\n",
    "        \n",
    "        state = state_bar\n",
    "        action = action_bar\n",
    "        \n",
    "    return s_a, r_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick highest q value action or if you slip, pick random action\n",
    "def choose_action(q_values, state, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        relevant_qs = [q_values[(state, a)] for a in range(0, env.action_space.n)]\n",
    "        # there can be more than one best action\n",
    "        best_actions_indexes = [i for i, v in enumerate(relevant_qs) if v == max(relevant_qs)]\n",
    "        # in this case randomly choose one of them\n",
    "        return random.choice(best_actions_indexes)\n",
    "    else:\n",
    "        return random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_executions = 100\n",
    "no_episodes = 1000\n",
    "\n",
    "epsilons = [0.01, 0.1, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC Control\n",
    "plot_data_mc_control = []\n",
    "for e in epsilons:\n",
    "    rewards = np.zeros(no_executions * no_episodes)\n",
    "    rewards = rewards.reshape(no_executions, no_episodes)\n",
    "    for j in range(0, no_executions):\n",
    "        q_values = init_q()\n",
    "        for i in range(0, no_episodes):\n",
    "            s, r = play_episode_mc_control(q_values, epsilon=e)\n",
    "            rewards[j][i] = sum(r)\n",
    "\n",
    "            # update q-values\n",
    "            for i2, q in enumerate(s):\n",
    "                return_i = sum(r[i2:]) # empirical return for episode\n",
    "\n",
    "                # running mean with alpha = 0.3 and no discount factor\n",
    "                q_values[q] += 0.3 * (return_i - q_values[q])\n",
    "\n",
    "    rewards = rewards.mean(axis=0)\n",
    "    plot_data_mc_control.append(np.cumsum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_SARSA = []\n",
    "for e in epsilons:\n",
    "    rewards = np.zeros(no_executions * no_episodes)\n",
    "    rewards = rewards.reshape(no_executions, no_episodes)\n",
    "    for j in range(0, no_executions):\n",
    "        q_values = init_q()\n",
    "        for i in range(0, no_episodes):\n",
    "            s, r = play_episode_SARSA(q_values, epsilon=e)\n",
    "            rewards[j][i] = sum(r)\n",
    "\n",
    "    rewards = rewards.mean(axis=0)\n",
    "    plot_data_SARSA.append(np.cumsum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_QLearning = []\n",
    "for e in epsilons:\n",
    "    rewards = np.zeros(no_executions * no_episodes)\n",
    "    rewards = rewards.reshape(no_executions, no_episodes)\n",
    "    for j in range(0, no_executions):\n",
    "        q_values = init_q()\n",
    "        for i in range(0, no_episodes):\n",
    "            s, r = play_episode_QLearning(q_values, epsilon=e)\n",
    "            rewards[j][i] = sum(r)\n",
    "\n",
    "    rewards = rewards.mean(axis=0)\n",
    "    plot_data_QLearning.append(np.cumsum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epsilons = [0.01, 0.1, 0.1]\n",
    "algorithm = [\"MC_Control\", \"SARSA\", \"QLearning\"]\n",
    "plot_data = [plot_data_mc_control[0], plot_data_SARSA[1], plot_data_QLearning[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"No. of episodes\")\n",
    "plt.ylabel(\"Sum of rewards\")\n",
    "for i, eps in enumerate(best_epsilons):\n",
    "    plt.plot(range(0, no_episodes), plot_data[i], label=algorithm[i] + \" e=\" + str(eps))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
