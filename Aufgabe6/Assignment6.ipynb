{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+KJs/b5SZzMEt6hjJtfG+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerNiccoo/TheEarlyBird/blob/main/Aufgabe6/Assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjmdE7okirHG",
        "outputId": "b51dd782-e8e5-4b6d-b8bd-912be9bb6b9f"
      },
      "source": [
        "!pip3 install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 7.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqtnoqVwkaJ1"
      },
      "source": [
        "import gym\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "from collections import namedtuple, deque\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# pytorch imports\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky4wLgkgkmYo"
      },
      "source": [
        "## Initialisieren der Umgebung\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVaBp1kskhJ8",
        "outputId": "b5386f79-ce0e-4a10-ad2a-d275db9d408b"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\r\n",
        "env.seed(0)\r\n",
        "print('State shape: ', env.observation_space.shape)\r\n",
        "print('Number of Actions: ', env.action_space.n)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State shape:  (8,)\n",
            "Number of Actions:  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1o4Fg1qkUN1"
      },
      "source": [
        "1. Define a neural network with two fully connected-layers. The hidden layer uses a\r\n",
        "Relu activation. The output layer uses a softmax. Try different hidden layer sizes\r\n",
        "(between 100 and 500). The network takes as input a vector of the current states\r\n",
        "and gives out a probability for each action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF6CM3E8qy30"
      },
      "source": [
        "class TwoLayerNet(torch.nn.Module):\r\n",
        "    def __init__(self, state_size, hidden_layer, action_size):\r\n",
        "        super(TwoLayerNet, self).__init__()\r\n",
        "        self.linear1 = torch.nn.Linear(state_size, hidden_layer)\r\n",
        "        self.linear2 = torch.nn.Linear(hidden_layer, action_size)\r\n",
        "\r\n",
        "    def forward(self, state):\r\n",
        "        #Transformiere state (numpay array) zu Torch Tensor, da TwoLayerNet Tensor als Input erwartet\r\n",
        "        state = torch.from_numpy(state)\r\n",
        "        h_relu = F.relu(self.linear1(state))\r\n",
        "        action_pred = F.softmax(self.linear2(h_relu))\r\n",
        "        return action_pred"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzNcy7k5m-3y"
      },
      "source": [
        "2. Generate 100 episodes by sampling actions using the network output. Limit the\r\n",
        "number of steps per episode to 500. Sum up the reward of all steps of one episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR6yPXnlm9LJ"
      },
      "source": [
        "def play_episodes(max_steps, model): \r\n",
        "\r\n",
        "    t = 0\r\n",
        "    done = False\r\n",
        "    action_space = [0, 1, 2, 3]\r\n",
        "\r\n",
        "    #State-Action-Pairs pro Zeitschritt einer Episode\r\n",
        "    state_action_per_timestep = []\r\n",
        "    #Reward pro Zeitschritt einer Episode\r\n",
        "    reward_per_timestep = []\r\n",
        "     \r\n",
        "    state = env.reset() #State ist ein 1-D Numpy Array mit 8 Einträgen \r\n",
        "\r\n",
        "    #Erstellung einer Episode\r\n",
        "    while t < max_steps: \r\n",
        "\r\n",
        "      #print(\"STATE:::::::: \", state)\r\n",
        "      #print(\"STATE TYPE + SHAPE ::::::::::::\", state.shape, type(state) )\r\n",
        "        \r\n",
        "      action_output = model(state)\r\n",
        "      #print(\"ACTIONS OUTPUT::::::::::::::\", action_output)\r\n",
        "      sampled_action = random.choices(action_space, action_output, k=1)\r\n",
        "      #print(\"CHOOSEN ACTION:::::::::.\", sampled_action[0])\r\n",
        "\r\n",
        "      state_action_per_timestep.append((state, sampled_action[0]))\r\n",
        "\r\n",
        "      next_state, reward, done, _ = env.step(sampled_action[0])\r\n",
        "      reward_per_timestep.append(reward)\r\n",
        "\r\n",
        "      state = next_state\r\n",
        "      t += 1\r\n",
        "     \r\n",
        "      if done: \r\n",
        "        break\r\n",
        "    \r\n",
        "    return state_action_per_timestep, reward_per_timestep, t"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFureXbU3fN7",
        "outputId": "5aa425bb-ea98-4192-9aeb-86417eb23ea8"
      },
      "source": [
        "episode_n = 100\r\n",
        "max_steps = 500\r\n",
        "\r\n",
        "#Rewards aller n Episoden\r\n",
        "rewards_per_episode = []\r\n",
        "#Durchschnittlicher Reward einer Episode\r\n",
        "mean_rewards_per_episode = []\r\n",
        "\r\n",
        "state_size, action_size, hidden_layer = 8, 64, 4\r\n",
        "model = TwoLayerNet(state_size, action_size, hidden_layer)\r\n",
        "\r\n",
        "for i in range(0, episode_n):\r\n",
        "  state_actions, reward, episode_length = play_episodes(max_steps, model)\r\n",
        " \r\n",
        "  #reward_per_episode = Liste mit allen summierten Rewards von 100 Episoden \r\n",
        "  rewards_per_episode.append((sum(reward), state_actions))\r\n",
        "  mean_rewards_per_episode.append((sum(reward)/episode_length, state_actions))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EGv8n92D-WD"
      },
      "source": [
        "3. Print out the mean reward per episode of the 100 episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlnhEoNTEBiq"
      },
      "source": [
        "#mean_rewards_per_episode"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSwfAJp-HOkR"
      },
      "source": [
        "4. Identify the 20 best of those episodes in terms of reward and use the\r\n",
        "<state, action> pairs of these episodes as training examples for the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tv-nfzgKPWY"
      },
      "source": [
        "#Sortiere Rewards absteigend\r\n",
        "reward_sorted = sorted(rewards_per_episode)\r\n",
        "mean_rewards_sorted = sorted(mean_rewards_per_episode)\r\n",
        "\r\n",
        "#Schneide die besten 20 ab und extrahiere die State,actions pairs dieser Episoden \r\n",
        "top20 = reward_sorted[:20]\r\n",
        "training_pairs = [x[1] for x in top20]\r\n",
        "\r\n",
        "#print(training_pairs)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8GY0ZOUsFT5",
        "outputId": "137ccbbd-e995-4a54-b90a-7f611fed486f"
      },
      "source": [
        "\r\n",
        "\r\n",
        "for pair in training_pairs:\r\n",
        "  print(pair[0][0])\r\n",
        "  #print(pair[1])\r\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.00491934  1.4167392   0.4982709   0.25862202 -0.0056936  -0.11286581\n",
            "  0.          0.        ]\n",
            "[-0.00466557  1.4025742  -0.47258395 -0.37093422  0.005413    0.1070473\n",
            "  0.          0.        ]\n",
            "[ 0.00260124  1.4138703   0.26345855  0.13112561 -0.00300736 -0.05967727\n",
            "  0.          0.        ]\n",
            "[-0.00428696  1.3993648  -0.434241   -0.5135697   0.00497432  0.09836202\n",
            "  0.          0.        ]\n",
            "[ 8.0671313e-04  1.4174215e+00  8.1699163e-02  2.8894600e-01\n",
            " -9.2802680e-04 -1.8506095e-02  0.0000000e+00  0.0000000e+00]\n",
            "[ 0.00639868  1.4196268   0.64809746  0.38694793 -0.00740764 -0.14680363\n",
            "  0.          0.        ]\n",
            "[-0.00732298  1.4101491  -0.7417462  -0.03429733  0.00849221  0.16801645\n",
            "  0.          0.        ]\n",
            "[ 0.00447531  1.410691    0.45327792 -0.01018432 -0.00517889 -0.10267415\n",
            "  0.          0.        ]\n",
            "[ 1.3033866e-03  1.4203258e+00  1.3199742e-01  4.1802695e-01\n",
            " -1.5034557e-03 -2.9899385e-02  0.0000000e+00  0.0000000e+00]\n",
            "[ 0.0043931   1.4187808   0.44496006  0.3493579  -0.00508375 -0.10079016\n",
            "  0.          0.        ]\n",
            "[-0.00316534  1.4164848  -0.3206212   0.24732192  0.00367453  0.07262553\n",
            "  0.          0.        ]\n",
            "[ 0.00786753  1.4135568   0.79688084  0.11716431 -0.00910973 -0.18050547\n",
            "  0.          0.        ]\n",
            "[ 0.00783882  1.4127247   0.79396087  0.08018272 -0.00907633 -0.17984387\n",
            "  0.          0.        ]\n",
            "[ 0.00345478  1.4120507   0.3499194   0.05024264 -0.00399647 -0.07926197\n",
            "  0.          0.        ]\n",
            "[-0.00696955  1.4210149  -0.7059692   0.44863275  0.00808291  0.15991262\n",
            "  0.          0.        ]\n",
            "[-0.00252447  1.4194614  -0.25570562  0.37961045  0.0029319   0.05792117\n",
            "  0.          0.        ]\n",
            "[ 0.00602512  1.418225    0.6102644   0.32465765 -0.00697483 -0.13823403\n",
            "  0.          0.        ]\n",
            "[ 0.0056283   1.4071182   0.5700717  -0.16898698 -0.00651501 -0.12912974\n",
            "  0.          0.        ]\n",
            "[ 0.00751457  1.404685    0.7611199  -0.27714646 -0.00870062 -0.17240508\n",
            "  0.          0.        ]\n",
            "[ 0.00765076  1.4068048   0.7749286  -0.18293089 -0.00885859 -0.17553289\n",
            "  0.          0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzh8yrAkq5Sf"
      },
      "source": [
        "def train(training_set, model):\r\n",
        "  \r\n",
        "  model.train()\r\n",
        "  for pair in training_set:\r\n",
        "\r\n",
        "    state = pair[0][0]\r\n",
        "    target_action = pair[0][1]\r\n",
        "\r\n",
        "    target = torch.zeros(1, dtype=torch.long)\r\n",
        "    target[0] = target_action\r\n",
        "\r\n",
        "    output_action = model(state).unsqueeze(0)\r\n",
        "    #print(f\"TARGET ACTIOM {target.shape}\")\r\n",
        "    #print(f\"TARGET ACTIOM {output_action.shape}\")\r\n",
        "\r\n",
        "    loss = F.nll_loss(output_action, target)\r\n",
        "    loss.backward()"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT44rMkVxkAC",
        "outputId": "f8cb8085-53e4-4003-90b4-969a2b846576"
      },
      "source": [
        "train(training_pairs, model)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2387, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2692, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2697, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2258, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2285, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2546, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2579, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2454, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2773, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2736, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2699, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2321, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2602, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2849, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2475, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2110, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2546, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2870, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2894, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2673, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cGNTFJRGI3z9",
        "outputId": "e83ed001-74f8-47cb-be64-1c43cc840074"
      },
      "source": [
        "mean_reward = 0 # as Learning Indicator\r\n",
        "episode_n = 100\r\n",
        "max_steps = 500\r\n",
        "\r\n",
        "#Definiere das Neuronale Netz mit einer Hidden_Layer\r\n",
        "state_size, hidden_layer, action_size = 8, 64, 4\r\n",
        "model = TwoLayerNet(state_size, hidden_layer, action_size)\r\n",
        "\r\n",
        "while mean_reward < 100: \r\n",
        "\r\n",
        "  #2: Generate 100 episodes by sampling actions using the network output\r\n",
        "  \r\n",
        "  #Rewards aller n Episoden\r\n",
        "  rewards_per_episode = []\r\n",
        "  #Durchschnittlicher Reward einer Episode\r\n",
        "  mean_rewards_per_episode = []\r\n",
        "\r\n",
        "  for i in range(0, episode_n):\r\n",
        "    state_actions, reward, episode_length = play_episodes(max_steps, model)\r\n",
        "  \r\n",
        "    mean_reward = sum(reward) / episode_length\r\n",
        "    sum_reward = sum(reward) \r\n",
        "    \r\n",
        "    #reward_per_episode = Liste mit allen summierten Rewards von 100 Episoden\r\n",
        "    rewards_per_episode.append((sum_reward, state_actions))\r\n",
        "    mean_rewards_per_episode.append((mean_reward, state_actions))\r\n",
        "\r\n",
        "    #3: Print out the mean reward per episode of the 100 episodes.\r\n",
        "    print(f\"Mean Reward in Episode {i}: {mean_reward}\")\r\n",
        "  \r\n",
        "  #4: Identify the 20 best of those episodes in terms of reward and use the <state, action> pairs of these episodes as training examples for the network.\r\n",
        "  #Sortiere Rewards absteigend\r\n",
        "  reward_sorted = sorted(rewards_per_episode)\r\n",
        "  \r\n",
        "  #Schneide die besten 20 ab und extrahiere die State,actions pairs dieser Episoden \r\n",
        "  top20 = reward_sorted[:20]\r\n",
        "  training_pairs = [x[1] for x in top20]\r\n",
        "\r\n",
        "  train(training_pairs, model)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mDie letzten 5000 Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2389, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3148, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -2.9255457988371765\n",
            "Mean Reward in Episode 1: -4.844163002086325\n",
            "Mean Reward in Episode 2: -1.3902082939581077\n",
            "Mean Reward in Episode 3: -0.8695420878226007\n",
            "Mean Reward in Episode 4: -1.0608833045172517\n",
            "Mean Reward in Episode 5: -1.440458687846641\n",
            "Mean Reward in Episode 6: -3.672073022156297\n",
            "Mean Reward in Episode 7: -0.9702208767381253\n",
            "Mean Reward in Episode 8: -1.1984866120120583\n",
            "Mean Reward in Episode 9: -2.8212644372224873\n",
            "Mean Reward in Episode 10: -1.6390654641815396\n",
            "Mean Reward in Episode 11: -1.0622972945017666\n",
            "Mean Reward in Episode 12: -0.07888719819355092\n",
            "Mean Reward in Episode 13: -2.2478755775260857\n",
            "Mean Reward in Episode 14: -3.6223556758836786\n",
            "Mean Reward in Episode 15: -1.3674643934859754\n",
            "Mean Reward in Episode 16: -4.250819476180111\n",
            "Mean Reward in Episode 17: -2.9674588188234305\n",
            "Mean Reward in Episode 18: -3.5343743165040484\n",
            "Mean Reward in Episode 19: -1.9300817672077808\n",
            "Mean Reward in Episode 20: -1.2720600421093744\n",
            "Mean Reward in Episode 21: -2.1445959178927647\n",
            "Mean Reward in Episode 22: -1.1282895153488277\n",
            "Mean Reward in Episode 23: -0.5929722224263563\n",
            "Mean Reward in Episode 24: -1.671682047029232\n",
            "Mean Reward in Episode 25: -2.119733234831762\n",
            "Mean Reward in Episode 26: -3.6293028456649754\n",
            "Mean Reward in Episode 27: -1.8942370353510418\n",
            "Mean Reward in Episode 28: -0.7887365234854385\n",
            "Mean Reward in Episode 29: -2.0085712887755354\n",
            "Mean Reward in Episode 30: -3.9551485065915477\n",
            "Mean Reward in Episode 31: -1.7671112134335927\n",
            "Mean Reward in Episode 32: -1.858341849687329\n",
            "Mean Reward in Episode 33: -1.5148294938430742\n",
            "Mean Reward in Episode 34: -1.369419344349253\n",
            "Mean Reward in Episode 35: -0.9070084265777832\n",
            "Mean Reward in Episode 36: -2.408641912970522\n",
            "Mean Reward in Episode 37: -2.6383920736551407\n",
            "Mean Reward in Episode 38: -1.531762440138341\n",
            "Mean Reward in Episode 39: -1.0433920743915384\n",
            "Mean Reward in Episode 40: -1.3540216925199664\n",
            "Mean Reward in Episode 41: -1.7317217713120252\n",
            "Mean Reward in Episode 42: -1.5076911054404143\n",
            "Mean Reward in Episode 43: -1.113393696561566\n",
            "Mean Reward in Episode 44: -2.187173279896475\n",
            "Mean Reward in Episode 45: -3.3501037143352583\n",
            "Mean Reward in Episode 46: -0.0031683232494231104\n",
            "Mean Reward in Episode 47: 0.10637386112327063\n",
            "Mean Reward in Episode 48: -4.91205919892075\n",
            "Mean Reward in Episode 49: -1.0172188215598843\n",
            "Mean Reward in Episode 50: -2.287804919440874\n",
            "Mean Reward in Episode 51: -0.16236574694672506\n",
            "Mean Reward in Episode 52: -1.3017261437288803\n",
            "Mean Reward in Episode 53: -0.9795158659240394\n",
            "Mean Reward in Episode 54: -1.3074992169428827\n",
            "Mean Reward in Episode 55: -2.022823727587308\n",
            "Mean Reward in Episode 56: -2.6228114109944847\n",
            "Mean Reward in Episode 57: -3.797870192530128\n",
            "Mean Reward in Episode 58: -2.3750605586584252\n",
            "Mean Reward in Episode 59: -4.63841669382577\n",
            "Mean Reward in Episode 60: -2.0423515226487323\n",
            "Mean Reward in Episode 61: -3.2664124344362584\n",
            "Mean Reward in Episode 62: -2.8247205359382606\n",
            "Mean Reward in Episode 63: -1.4512577127245092\n",
            "Mean Reward in Episode 64: -3.0046581763073696\n",
            "Mean Reward in Episode 65: -2.1964201176441907\n",
            "Mean Reward in Episode 66: -2.606312864542119\n",
            "Mean Reward in Episode 67: -2.6840335056147375\n",
            "Mean Reward in Episode 68: -0.8897855660778357\n",
            "Mean Reward in Episode 69: -1.8491653428658776\n",
            "Mean Reward in Episode 70: -2.8412523764983617\n",
            "Mean Reward in Episode 71: -3.311667853489659\n",
            "Mean Reward in Episode 72: -1.688238549326245\n",
            "Mean Reward in Episode 73: -1.3680027800349\n",
            "Mean Reward in Episode 74: -1.403989488641643\n",
            "Mean Reward in Episode 75: -1.1769694166874456\n",
            "Mean Reward in Episode 76: -1.1238573299209742\n",
            "Mean Reward in Episode 77: -1.0080385917790218\n",
            "Mean Reward in Episode 78: -3.9040916897746634\n",
            "Mean Reward in Episode 79: -1.2215428095375402\n",
            "Mean Reward in Episode 80: -1.0465419928206607\n",
            "Mean Reward in Episode 81: -3.817999721173919\n",
            "Mean Reward in Episode 82: -1.1266031102992162\n",
            "Mean Reward in Episode 83: -1.7030113665201552\n",
            "Mean Reward in Episode 84: -0.08556165661101253\n",
            "Mean Reward in Episode 85: -3.4525437557545224\n",
            "Mean Reward in Episode 86: -2.5108479711649965\n",
            "Mean Reward in Episode 87: -1.0345275466647483\n",
            "Mean Reward in Episode 88: -4.010060527481555\n",
            "Mean Reward in Episode 89: -2.0970445002519105\n",
            "Mean Reward in Episode 90: -1.250576165227736\n",
            "Mean Reward in Episode 91: -1.4270390667180595\n",
            "Mean Reward in Episode 92: -3.257205247378795\n",
            "Mean Reward in Episode 93: -2.5634742050399684\n",
            "Mean Reward in Episode 94: -1.1903610448530284\n",
            "Mean Reward in Episode 95: -2.710383460082745\n",
            "Mean Reward in Episode 96: -1.3914689010413772\n",
            "Mean Reward in Episode 97: -2.7273278044395446\n",
            "Mean Reward in Episode 98: -2.863909780056786\n",
            "Mean Reward in Episode 99: -1.2000325023616487\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3046, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3065, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2287, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3111, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2383, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2163, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2426, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2457, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2423, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2182, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2314, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2085, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2129, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3005, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2504, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3125, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2164, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3014, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2377, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3026, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.782844853368549\n",
            "Mean Reward in Episode 1: -1.0746940737794046\n",
            "Mean Reward in Episode 2: -3.3700976501929816\n",
            "Mean Reward in Episode 3: -3.9448311861420073\n",
            "Mean Reward in Episode 4: -0.9539249454340434\n",
            "Mean Reward in Episode 5: -1.7213458527964418\n",
            "Mean Reward in Episode 6: -1.3947410383536696\n",
            "Mean Reward in Episode 7: -2.724897170604986\n",
            "Mean Reward in Episode 8: -1.5751115737340615\n",
            "Mean Reward in Episode 9: -4.179914966196195\n",
            "Mean Reward in Episode 10: -2.000662309097285\n",
            "Mean Reward in Episode 11: -1.4936736359088532\n",
            "Mean Reward in Episode 12: -2.456594822443847\n",
            "Mean Reward in Episode 13: -1.763721118181722\n",
            "Mean Reward in Episode 14: -0.9707089639200795\n",
            "Mean Reward in Episode 15: -4.607997828453317\n",
            "Mean Reward in Episode 16: -1.7528959722472472\n",
            "Mean Reward in Episode 17: -2.51596488988477\n",
            "Mean Reward in Episode 18: -3.7236468720776585\n",
            "Mean Reward in Episode 19: -1.2963368408723923\n",
            "Mean Reward in Episode 20: -1.239689505127974\n",
            "Mean Reward in Episode 21: -1.5599997322897887\n",
            "Mean Reward in Episode 22: -2.465294207111775\n",
            "Mean Reward in Episode 23: -1.4312818211209717\n",
            "Mean Reward in Episode 24: -0.9693600681274682\n",
            "Mean Reward in Episode 25: -1.3737492839180103\n",
            "Mean Reward in Episode 26: -4.089500742929958\n",
            "Mean Reward in Episode 27: -2.6505604966264915\n",
            "Mean Reward in Episode 28: -0.878479131958019\n",
            "Mean Reward in Episode 29: -3.311464452406177\n",
            "Mean Reward in Episode 30: -3.9978309249498767\n",
            "Mean Reward in Episode 31: -3.975181999896305\n",
            "Mean Reward in Episode 32: -1.6530716036971391\n",
            "Mean Reward in Episode 33: -2.155016276100679\n",
            "Mean Reward in Episode 34: -4.458261191122058\n",
            "Mean Reward in Episode 35: -2.46011501880013\n",
            "Mean Reward in Episode 36: -1.2200469106092275\n",
            "Mean Reward in Episode 37: -3.436686343084224\n",
            "Mean Reward in Episode 38: -1.371257534504501\n",
            "Mean Reward in Episode 39: -1.5988758784086115\n",
            "Mean Reward in Episode 40: -3.0900347870043623\n",
            "Mean Reward in Episode 41: -2.18839016059644\n",
            "Mean Reward in Episode 42: -2.08486324237052\n",
            "Mean Reward in Episode 43: -1.9304834345846222\n",
            "Mean Reward in Episode 44: -3.8387002892175626\n",
            "Mean Reward in Episode 45: -1.6338987160287053\n",
            "Mean Reward in Episode 46: -5.164389033925468\n",
            "Mean Reward in Episode 47: -4.601733279500693\n",
            "Mean Reward in Episode 48: -1.16955659106125\n",
            "Mean Reward in Episode 49: -1.2803563278064143\n",
            "Mean Reward in Episode 50: -1.7413275831685378\n",
            "Mean Reward in Episode 51: -1.328112182167337\n",
            "Mean Reward in Episode 52: -4.379895976732866\n",
            "Mean Reward in Episode 53: -1.4320678010269516\n",
            "Mean Reward in Episode 54: -2.001862086795838\n",
            "Mean Reward in Episode 55: -2.3424205702074574\n",
            "Mean Reward in Episode 56: -0.9816005551094247\n",
            "Mean Reward in Episode 57: -4.659055879768165\n",
            "Mean Reward in Episode 58: -1.9979959218625278\n",
            "Mean Reward in Episode 59: -3.824094284956984\n",
            "Mean Reward in Episode 60: -1.0494275532528032\n",
            "Mean Reward in Episode 61: -4.720901206447639\n",
            "Mean Reward in Episode 62: -3.1437918210864986\n",
            "Mean Reward in Episode 63: -1.511451082079152\n",
            "Mean Reward in Episode 64: -2.5587528097579093\n",
            "Mean Reward in Episode 65: -1.1488182202582213\n",
            "Mean Reward in Episode 66: -3.1757880954663045\n",
            "Mean Reward in Episode 67: -2.096240984337857\n",
            "Mean Reward in Episode 68: -2.0285379548579088\n",
            "Mean Reward in Episode 69: -3.7407688273831865\n",
            "Mean Reward in Episode 70: -2.1765781055690394\n",
            "Mean Reward in Episode 71: -2.667270161010329\n",
            "Mean Reward in Episode 72: -3.3054029374617104\n",
            "Mean Reward in Episode 73: -1.3568542143294997\n",
            "Mean Reward in Episode 74: -1.0610853677890701\n",
            "Mean Reward in Episode 75: -2.4441023045256056\n",
            "Mean Reward in Episode 76: -2.666403220567856\n",
            "Mean Reward in Episode 77: -1.3262900026001085\n",
            "Mean Reward in Episode 78: -1.7969007448678693\n",
            "Mean Reward in Episode 79: -1.1447109737252554\n",
            "Mean Reward in Episode 80: -3.575891867399435\n",
            "Mean Reward in Episode 81: -4.460546321996191\n",
            "Mean Reward in Episode 82: -3.0016840786232826\n",
            "Mean Reward in Episode 83: -1.4649149752839903\n",
            "Mean Reward in Episode 84: -3.961524480528927\n",
            "Mean Reward in Episode 85: -1.7716409471025183\n",
            "Mean Reward in Episode 86: -1.4832686996846878\n",
            "Mean Reward in Episode 87: -1.7951185783026262\n",
            "Mean Reward in Episode 88: -3.3264494366849093\n",
            "Mean Reward in Episode 89: -0.7760004373456793\n",
            "Mean Reward in Episode 90: -0.9536732000498783\n",
            "Mean Reward in Episode 91: -1.9106900396093818\n",
            "Mean Reward in Episode 92: -1.9882961122832037\n",
            "Mean Reward in Episode 93: -3.766697329062285\n",
            "Mean Reward in Episode 94: -3.8479517377812207\n",
            "Mean Reward in Episode 95: -1.507598933684082\n",
            "Mean Reward in Episode 96: -3.731361775536523\n",
            "Mean Reward in Episode 97: -1.0554656775558129\n",
            "Mean Reward in Episode 98: -2.6991107969438497\n",
            "Mean Reward in Episode 99: -2.2236506997347893\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2172, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3083, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3244, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2161, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3102, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3050, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2338, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2322, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2212, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3000, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2314, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2562, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2341, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2231, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3012, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3033, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2162, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3209, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2119, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2322, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.5809141632835868\n",
            "Mean Reward in Episode 1: -2.5448311928905696\n",
            "Mean Reward in Episode 2: -1.7124249099369695\n",
            "Mean Reward in Episode 3: -2.1164100212691346\n",
            "Mean Reward in Episode 4: -1.1659913239598358\n",
            "Mean Reward in Episode 5: -1.469529814970206\n",
            "Mean Reward in Episode 6: -1.5067193644195513\n",
            "Mean Reward in Episode 7: -3.5202044680521203\n",
            "Mean Reward in Episode 8: -1.3556254742475926\n",
            "Mean Reward in Episode 9: -3.5495606546769722\n",
            "Mean Reward in Episode 10: -4.130562378873947\n",
            "Mean Reward in Episode 11: -3.388815873205274\n",
            "Mean Reward in Episode 12: -1.6939667134219023\n",
            "Mean Reward in Episode 13: -0.8237231302566163\n",
            "Mean Reward in Episode 14: -0.794176730469539\n",
            "Mean Reward in Episode 15: -2.409549237235767\n",
            "Mean Reward in Episode 16: -2.6367818040047752\n",
            "Mean Reward in Episode 17: -1.8562113504904083\n",
            "Mean Reward in Episode 18: -1.4895014414006613\n",
            "Mean Reward in Episode 19: -0.9714667227038494\n",
            "Mean Reward in Episode 20: -3.7937174579841466\n",
            "Mean Reward in Episode 21: -1.6318960833259482\n",
            "Mean Reward in Episode 22: -1.0448385754848475\n",
            "Mean Reward in Episode 23: -3.618139276544528\n",
            "Mean Reward in Episode 24: -2.2816599607463792\n",
            "Mean Reward in Episode 25: -0.9338775958180913\n",
            "Mean Reward in Episode 26: -2.0599855737666575\n",
            "Mean Reward in Episode 27: -3.0125006972780146\n",
            "Mean Reward in Episode 28: -1.5627239076485389\n",
            "Mean Reward in Episode 29: -0.790969375341394\n",
            "Mean Reward in Episode 30: -1.3733888594966894\n",
            "Mean Reward in Episode 31: -2.0374738364524183\n",
            "Mean Reward in Episode 32: -2.439485095103581\n",
            "Mean Reward in Episode 33: -1.2279772104303852\n",
            "Mean Reward in Episode 34: -2.80997205869667\n",
            "Mean Reward in Episode 35: -3.51002000077431\n",
            "Mean Reward in Episode 36: -3.135897856283368\n",
            "Mean Reward in Episode 37: -1.2776694017564556\n",
            "Mean Reward in Episode 38: -2.7264891823460733\n",
            "Mean Reward in Episode 39: -2.566566718970783\n",
            "Mean Reward in Episode 40: -1.6501161681238696\n",
            "Mean Reward in Episode 41: -1.050863663843463\n",
            "Mean Reward in Episode 42: -0.632391267984134\n",
            "Mean Reward in Episode 43: -4.157134688503608\n",
            "Mean Reward in Episode 44: -3.004708313425452\n",
            "Mean Reward in Episode 45: -3.9171449917967585\n",
            "Mean Reward in Episode 46: -4.232057714546831\n",
            "Mean Reward in Episode 47: -3.472102929801554\n",
            "Mean Reward in Episode 48: -1.3209073978014971\n",
            "Mean Reward in Episode 49: -1.085670367704658\n",
            "Mean Reward in Episode 50: -0.28842582891053553\n",
            "Mean Reward in Episode 51: -1.313773731483659\n",
            "Mean Reward in Episode 52: -1.6316710281907258\n",
            "Mean Reward in Episode 53: -1.7454434521906463\n",
            "Mean Reward in Episode 54: -1.099935934329642\n",
            "Mean Reward in Episode 55: -1.366079215307763\n",
            "Mean Reward in Episode 56: -3.497123801595155\n",
            "Mean Reward in Episode 57: -2.147005233249698\n",
            "Mean Reward in Episode 58: -1.6665035456797408\n",
            "Mean Reward in Episode 59: -4.084473886860638\n",
            "Mean Reward in Episode 60: -3.1881131143095156\n",
            "Mean Reward in Episode 61: -1.651482232956418\n",
            "Mean Reward in Episode 62: -1.3391455679795583\n",
            "Mean Reward in Episode 63: -1.8908737473631807\n",
            "Mean Reward in Episode 64: -1.3089155232239407\n",
            "Mean Reward in Episode 65: -3.4568390702607514\n",
            "Mean Reward in Episode 66: -2.605460267133897\n",
            "Mean Reward in Episode 67: -3.5148219650317962\n",
            "Mean Reward in Episode 68: -3.6584395757514745\n",
            "Mean Reward in Episode 69: -1.3914747937413223\n",
            "Mean Reward in Episode 70: -3.5498880093291834\n",
            "Mean Reward in Episode 71: -1.4306908190154324\n",
            "Mean Reward in Episode 72: -3.610945910100766\n",
            "Mean Reward in Episode 73: -1.5325572510023762\n",
            "Mean Reward in Episode 74: -1.4852643873685074\n",
            "Mean Reward in Episode 75: -1.2199091463316494\n",
            "Mean Reward in Episode 76: -0.8029487217714246\n",
            "Mean Reward in Episode 77: -1.406934912447275\n",
            "Mean Reward in Episode 78: -2.1685158212881124\n",
            "Mean Reward in Episode 79: -4.081585239601006\n",
            "Mean Reward in Episode 80: -2.3749033643365642\n",
            "Mean Reward in Episode 81: -0.8646901949437711\n",
            "Mean Reward in Episode 82: -3.5261030292849753\n",
            "Mean Reward in Episode 83: -3.8373840868005242\n",
            "Mean Reward in Episode 84: -2.043057421744077\n",
            "Mean Reward in Episode 85: -1.8839040946996397\n",
            "Mean Reward in Episode 86: -2.240430243796096\n",
            "Mean Reward in Episode 87: -1.4980977954503119\n",
            "Mean Reward in Episode 88: -2.335161368372736\n",
            "Mean Reward in Episode 89: -3.227218194504568\n",
            "Mean Reward in Episode 90: -1.0417428582062747\n",
            "Mean Reward in Episode 91: -1.6786272056387668\n",
            "Mean Reward in Episode 92: -1.0280498247549497\n",
            "Mean Reward in Episode 93: -2.460748938330632\n",
            "Mean Reward in Episode 94: -1.1532103162538452\n",
            "Mean Reward in Episode 95: -4.636176649104903\n",
            "Mean Reward in Episode 96: -0.974473288516672\n",
            "Mean Reward in Episode 97: -1.3032906957678432\n",
            "Mean Reward in Episode 98: -0.5874523578762876\n",
            "Mean Reward in Episode 99: -5.676671109351369\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2210, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2470, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2398, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2292, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2327, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2062, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2460, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2297, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3067, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3045, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3130, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2129, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2299, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2165, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2394, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3120, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2201, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2258, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2203, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2219, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -4.409480216939596\n",
            "Mean Reward in Episode 1: -1.8583260178714522\n",
            "Mean Reward in Episode 2: -1.9037526134222873\n",
            "Mean Reward in Episode 3: -1.5531555640423822\n",
            "Mean Reward in Episode 4: -1.9459501615212373\n",
            "Mean Reward in Episode 5: -1.975664257347015\n",
            "Mean Reward in Episode 6: -2.2908109425653884\n",
            "Mean Reward in Episode 7: -1.35590035111091\n",
            "Mean Reward in Episode 8: -1.7003867595962288\n",
            "Mean Reward in Episode 9: -3.537656617174467\n",
            "Mean Reward in Episode 10: -4.030402984221046\n",
            "Mean Reward in Episode 11: -1.5260468906217688\n",
            "Mean Reward in Episode 12: -1.3920225016894694\n",
            "Mean Reward in Episode 13: -3.252342067759101\n",
            "Mean Reward in Episode 14: -1.2193580647779156\n",
            "Mean Reward in Episode 15: -2.2319402857496717\n",
            "Mean Reward in Episode 16: -3.2278158673558552\n",
            "Mean Reward in Episode 17: -2.8728275069182705\n",
            "Mean Reward in Episode 18: -1.1514536868417027\n",
            "Mean Reward in Episode 19: -2.106558907620779\n",
            "Mean Reward in Episode 20: -5.00458568861696\n",
            "Mean Reward in Episode 21: -2.0753000802184114\n",
            "Mean Reward in Episode 22: -2.443786689028479\n",
            "Mean Reward in Episode 23: -2.701186172519123\n",
            "Mean Reward in Episode 24: -1.1047308296791638\n",
            "Mean Reward in Episode 25: -1.1775755990055885\n",
            "Mean Reward in Episode 26: -2.8743026311182844\n",
            "Mean Reward in Episode 27: -0.8629188019464173\n",
            "Mean Reward in Episode 28: -3.4913364461099388\n",
            "Mean Reward in Episode 29: -2.570203791510459\n",
            "Mean Reward in Episode 30: -1.977266954787344\n",
            "Mean Reward in Episode 31: -2.0388450271949714\n",
            "Mean Reward in Episode 32: -1.6706196329568204\n",
            "Mean Reward in Episode 33: -3.9303603568781496\n",
            "Mean Reward in Episode 34: -1.4895314120704717\n",
            "Mean Reward in Episode 35: -1.3246507115556116\n",
            "Mean Reward in Episode 36: -1.3465953700927917\n",
            "Mean Reward in Episode 37: -3.323396494600524\n",
            "Mean Reward in Episode 38: -4.87058640456603\n",
            "Mean Reward in Episode 39: -0.6785513417698177\n",
            "Mean Reward in Episode 40: -2.6717879173699024\n",
            "Mean Reward in Episode 41: -1.1970944169621416\n",
            "Mean Reward in Episode 42: -2.798645515037794\n",
            "Mean Reward in Episode 43: -3.113755724089364\n",
            "Mean Reward in Episode 44: -4.545235747155603\n",
            "Mean Reward in Episode 45: -1.0901048990858355\n",
            "Mean Reward in Episode 46: -0.7176627480270422\n",
            "Mean Reward in Episode 47: -4.156031225993303\n",
            "Mean Reward in Episode 48: -0.8335644415078413\n",
            "Mean Reward in Episode 49: -2.033592090557233\n",
            "Mean Reward in Episode 50: -2.2163528584014958\n",
            "Mean Reward in Episode 51: -3.497869663927521\n",
            "Mean Reward in Episode 52: -2.207174096697384\n",
            "Mean Reward in Episode 53: -1.19179121465565\n",
            "Mean Reward in Episode 54: -0.8020314622481923\n",
            "Mean Reward in Episode 55: -0.7873394980051039\n",
            "Mean Reward in Episode 56: -5.040584097528421\n",
            "Mean Reward in Episode 57: -3.0330329757791694\n",
            "Mean Reward in Episode 58: -1.1637328197306749\n",
            "Mean Reward in Episode 59: -0.7084484765677524\n",
            "Mean Reward in Episode 60: -1.1855566785766942\n",
            "Mean Reward in Episode 61: -2.0238395271746104\n",
            "Mean Reward in Episode 62: -3.545534600755531\n",
            "Mean Reward in Episode 63: -1.2274298234854935\n",
            "Mean Reward in Episode 64: -1.6865575604925471\n",
            "Mean Reward in Episode 65: -1.8393163698590098\n",
            "Mean Reward in Episode 66: -1.7812428388083306\n",
            "Mean Reward in Episode 67: -1.4543366815225465\n",
            "Mean Reward in Episode 68: -0.5962624754034264\n",
            "Mean Reward in Episode 69: -1.5244177655280846\n",
            "Mean Reward in Episode 70: -2.691530361698617\n",
            "Mean Reward in Episode 71: -2.103709771427332\n",
            "Mean Reward in Episode 72: -4.888519467863464\n",
            "Mean Reward in Episode 73: -1.0094253567734726\n",
            "Mean Reward in Episode 74: -3.707899568328876\n",
            "Mean Reward in Episode 75: -1.5226328826100117\n",
            "Mean Reward in Episode 76: -2.5607367829163956\n",
            "Mean Reward in Episode 77: -4.293676841003869\n",
            "Mean Reward in Episode 78: -2.045199201626524\n",
            "Mean Reward in Episode 79: -1.19657071365728\n",
            "Mean Reward in Episode 80: -4.227873816160258\n",
            "Mean Reward in Episode 81: -4.670335383719769\n",
            "Mean Reward in Episode 82: -3.3233279717282826\n",
            "Mean Reward in Episode 83: -2.4301329728428347\n",
            "Mean Reward in Episode 84: -4.6482632025286845\n",
            "Mean Reward in Episode 85: -2.4738306219370236\n",
            "Mean Reward in Episode 86: -1.7826834598711976\n",
            "Mean Reward in Episode 87: -2.693791281263169\n",
            "Mean Reward in Episode 88: -3.186799846241975\n",
            "Mean Reward in Episode 89: -1.4818548228083095\n",
            "Mean Reward in Episode 90: -2.5987568071913487\n",
            "Mean Reward in Episode 91: -1.6438256454034905\n",
            "Mean Reward in Episode 92: -1.5864046984842481\n",
            "Mean Reward in Episode 93: -1.4701365688900299\n",
            "Mean Reward in Episode 94: -1.8346024677986497\n",
            "Mean Reward in Episode 95: -1.6991655780613808\n",
            "Mean Reward in Episode 96: -2.0581498925363197\n",
            "Mean Reward in Episode 97: -1.8018372548095698\n",
            "Mean Reward in Episode 98: -3.087624051321338\n",
            "Mean Reward in Episode 99: -1.8908174210152944\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2272, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2447, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3090, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2447, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2510, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2407, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2207, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2328, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3063, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2309, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2534, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2387, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2213, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2220, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3088, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2141, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2153, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3096, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2158, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2391, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.183605940928881\n",
            "Mean Reward in Episode 1: -2.133344940740334\n",
            "Mean Reward in Episode 2: 0.09851948923720952\n",
            "Mean Reward in Episode 3: -0.9739753325622766\n",
            "Mean Reward in Episode 4: -2.282510606952024\n",
            "Mean Reward in Episode 5: -1.4964179166607825\n",
            "Mean Reward in Episode 6: -2.7280471781644215\n",
            "Mean Reward in Episode 7: -2.5497145136853576\n",
            "Mean Reward in Episode 8: -4.708955789228749\n",
            "Mean Reward in Episode 9: -2.508803351495885\n",
            "Mean Reward in Episode 10: -2.1753437260106954\n",
            "Mean Reward in Episode 11: -3.467702202708321\n",
            "Mean Reward in Episode 12: -0.03775054572774365\n",
            "Mean Reward in Episode 13: -0.743305173731671\n",
            "Mean Reward in Episode 14: -1.4316899740973044\n",
            "Mean Reward in Episode 15: -1.4510713730964309\n",
            "Mean Reward in Episode 16: -1.2897219154099169\n",
            "Mean Reward in Episode 17: -1.30787190911577\n",
            "Mean Reward in Episode 18: -1.9036942575944453\n",
            "Mean Reward in Episode 19: -0.7625379099187595\n",
            "Mean Reward in Episode 20: -2.0053480922687537\n",
            "Mean Reward in Episode 21: -2.133411023765943\n",
            "Mean Reward in Episode 22: -0.2694337356316168\n",
            "Mean Reward in Episode 23: -3.6129408106247234\n",
            "Mean Reward in Episode 24: -2.8828929609607563\n",
            "Mean Reward in Episode 25: -1.7395014741705819\n",
            "Mean Reward in Episode 26: -3.1078842987486617\n",
            "Mean Reward in Episode 27: -0.8702895327227348\n",
            "Mean Reward in Episode 28: -3.149553895211358\n",
            "Mean Reward in Episode 29: -3.9229659705256688\n",
            "Mean Reward in Episode 30: -3.0596619621416146\n",
            "Mean Reward in Episode 31: -0.343588148498588\n",
            "Mean Reward in Episode 32: -2.484786910150368\n",
            "Mean Reward in Episode 33: -3.6387777131818413\n",
            "Mean Reward in Episode 34: -1.7305104596514467\n",
            "Mean Reward in Episode 35: -0.7590464712555024\n",
            "Mean Reward in Episode 36: -1.3974218745346614\n",
            "Mean Reward in Episode 37: -2.256667215873791\n",
            "Mean Reward in Episode 38: -3.828706929782225\n",
            "Mean Reward in Episode 39: -3.02245966740021\n",
            "Mean Reward in Episode 40: -3.0777832415486\n",
            "Mean Reward in Episode 41: -1.3419546234260928\n",
            "Mean Reward in Episode 42: -3.4076889830777715\n",
            "Mean Reward in Episode 43: -0.027292043339756156\n",
            "Mean Reward in Episode 44: -2.0495755785926413\n",
            "Mean Reward in Episode 45: -1.1403117599060824\n",
            "Mean Reward in Episode 46: -3.4486914360598457\n",
            "Mean Reward in Episode 47: -4.1375981568991085\n",
            "Mean Reward in Episode 48: -1.9120333862939396\n",
            "Mean Reward in Episode 49: -1.3751262660688512\n",
            "Mean Reward in Episode 50: -1.3287755944056692\n",
            "Mean Reward in Episode 51: -1.458470434183974\n",
            "Mean Reward in Episode 52: -4.190639177833285\n",
            "Mean Reward in Episode 53: -0.704679073312246\n",
            "Mean Reward in Episode 54: -0.976288991000159\n",
            "Mean Reward in Episode 55: -1.9026697989119197\n",
            "Mean Reward in Episode 56: -3.7048419015022342\n",
            "Mean Reward in Episode 57: -3.0160153352257555\n",
            "Mean Reward in Episode 58: -2.7971842506370503\n",
            "Mean Reward in Episode 59: -2.724818825261288\n",
            "Mean Reward in Episode 60: -2.4322664178142968\n",
            "Mean Reward in Episode 61: -1.6388942172850398\n",
            "Mean Reward in Episode 62: -2.4834052292637123\n",
            "Mean Reward in Episode 63: -1.3811307655094365\n",
            "Mean Reward in Episode 64: -2.44440482086015\n",
            "Mean Reward in Episode 65: -1.880014206106321\n",
            "Mean Reward in Episode 66: -3.60245836580171\n",
            "Mean Reward in Episode 67: -1.5262506751488452\n",
            "Mean Reward in Episode 68: -1.8741711489837096\n",
            "Mean Reward in Episode 69: -1.5205605599542107\n",
            "Mean Reward in Episode 70: -5.1362156506535985\n",
            "Mean Reward in Episode 71: -2.592219494736846\n",
            "Mean Reward in Episode 72: -2.7405586603600725\n",
            "Mean Reward in Episode 73: -2.2812188967059837\n",
            "Mean Reward in Episode 74: -0.9250194870261079\n",
            "Mean Reward in Episode 75: -1.9529790348711613\n",
            "Mean Reward in Episode 76: -1.4353672889663085\n",
            "Mean Reward in Episode 77: -3.073190323969174\n",
            "Mean Reward in Episode 78: -1.6097779452356016\n",
            "Mean Reward in Episode 79: -2.8489068179131016\n",
            "Mean Reward in Episode 80: -1.119416746665968\n",
            "Mean Reward in Episode 81: -2.019434803426922\n",
            "Mean Reward in Episode 82: -1.2583145984525168\n",
            "Mean Reward in Episode 83: -1.187983376867369\n",
            "Mean Reward in Episode 84: -1.3121819270899429\n",
            "Mean Reward in Episode 85: -1.8654156817538325\n",
            "Mean Reward in Episode 86: -1.538935079657463\n",
            "Mean Reward in Episode 87: -4.647949699243625\n",
            "Mean Reward in Episode 88: -1.6388358314371505\n",
            "Mean Reward in Episode 89: -1.1805884148381962\n",
            "Mean Reward in Episode 90: -2.3667438267120593\n",
            "Mean Reward in Episode 91: -2.0162497720762573\n",
            "Mean Reward in Episode 92: -3.9934681958059763\n",
            "Mean Reward in Episode 93: -1.5463696350557319\n",
            "Mean Reward in Episode 94: -3.837298810605821\n",
            "Mean Reward in Episode 95: -1.432189327659373\n",
            "Mean Reward in Episode 96: -1.5975846494654227\n",
            "Mean Reward in Episode 97: -1.7979983439979188\n",
            "Mean Reward in Episode 98: -4.3597487630841405\n",
            "Mean Reward in Episode 99: -2.144342094629726\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2265, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2406, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2454, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2389, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2170, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3053, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2307, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3119, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3054, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3051, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2304, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3023, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3045, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2285, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2150, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2350, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3038, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3077, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2380, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2376, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -0.6260304338866474\n",
            "Mean Reward in Episode 1: -1.7318705111353092\n",
            "Mean Reward in Episode 2: -4.752235088474836\n",
            "Mean Reward in Episode 3: -3.1284977574947126\n",
            "Mean Reward in Episode 4: -1.3828836900064918\n",
            "Mean Reward in Episode 5: -2.242391643744339\n",
            "Mean Reward in Episode 6: -2.498551170748212\n",
            "Mean Reward in Episode 7: -2.432068129294933\n",
            "Mean Reward in Episode 8: -2.64951591096204\n",
            "Mean Reward in Episode 9: -3.4347719932200524\n",
            "Mean Reward in Episode 10: -1.482293275931957\n",
            "Mean Reward in Episode 11: -1.5065343597651142\n",
            "Mean Reward in Episode 12: -1.5675637376153286\n",
            "Mean Reward in Episode 13: -2.4069336139837607\n",
            "Mean Reward in Episode 14: -0.6531367515873204\n",
            "Mean Reward in Episode 15: -1.8077775036306227\n",
            "Mean Reward in Episode 16: -2.4142236521461466\n",
            "Mean Reward in Episode 17: -2.843551547719613\n",
            "Mean Reward in Episode 18: -3.065072129617132\n",
            "Mean Reward in Episode 19: -4.169624518511497\n",
            "Mean Reward in Episode 20: -2.256130250392\n",
            "Mean Reward in Episode 21: -0.21695765591668317\n",
            "Mean Reward in Episode 22: -3.887637573992957\n",
            "Mean Reward in Episode 23: -1.2580976384975797\n",
            "Mean Reward in Episode 24: -1.4197800759215349\n",
            "Mean Reward in Episode 25: -1.5381447015128333\n",
            "Mean Reward in Episode 26: -2.056601849517139\n",
            "Mean Reward in Episode 27: -3.0976545078862356\n",
            "Mean Reward in Episode 28: -3.631152792842824\n",
            "Mean Reward in Episode 29: -1.4145616540361372\n",
            "Mean Reward in Episode 30: -0.19535400987480167\n",
            "Mean Reward in Episode 31: -3.4093596692297603\n",
            "Mean Reward in Episode 32: -2.9420274004216234\n",
            "Mean Reward in Episode 33: -2.514614806780119\n",
            "Mean Reward in Episode 34: -1.2630410349320726\n",
            "Mean Reward in Episode 35: -1.6243190218594987\n",
            "Mean Reward in Episode 36: -1.635351451634966\n",
            "Mean Reward in Episode 37: -2.694021420084084\n",
            "Mean Reward in Episode 38: -1.8589907180956315\n",
            "Mean Reward in Episode 39: -3.2963049107887588\n",
            "Mean Reward in Episode 40: -3.078926664666714\n",
            "Mean Reward in Episode 41: -2.082544401087549\n",
            "Mean Reward in Episode 42: -2.912200599870271\n",
            "Mean Reward in Episode 43: -1.3410096065565982\n",
            "Mean Reward in Episode 44: -1.3601302760226905\n",
            "Mean Reward in Episode 45: -1.3226476168250096\n",
            "Mean Reward in Episode 46: -3.448882692883044\n",
            "Mean Reward in Episode 47: -1.7086770840423102\n",
            "Mean Reward in Episode 48: -1.1461408362827095\n",
            "Mean Reward in Episode 49: -1.9819170938339001\n",
            "Mean Reward in Episode 50: -1.778681234631975\n",
            "Mean Reward in Episode 51: -2.2938395947392833\n",
            "Mean Reward in Episode 52: -1.7553158808153373\n",
            "Mean Reward in Episode 53: -1.8592690249474964\n",
            "Mean Reward in Episode 54: -0.9996339836213792\n",
            "Mean Reward in Episode 55: -3.006300164186391\n",
            "Mean Reward in Episode 56: -1.8076564798406685\n",
            "Mean Reward in Episode 57: -3.1990765754050967\n",
            "Mean Reward in Episode 58: -3.068172136720574\n",
            "Mean Reward in Episode 59: -1.3915445075931916\n",
            "Mean Reward in Episode 60: -1.949652627149032\n",
            "Mean Reward in Episode 61: -3.5690133221709837\n",
            "Mean Reward in Episode 62: -0.7361278977876974\n",
            "Mean Reward in Episode 63: -4.616117099448087\n",
            "Mean Reward in Episode 64: -1.6966836976756856\n",
            "Mean Reward in Episode 65: -1.3873454733147534\n",
            "Mean Reward in Episode 66: -1.2933711068805942\n",
            "Mean Reward in Episode 67: -1.395670527623096\n",
            "Mean Reward in Episode 68: -4.017947854379379\n",
            "Mean Reward in Episode 69: -1.682284194499064\n",
            "Mean Reward in Episode 70: -1.6020187764059501\n",
            "Mean Reward in Episode 71: -2.8718148161073795\n",
            "Mean Reward in Episode 72: -0.9162087913751232\n",
            "Mean Reward in Episode 73: -2.0339183843881075\n",
            "Mean Reward in Episode 74: -1.6073405893460417\n",
            "Mean Reward in Episode 75: -3.480617612501541\n",
            "Mean Reward in Episode 76: -3.5836873985457522\n",
            "Mean Reward in Episode 77: -2.194481710096648\n",
            "Mean Reward in Episode 78: -3.253196298275101\n",
            "Mean Reward in Episode 79: -0.8093047160307757\n",
            "Mean Reward in Episode 80: -2.4225554723937894\n",
            "Mean Reward in Episode 81: -2.419555041022186\n",
            "Mean Reward in Episode 82: -0.7422056370951954\n",
            "Mean Reward in Episode 83: -3.788164529382677\n",
            "Mean Reward in Episode 84: -0.7515819325221779\n",
            "Mean Reward in Episode 85: -1.2545940191879428\n",
            "Mean Reward in Episode 86: -2.118955779027604\n",
            "Mean Reward in Episode 87: -2.617108975815539\n",
            "Mean Reward in Episode 88: -0.4694517517371802\n",
            "Mean Reward in Episode 89: -2.020961936896129\n",
            "Mean Reward in Episode 90: -2.2564645946743704\n",
            "Mean Reward in Episode 91: -2.167588936826797\n",
            "Mean Reward in Episode 92: -0.9663447360565732\n",
            "Mean Reward in Episode 93: -1.3348131859623773\n",
            "Mean Reward in Episode 94: -1.4181723437944354\n",
            "Mean Reward in Episode 95: -2.2153296076638314\n",
            "Mean Reward in Episode 96: -1.590854115072816\n",
            "Mean Reward in Episode 97: -1.1310053938442268\n",
            "Mean Reward in Episode 98: -1.0039522449131493\n",
            "Mean Reward in Episode 99: -0.764112669550403\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2507, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2341, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3038, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2301, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2167, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2124, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2401, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3053, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2209, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2123, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2333, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2539, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2156, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3025, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2536, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2224, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3131, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2347, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2351, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3058, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.3847430133174905\n",
            "Mean Reward in Episode 1: -1.600904992010254\n",
            "Mean Reward in Episode 2: -0.930875594478722\n",
            "Mean Reward in Episode 3: -1.5886010589445683\n",
            "Mean Reward in Episode 4: -1.1995308282889419\n",
            "Mean Reward in Episode 5: -1.4230154427971191\n",
            "Mean Reward in Episode 6: -1.3446685835082497\n",
            "Mean Reward in Episode 7: -3.5634117814139303\n",
            "Mean Reward in Episode 8: -1.5152433488970412\n",
            "Mean Reward in Episode 9: -4.092371643669395\n",
            "Mean Reward in Episode 10: -4.093701309122193\n",
            "Mean Reward in Episode 11: -1.2297065431439638\n",
            "Mean Reward in Episode 12: -2.4735310138978557\n",
            "Mean Reward in Episode 13: -2.2475753087131483\n",
            "Mean Reward in Episode 14: -1.1309868818795963\n",
            "Mean Reward in Episode 15: -3.592658953757201\n",
            "Mean Reward in Episode 16: -4.114293219306983\n",
            "Mean Reward in Episode 17: -2.155691996946238\n",
            "Mean Reward in Episode 18: -2.728195820587207\n",
            "Mean Reward in Episode 19: -1.618510642968847\n",
            "Mean Reward in Episode 20: -2.062562745648089\n",
            "Mean Reward in Episode 21: -0.8156003387663322\n",
            "Mean Reward in Episode 22: -1.342502642424993\n",
            "Mean Reward in Episode 23: -2.4679926039663793\n",
            "Mean Reward in Episode 24: -0.9025702413421688\n",
            "Mean Reward in Episode 25: -1.0237529231432099\n",
            "Mean Reward in Episode 26: -2.396981143145197\n",
            "Mean Reward in Episode 27: -2.8255197569237542\n",
            "Mean Reward in Episode 28: -2.97196876572385\n",
            "Mean Reward in Episode 29: -2.137478918007066\n",
            "Mean Reward in Episode 30: -4.967827581167456\n",
            "Mean Reward in Episode 31: -1.2732104673357163\n",
            "Mean Reward in Episode 32: -2.412084814971314\n",
            "Mean Reward in Episode 33: -1.716147781293097\n",
            "Mean Reward in Episode 34: -3.5359715795239155\n",
            "Mean Reward in Episode 35: -0.8995214089365356\n",
            "Mean Reward in Episode 36: -1.46300128783597\n",
            "Mean Reward in Episode 37: -1.2740851603808803\n",
            "Mean Reward in Episode 38: -2.275212153361321\n",
            "Mean Reward in Episode 39: -1.261357369466877\n",
            "Mean Reward in Episode 40: -2.1581459563143914\n",
            "Mean Reward in Episode 41: -3.0227152698441695\n",
            "Mean Reward in Episode 42: -1.1971387501327742\n",
            "Mean Reward in Episode 43: -2.18490304591945\n",
            "Mean Reward in Episode 44: -1.568522458884178\n",
            "Mean Reward in Episode 45: 0.2187362030393936\n",
            "Mean Reward in Episode 46: -5.643282046927723\n",
            "Mean Reward in Episode 47: -1.744743510324106\n",
            "Mean Reward in Episode 48: -4.398576664726105\n",
            "Mean Reward in Episode 49: -2.018448014940578\n",
            "Mean Reward in Episode 50: -1.2616524656664914\n",
            "Mean Reward in Episode 51: -2.2815172064008706\n",
            "Mean Reward in Episode 52: -4.323125818967442\n",
            "Mean Reward in Episode 53: -3.92183625110939\n",
            "Mean Reward in Episode 54: -1.3260304694264418\n",
            "Mean Reward in Episode 55: -1.5191515787256695\n",
            "Mean Reward in Episode 56: -1.6311951315652464\n",
            "Mean Reward in Episode 57: -4.807244204276254\n",
            "Mean Reward in Episode 58: -1.5758252242238062\n",
            "Mean Reward in Episode 59: -0.22208594793224978\n",
            "Mean Reward in Episode 60: -1.1327311315119748\n",
            "Mean Reward in Episode 61: -1.8255178860377403\n",
            "Mean Reward in Episode 62: -2.9106406996515197\n",
            "Mean Reward in Episode 63: -1.926818089560727\n",
            "Mean Reward in Episode 64: -1.4703973368069\n",
            "Mean Reward in Episode 65: -1.557172204300534\n",
            "Mean Reward in Episode 66: -0.029298554924845035\n",
            "Mean Reward in Episode 67: -1.3306290578818512\n",
            "Mean Reward in Episode 68: -1.3827325115890265\n",
            "Mean Reward in Episode 69: -1.309906671779064\n",
            "Mean Reward in Episode 70: -2.6068397221492168\n",
            "Mean Reward in Episode 71: -0.9409893187918383\n",
            "Mean Reward in Episode 72: -5.132481891277229\n",
            "Mean Reward in Episode 73: -1.770576666763232\n",
            "Mean Reward in Episode 74: -4.104682094814151\n",
            "Mean Reward in Episode 75: -5.38735354991321\n",
            "Mean Reward in Episode 76: -1.9080441337808705\n",
            "Mean Reward in Episode 77: -2.9131103366449715\n",
            "Mean Reward in Episode 78: -3.327024332540992\n",
            "Mean Reward in Episode 79: -1.4417767181025174\n",
            "Mean Reward in Episode 80: -1.2559228154506141\n",
            "Mean Reward in Episode 81: -2.811936995640013\n",
            "Mean Reward in Episode 82: -2.756063810430259\n",
            "Mean Reward in Episode 83: -3.335965104506462\n",
            "Mean Reward in Episode 84: -1.533641135804858\n",
            "Mean Reward in Episode 85: -0.7281049678781458\n",
            "Mean Reward in Episode 86: -2.7822216017047894\n",
            "Mean Reward in Episode 87: -4.878428918519795\n",
            "Mean Reward in Episode 88: -1.0839183139252309\n",
            "Mean Reward in Episode 89: -2.382199285938367\n",
            "Mean Reward in Episode 90: -3.3639188792292702\n",
            "Mean Reward in Episode 91: -2.114552718019418\n",
            "Mean Reward in Episode 92: -1.4183335240762633\n",
            "Mean Reward in Episode 93: -4.6089202292733376\n",
            "Mean Reward in Episode 94: -3.047336658654442\n",
            "Mean Reward in Episode 95: -1.7810217325687447\n",
            "Mean Reward in Episode 96: -1.8487725060383589\n",
            "Mean Reward in Episode 97: -1.471480558429942\n",
            "Mean Reward in Episode 98: -3.621954683953519\n",
            "Mean Reward in Episode 99: -2.602000481561252\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2221, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2516, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3107, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3071, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2306, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2200, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2106, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2381, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3054, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3023, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2321, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2375, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2368, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3089, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2292, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3161, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2285, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3084, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3143, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2105, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.433366546289156\n",
            "Mean Reward in Episode 1: -2.00186019705795\n",
            "Mean Reward in Episode 2: -2.668781272285663\n",
            "Mean Reward in Episode 3: -1.248747559941279\n",
            "Mean Reward in Episode 4: -1.9323593278991995\n",
            "Mean Reward in Episode 5: -2.7388226042708674\n",
            "Mean Reward in Episode 6: -2.9865631780477626\n",
            "Mean Reward in Episode 7: -1.4085370570541602\n",
            "Mean Reward in Episode 8: -1.8143488313900284\n",
            "Mean Reward in Episode 9: -3.9483671473423216\n",
            "Mean Reward in Episode 10: -3.123136103253318\n",
            "Mean Reward in Episode 11: -1.2595074491856861\n",
            "Mean Reward in Episode 12: -3.4798408666423213\n",
            "Mean Reward in Episode 13: -2.6645863384549546\n",
            "Mean Reward in Episode 14: -1.0749218253694492\n",
            "Mean Reward in Episode 15: -3.041435643925102\n",
            "Mean Reward in Episode 16: -1.9956121054853841\n",
            "Mean Reward in Episode 17: -0.5759097320582137\n",
            "Mean Reward in Episode 18: -3.342981857293113\n",
            "Mean Reward in Episode 19: -2.9079849737079386\n",
            "Mean Reward in Episode 20: -1.4134392226626435\n",
            "Mean Reward in Episode 21: -2.426218699086195\n",
            "Mean Reward in Episode 22: -1.5696124109808578\n",
            "Mean Reward in Episode 23: -1.3350391407791673\n",
            "Mean Reward in Episode 24: -2.700060971065179\n",
            "Mean Reward in Episode 25: -1.5645961653640632\n",
            "Mean Reward in Episode 26: -2.0429983716313034\n",
            "Mean Reward in Episode 27: -3.073005507901617\n",
            "Mean Reward in Episode 28: -2.971050101501312\n",
            "Mean Reward in Episode 29: -1.9246339894797424\n",
            "Mean Reward in Episode 30: -1.4891797326354788\n",
            "Mean Reward in Episode 31: -3.302923396217611\n",
            "Mean Reward in Episode 32: -1.0386353438603029\n",
            "Mean Reward in Episode 33: -2.8898057647566855\n",
            "Mean Reward in Episode 34: -1.7645301834470035\n",
            "Mean Reward in Episode 35: -1.1143075144474346\n",
            "Mean Reward in Episode 36: -4.406928468940835\n",
            "Mean Reward in Episode 37: -1.944822247658744\n",
            "Mean Reward in Episode 38: -1.450703743479925\n",
            "Mean Reward in Episode 39: -1.940260238357476\n",
            "Mean Reward in Episode 40: -0.5983938006736501\n",
            "Mean Reward in Episode 41: -1.5819495279647382\n",
            "Mean Reward in Episode 42: -4.50114804421031\n",
            "Mean Reward in Episode 43: -2.54241297085354\n",
            "Mean Reward in Episode 44: -2.5777477776862683\n",
            "Mean Reward in Episode 45: -0.33507567708686864\n",
            "Mean Reward in Episode 46: -1.8496575877235841\n",
            "Mean Reward in Episode 47: -1.352433026381942\n",
            "Mean Reward in Episode 48: -4.235973390097852\n",
            "Mean Reward in Episode 49: -1.0929985989285131\n",
            "Mean Reward in Episode 50: -1.8029709001451304\n",
            "Mean Reward in Episode 51: -1.4745235443719251\n",
            "Mean Reward in Episode 52: -2.6113989110521656\n",
            "Mean Reward in Episode 53: -3.1320752243648657\n",
            "Mean Reward in Episode 54: -1.6052148555309163\n",
            "Mean Reward in Episode 55: -1.7980667372371455\n",
            "Mean Reward in Episode 56: -1.0604675943161272\n",
            "Mean Reward in Episode 57: -1.4500900108125618\n",
            "Mean Reward in Episode 58: -2.0477235289497195\n",
            "Mean Reward in Episode 59: -3.3082569654613176\n",
            "Mean Reward in Episode 60: -2.079745856214469\n",
            "Mean Reward in Episode 61: -1.6752748947596485\n",
            "Mean Reward in Episode 62: -2.5008472869339182\n",
            "Mean Reward in Episode 63: -1.3156273534485545\n",
            "Mean Reward in Episode 64: -2.2073504453737405\n",
            "Mean Reward in Episode 65: -1.3570160170289172\n",
            "Mean Reward in Episode 66: -1.4112795884731577\n",
            "Mean Reward in Episode 67: -2.5379832053924027\n",
            "Mean Reward in Episode 68: -3.362158700708738\n",
            "Mean Reward in Episode 69: -4.966646876558635\n",
            "Mean Reward in Episode 70: -3.6255358263056046\n",
            "Mean Reward in Episode 71: -1.0579397291329198\n",
            "Mean Reward in Episode 72: -0.33741699637995426\n",
            "Mean Reward in Episode 73: -4.6554747548030955\n",
            "Mean Reward in Episode 74: -1.4933426775756662\n",
            "Mean Reward in Episode 75: -3.6459465019617694\n",
            "Mean Reward in Episode 76: -3.560895705817525\n",
            "Mean Reward in Episode 77: -5.180549177513944\n",
            "Mean Reward in Episode 78: -0.9262107990824204\n",
            "Mean Reward in Episode 79: -1.4829030762765496\n",
            "Mean Reward in Episode 80: -1.9784797167470314\n",
            "Mean Reward in Episode 81: -1.3927780629533753\n",
            "Mean Reward in Episode 82: -1.3382328495765043\n",
            "Mean Reward in Episode 83: -1.6117178033686776\n",
            "Mean Reward in Episode 84: -3.335781688821681\n",
            "Mean Reward in Episode 85: -1.041735458368309\n",
            "Mean Reward in Episode 86: -2.5329520879586735\n",
            "Mean Reward in Episode 87: -1.4382604968052106\n",
            "Mean Reward in Episode 88: -1.1681731501536954\n",
            "Mean Reward in Episode 89: -1.4029061517936072\n",
            "Mean Reward in Episode 90: -2.721056698041141\n",
            "Mean Reward in Episode 91: -2.0309017398938134\n",
            "Mean Reward in Episode 92: -4.550465699465453\n",
            "Mean Reward in Episode 93: -1.0668997168322545\n",
            "Mean Reward in Episode 94: -4.656926485880231\n",
            "Mean Reward in Episode 95: -0.9930201723979144\n",
            "Mean Reward in Episode 96: -1.6817541567550762\n",
            "Mean Reward in Episode 97: -4.593342591452926\n",
            "Mean Reward in Episode 98: -0.9310663515301403\n",
            "Mean Reward in Episode 99: -3.075711610837617\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2169, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2310, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2201, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2485, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2392, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2124, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2329, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3061, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2130, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2155, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2322, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3119, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2220, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2164, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2210, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2451, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2214, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2468, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2222, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2223, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.8200037894074876\n",
            "Mean Reward in Episode 1: -1.8231534823292497\n",
            "Mean Reward in Episode 2: -1.764339144030853\n",
            "Mean Reward in Episode 3: -2.650922495574279\n",
            "Mean Reward in Episode 4: -1.067712585305423\n",
            "Mean Reward in Episode 5: -3.6552714583149584\n",
            "Mean Reward in Episode 6: -2.1378415719026256\n",
            "Mean Reward in Episode 7: -2.2375222816247096\n",
            "Mean Reward in Episode 8: -1.2243957294350936\n",
            "Mean Reward in Episode 9: -2.312493667103707\n",
            "Mean Reward in Episode 10: -1.9888011437953874\n",
            "Mean Reward in Episode 11: -0.3997055883849347\n",
            "Mean Reward in Episode 12: -1.3623934025764248\n",
            "Mean Reward in Episode 13: -1.7758102034709877\n",
            "Mean Reward in Episode 14: -0.6905324955949412\n",
            "Mean Reward in Episode 15: -3.8638606523069816\n",
            "Mean Reward in Episode 16: -2.3592001259219035\n",
            "Mean Reward in Episode 17: -1.603998667532621\n",
            "Mean Reward in Episode 18: -3.1303029554731214\n",
            "Mean Reward in Episode 19: -0.6827917360489324\n",
            "Mean Reward in Episode 20: -4.270906403474007\n",
            "Mean Reward in Episode 21: -1.7101063856717758\n",
            "Mean Reward in Episode 22: -0.9801247066140978\n",
            "Mean Reward in Episode 23: -3.0774974591792765\n",
            "Mean Reward in Episode 24: -0.5599823824388847\n",
            "Mean Reward in Episode 25: -1.1337451779000274\n",
            "Mean Reward in Episode 26: -3.039075900686031\n",
            "Mean Reward in Episode 27: -1.0122815927093924\n",
            "Mean Reward in Episode 28: -1.6610424062117535\n",
            "Mean Reward in Episode 29: -3.756807196688871\n",
            "Mean Reward in Episode 30: -4.442696272926345\n",
            "Mean Reward in Episode 31: -0.5733679140068938\n",
            "Mean Reward in Episode 32: -1.2514251887097776\n",
            "Mean Reward in Episode 33: -1.67352488921163\n",
            "Mean Reward in Episode 34: -2.6011102405517246\n",
            "Mean Reward in Episode 35: -2.420009332411295\n",
            "Mean Reward in Episode 36: -1.5723198670259713\n",
            "Mean Reward in Episode 37: -0.7056408469457733\n",
            "Mean Reward in Episode 38: -1.1812329900045875\n",
            "Mean Reward in Episode 39: -2.6137746250568945\n",
            "Mean Reward in Episode 40: -1.6893559163990297\n",
            "Mean Reward in Episode 41: -0.9759929694853184\n",
            "Mean Reward in Episode 42: -2.2829741231971954\n",
            "Mean Reward in Episode 43: -1.6847669851964713\n",
            "Mean Reward in Episode 44: -1.5682817840196723\n",
            "Mean Reward in Episode 45: -3.0568478040455203\n",
            "Mean Reward in Episode 46: -0.3582773698443616\n",
            "Mean Reward in Episode 47: -4.144636305358258\n",
            "Mean Reward in Episode 48: -1.4525840593527037\n",
            "Mean Reward in Episode 49: -1.2072197869183623\n",
            "Mean Reward in Episode 50: -2.111721251576475\n",
            "Mean Reward in Episode 51: -1.5219867842243218\n",
            "Mean Reward in Episode 52: -1.2574386184607635\n",
            "Mean Reward in Episode 53: -1.4290341595743632\n",
            "Mean Reward in Episode 54: -1.6826100131809554\n",
            "Mean Reward in Episode 55: -2.3206555559784343\n",
            "Mean Reward in Episode 56: -1.3084267556487486\n",
            "Mean Reward in Episode 57: -4.415319026587046\n",
            "Mean Reward in Episode 58: -1.6249135835758501\n",
            "Mean Reward in Episode 59: -1.6963112836303764\n",
            "Mean Reward in Episode 60: -0.9996197873787729\n",
            "Mean Reward in Episode 61: -2.6579492188222194\n",
            "Mean Reward in Episode 62: -2.127189712101041\n",
            "Mean Reward in Episode 63: -1.4824346223395792\n",
            "Mean Reward in Episode 64: -0.08098583300406376\n",
            "Mean Reward in Episode 65: -1.069169269840691\n",
            "Mean Reward in Episode 66: -2.18898541112221\n",
            "Mean Reward in Episode 67: -2.3318703021136833\n",
            "Mean Reward in Episode 68: -1.0652670934140234\n",
            "Mean Reward in Episode 69: -2.5831417449106135\n",
            "Mean Reward in Episode 70: -2.2307427537205564\n",
            "Mean Reward in Episode 71: -1.7960375967508897\n",
            "Mean Reward in Episode 72: -4.281588946102897\n",
            "Mean Reward in Episode 73: -1.206863592622713\n",
            "Mean Reward in Episode 74: -2.3449155895086093\n",
            "Mean Reward in Episode 75: -4.200684965686672\n",
            "Mean Reward in Episode 76: -0.10024767105620687\n",
            "Mean Reward in Episode 77: -2.37777196920515\n",
            "Mean Reward in Episode 78: -3.1869456945609373\n",
            "Mean Reward in Episode 79: -2.239644770901863\n",
            "Mean Reward in Episode 80: -1.102709023100904\n",
            "Mean Reward in Episode 81: -1.8149936450115849\n",
            "Mean Reward in Episode 82: -0.6994368209432719\n",
            "Mean Reward in Episode 83: -1.8177953476855082\n",
            "Mean Reward in Episode 84: -1.741913361273955\n",
            "Mean Reward in Episode 85: -2.0929979623671726\n",
            "Mean Reward in Episode 86: -1.2508198478423795\n",
            "Mean Reward in Episode 87: -2.5958607103040534\n",
            "Mean Reward in Episode 88: -1.2285602057231733\n",
            "Mean Reward in Episode 89: -4.078173374439882\n",
            "Mean Reward in Episode 90: -4.200375037541534\n",
            "Mean Reward in Episode 91: -1.3597185982296023\n",
            "Mean Reward in Episode 92: -2.9044823640670283\n",
            "Mean Reward in Episode 93: -3.1726080917808774\n",
            "Mean Reward in Episode 94: -1.288689954096398\n",
            "Mean Reward in Episode 95: -2.995262658371916\n",
            "Mean Reward in Episode 96: -1.8563476564075798\n",
            "Mean Reward in Episode 97: -1.731320188989071\n",
            "Mean Reward in Episode 98: -1.4559709496935882\n",
            "Mean Reward in Episode 99: -1.1732696390287662\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2286, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2293, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2391, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2226, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2428, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2223, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3041, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2515, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3043, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3139, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3267, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2346, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3059, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2312, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2050, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2224, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2169, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2222, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2205, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2290, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.9967859487306137\n",
            "Mean Reward in Episode 1: -3.0368789786487325\n",
            "Mean Reward in Episode 2: -3.790335273011358\n",
            "Mean Reward in Episode 3: -1.3141157870170597\n",
            "Mean Reward in Episode 4: -1.479257406636589\n",
            "Mean Reward in Episode 5: -1.1313933500163427\n",
            "Mean Reward in Episode 6: -1.0889860553482766\n",
            "Mean Reward in Episode 7: -4.294427164134467\n",
            "Mean Reward in Episode 8: -1.4956569645927893\n",
            "Mean Reward in Episode 9: -0.8106726957011503\n",
            "Mean Reward in Episode 10: -1.2370460487180188\n",
            "Mean Reward in Episode 11: -2.9190883440937516\n",
            "Mean Reward in Episode 12: -2.1053669735060234\n",
            "Mean Reward in Episode 13: -1.8124627423454303\n",
            "Mean Reward in Episode 14: -2.2971514858702835\n",
            "Mean Reward in Episode 15: -1.1998390391403233\n",
            "Mean Reward in Episode 16: -3.5545609869021026\n",
            "Mean Reward in Episode 17: -1.0057492067149938\n",
            "Mean Reward in Episode 18: -1.316825939293278\n",
            "Mean Reward in Episode 19: -1.269769425153047\n",
            "Mean Reward in Episode 20: -1.3460135380175582\n",
            "Mean Reward in Episode 21: -2.793213445249747\n",
            "Mean Reward in Episode 22: -1.6167326607497146\n",
            "Mean Reward in Episode 23: -1.8826810053733214\n",
            "Mean Reward in Episode 24: -2.8280044287801234\n",
            "Mean Reward in Episode 25: -2.17581156664758\n",
            "Mean Reward in Episode 26: -3.104855681461131\n",
            "Mean Reward in Episode 27: -1.0100591562811665\n",
            "Mean Reward in Episode 28: -1.3893362724250384\n",
            "Mean Reward in Episode 29: -1.5409183550950667\n",
            "Mean Reward in Episode 30: -4.074356283180436\n",
            "Mean Reward in Episode 31: -2.0811436700559462\n",
            "Mean Reward in Episode 32: -1.0704772783073608\n",
            "Mean Reward in Episode 33: -1.4326943472756555\n",
            "Mean Reward in Episode 34: -1.9519898603088999\n",
            "Mean Reward in Episode 35: -2.116420910899816\n",
            "Mean Reward in Episode 36: -1.889993845094984\n",
            "Mean Reward in Episode 37: -3.24748493039255\n",
            "Mean Reward in Episode 38: -0.9289383904157724\n",
            "Mean Reward in Episode 39: -3.2723837289046744\n",
            "Mean Reward in Episode 40: -1.4645779311047893\n",
            "Mean Reward in Episode 41: -4.233438587042576\n",
            "Mean Reward in Episode 42: -1.6113192493543416\n",
            "Mean Reward in Episode 43: -1.358092878860243\n",
            "Mean Reward in Episode 44: -0.224525410792661\n",
            "Mean Reward in Episode 45: -1.3282654968293555\n",
            "Mean Reward in Episode 46: -1.6559326599376218\n",
            "Mean Reward in Episode 47: -0.9897034605036975\n",
            "Mean Reward in Episode 48: -1.220305518135592\n",
            "Mean Reward in Episode 49: -0.5928485145775185\n",
            "Mean Reward in Episode 50: -1.024800121583548\n",
            "Mean Reward in Episode 51: -1.3406071913948454\n",
            "Mean Reward in Episode 52: -4.256930638239962\n",
            "Mean Reward in Episode 53: -2.361614122991762\n",
            "Mean Reward in Episode 54: -2.809295408314545\n",
            "Mean Reward in Episode 55: -4.9589514879443515\n",
            "Mean Reward in Episode 56: -1.3467671158726144\n",
            "Mean Reward in Episode 57: -3.7997403080181376\n",
            "Mean Reward in Episode 58: -1.1082770190139737\n",
            "Mean Reward in Episode 59: -3.722334276958757\n",
            "Mean Reward in Episode 60: -1.7334593477954467\n",
            "Mean Reward in Episode 61: -2.054405176263632\n",
            "Mean Reward in Episode 62: -4.362650194583214\n",
            "Mean Reward in Episode 63: -3.073022900640264\n",
            "Mean Reward in Episode 64: -1.4520069009862784\n",
            "Mean Reward in Episode 65: -1.8078294465783038\n",
            "Mean Reward in Episode 66: -1.7761193696983015\n",
            "Mean Reward in Episode 67: -1.594693181196462\n",
            "Mean Reward in Episode 68: -1.6260477656930592\n",
            "Mean Reward in Episode 69: -3.7536820071672863\n",
            "Mean Reward in Episode 70: -2.6447059783137203\n",
            "Mean Reward in Episode 71: -4.714169079535845\n",
            "Mean Reward in Episode 72: -3.8149328459643477\n",
            "Mean Reward in Episode 73: -1.7574159696036937\n",
            "Mean Reward in Episode 74: -5.396130499329459\n",
            "Mean Reward in Episode 75: -2.8370519323406773\n",
            "Mean Reward in Episode 76: -1.16456693660986\n",
            "Mean Reward in Episode 77: -2.786190689394463\n",
            "Mean Reward in Episode 78: -2.1060618005772644\n",
            "Mean Reward in Episode 79: -2.4154591762761575\n",
            "Mean Reward in Episode 80: -3.486073051091761\n",
            "Mean Reward in Episode 81: -1.0074657467930868\n",
            "Mean Reward in Episode 82: -2.523683168897953\n",
            "Mean Reward in Episode 83: -3.435477902092938\n",
            "Mean Reward in Episode 84: -1.3511465061909682\n",
            "Mean Reward in Episode 85: -1.5957346882837562\n",
            "Mean Reward in Episode 86: -2.4958981171032257\n",
            "Mean Reward in Episode 87: -3.224887592267075\n",
            "Mean Reward in Episode 88: -1.565048824605442\n",
            "Mean Reward in Episode 89: -0.07824567041260319\n",
            "Mean Reward in Episode 90: -1.4309976807565479\n",
            "Mean Reward in Episode 91: -2.6012212201786213\n",
            "Mean Reward in Episode 92: -1.8617016678889327\n",
            "Mean Reward in Episode 93: -0.045342858054157865\n",
            "Mean Reward in Episode 94: -3.3648716742600815\n",
            "Mean Reward in Episode 95: -2.0672729412524347\n",
            "Mean Reward in Episode 96: -1.8819837957684755\n",
            "Mean Reward in Episode 97: -3.8243069779267302\n",
            "Mean Reward in Episode 98: -2.638342995258789\n",
            "Mean Reward in Episode 99: -1.3101276288444534\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2348, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3137, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2475, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3034, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2386, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2170, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2339, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2318, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3060, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2296, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3211, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2209, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3217, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2493, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2454, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3059, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3146, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3025, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3033, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2195, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -2.556571430037139\n",
            "Mean Reward in Episode 1: -0.9740979141125847\n",
            "Mean Reward in Episode 2: -0.6882640249216972\n",
            "Mean Reward in Episode 3: -2.1962614156251217\n",
            "Mean Reward in Episode 4: -3.743661812005634\n",
            "Mean Reward in Episode 5: -1.661115552349115\n",
            "Mean Reward in Episode 6: -1.5753853209213147\n",
            "Mean Reward in Episode 7: -1.1962552203364256\n",
            "Mean Reward in Episode 8: -0.6657957164168932\n",
            "Mean Reward in Episode 9: -1.5142486249678644\n",
            "Mean Reward in Episode 10: -1.0431417023084937\n",
            "Mean Reward in Episode 11: -2.0753258550838396\n",
            "Mean Reward in Episode 12: -1.5021420239771588\n",
            "Mean Reward in Episode 13: -1.361743993503349\n",
            "Mean Reward in Episode 14: -1.3784215644126547\n",
            "Mean Reward in Episode 15: -3.0232188819307075\n",
            "Mean Reward in Episode 16: -1.8896306894145285\n",
            "Mean Reward in Episode 17: -4.081385234321199\n",
            "Mean Reward in Episode 18: -1.8238989807625314\n",
            "Mean Reward in Episode 19: -1.419671222076834\n",
            "Mean Reward in Episode 20: -3.7689064629503117\n",
            "Mean Reward in Episode 21: -4.77690772995061\n",
            "Mean Reward in Episode 22: -0.9852382457626079\n",
            "Mean Reward in Episode 23: -3.7311703072708524\n",
            "Mean Reward in Episode 24: -1.609608691139409\n",
            "Mean Reward in Episode 25: -0.15411696821310525\n",
            "Mean Reward in Episode 26: -2.630628598342556\n",
            "Mean Reward in Episode 27: -2.1413893311787113\n",
            "Mean Reward in Episode 28: -2.2118134133977776\n",
            "Mean Reward in Episode 29: -0.2913284032113097\n",
            "Mean Reward in Episode 30: -3.1810227095016685\n",
            "Mean Reward in Episode 31: -2.5417083171237396\n",
            "Mean Reward in Episode 32: -2.8641386714311596\n",
            "Mean Reward in Episode 33: -2.27307282330295\n",
            "Mean Reward in Episode 34: -1.328898112219505\n",
            "Mean Reward in Episode 35: -1.7769257413275426\n",
            "Mean Reward in Episode 36: -1.1930832577212829\n",
            "Mean Reward in Episode 37: -5.347067886701549\n",
            "Mean Reward in Episode 38: -1.4068016464671633\n",
            "Mean Reward in Episode 39: -3.91528486975597\n",
            "Mean Reward in Episode 40: -2.4324686342064807\n",
            "Mean Reward in Episode 41: -1.952744060514142\n",
            "Mean Reward in Episode 42: -2.953283831257659\n",
            "Mean Reward in Episode 43: -1.0439796926009215\n",
            "Mean Reward in Episode 44: -1.4171212410137848\n",
            "Mean Reward in Episode 45: -3.5517733106968783\n",
            "Mean Reward in Episode 46: -1.3667683581384085\n",
            "Mean Reward in Episode 47: -1.6473625677231452\n",
            "Mean Reward in Episode 48: -3.940953307116788\n",
            "Mean Reward in Episode 49: -1.7172284631938737\n",
            "Mean Reward in Episode 50: -0.8078941564431235\n",
            "Mean Reward in Episode 51: -4.739656654513283\n",
            "Mean Reward in Episode 52: -1.059741063054334\n",
            "Mean Reward in Episode 53: -3.164366513253746\n",
            "Mean Reward in Episode 54: -2.4037337345682674\n",
            "Mean Reward in Episode 55: -0.41658607952291793\n",
            "Mean Reward in Episode 56: -3.6487260825499725\n",
            "Mean Reward in Episode 57: -2.3681495716010925\n",
            "Mean Reward in Episode 58: -2.879670245771661\n",
            "Mean Reward in Episode 59: -1.6372635720500968\n",
            "Mean Reward in Episode 60: -3.432377660411654\n",
            "Mean Reward in Episode 61: -3.0200576145960256\n",
            "Mean Reward in Episode 62: 0.19106388981316916\n",
            "Mean Reward in Episode 63: -3.419897447602457\n",
            "Mean Reward in Episode 64: -2.1663539424039167\n",
            "Mean Reward in Episode 65: -3.325375300920598\n",
            "Mean Reward in Episode 66: -2.7304537856588036\n",
            "Mean Reward in Episode 67: -2.165393688948254\n",
            "Mean Reward in Episode 68: -2.585848331304214\n",
            "Mean Reward in Episode 69: -2.8540976739814745\n",
            "Mean Reward in Episode 70: -0.7790832702371775\n",
            "Mean Reward in Episode 71: -3.74049033607821\n",
            "Mean Reward in Episode 72: -2.618114036081987\n",
            "Mean Reward in Episode 73: -2.2047283432859492\n",
            "Mean Reward in Episode 74: -1.3623306984759915\n",
            "Mean Reward in Episode 75: -0.9571379091138075\n",
            "Mean Reward in Episode 76: -3.972645416756182\n",
            "Mean Reward in Episode 77: -1.4916874381629581\n",
            "Mean Reward in Episode 78: -3.0893701767847115\n",
            "Mean Reward in Episode 79: -1.839291834867531\n",
            "Mean Reward in Episode 80: -2.3594504426572636\n",
            "Mean Reward in Episode 81: -1.2707419367676875\n",
            "Mean Reward in Episode 82: -1.3518087153705218\n",
            "Mean Reward in Episode 83: -1.7107361235285243\n",
            "Mean Reward in Episode 84: -2.7581419866341923\n",
            "Mean Reward in Episode 85: -2.28385287088799\n",
            "Mean Reward in Episode 86: -1.6074588192105488\n",
            "Mean Reward in Episode 87: -1.3318823639480362\n",
            "Mean Reward in Episode 88: -3.2314965291556543\n",
            "Mean Reward in Episode 89: -0.9851607763553729\n",
            "Mean Reward in Episode 90: -1.4666528955010554\n",
            "Mean Reward in Episode 91: -1.2657385685513312\n",
            "Mean Reward in Episode 92: -2.9691269656748958\n",
            "Mean Reward in Episode 93: -3.7123608058019535\n",
            "Mean Reward in Episode 94: -2.4125667627255396\n",
            "Mean Reward in Episode 95: -2.108924903073949\n",
            "Mean Reward in Episode 96: -1.2866085518584793\n",
            "Mean Reward in Episode 97: -1.5250729000504417\n",
            "Mean Reward in Episode 98: -1.703056214510567\n",
            "Mean Reward in Episode 99: -0.029311572687880556\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3081, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3076, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2306, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2162, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2395, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2223, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3117, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2447, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3007, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2319, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3150, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2173, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3263, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2225, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2222, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2384, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2330, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2214, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2348, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2112, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.3483354739388809\n",
            "Mean Reward in Episode 1: -0.8971272347803761\n",
            "Mean Reward in Episode 2: -1.7438368892365397\n",
            "Mean Reward in Episode 3: -2.9647379131362266\n",
            "Mean Reward in Episode 4: -2.172556021985359\n",
            "Mean Reward in Episode 5: -5.025336832360137\n",
            "Mean Reward in Episode 6: -1.2751454499710084\n",
            "Mean Reward in Episode 7: -3.017688909135011\n",
            "Mean Reward in Episode 8: -5.052673700584113\n",
            "Mean Reward in Episode 9: -1.461313804276872\n",
            "Mean Reward in Episode 10: -3.8279475754100774\n",
            "Mean Reward in Episode 11: -2.381435584265713\n",
            "Mean Reward in Episode 12: -2.3898392719463524\n",
            "Mean Reward in Episode 13: -2.74944924778426\n",
            "Mean Reward in Episode 14: -2.958428821870008\n",
            "Mean Reward in Episode 15: -3.8243534628359863\n",
            "Mean Reward in Episode 16: -1.5194512986308613\n",
            "Mean Reward in Episode 17: -1.4287275358534697\n",
            "Mean Reward in Episode 18: -4.125930075595977\n",
            "Mean Reward in Episode 19: -4.007132074033673\n",
            "Mean Reward in Episode 20: -0.35401650769075166\n",
            "Mean Reward in Episode 21: -2.801504250554061\n",
            "Mean Reward in Episode 22: -2.4264941571895036\n",
            "Mean Reward in Episode 23: -1.7954490964849774\n",
            "Mean Reward in Episode 24: -1.9976663163298958\n",
            "Mean Reward in Episode 25: -1.1999918375433534\n",
            "Mean Reward in Episode 26: -1.923983894893474\n",
            "Mean Reward in Episode 27: -1.8699962141905824\n",
            "Mean Reward in Episode 28: -1.3150812916625372\n",
            "Mean Reward in Episode 29: -1.6272150302874246\n",
            "Mean Reward in Episode 30: -0.7704138228781006\n",
            "Mean Reward in Episode 31: -1.103341165199741\n",
            "Mean Reward in Episode 32: -2.565208189725134\n",
            "Mean Reward in Episode 33: -2.7594899153525905\n",
            "Mean Reward in Episode 34: -1.352103360931859\n",
            "Mean Reward in Episode 35: -2.5628899664687643\n",
            "Mean Reward in Episode 36: -0.9129991799483248\n",
            "Mean Reward in Episode 37: -0.9034418809712669\n",
            "Mean Reward in Episode 38: -2.9547821745338037\n",
            "Mean Reward in Episode 39: -0.79965402038427\n",
            "Mean Reward in Episode 40: -5.517148217993328\n",
            "Mean Reward in Episode 41: -1.5907886330226255\n",
            "Mean Reward in Episode 42: -1.5295530775490824\n",
            "Mean Reward in Episode 43: -2.356501308043364\n",
            "Mean Reward in Episode 44: -1.3754758506206082\n",
            "Mean Reward in Episode 45: -2.7097860475564306\n",
            "Mean Reward in Episode 46: -0.9308235405596033\n",
            "Mean Reward in Episode 47: -0.09001309950291625\n",
            "Mean Reward in Episode 48: -1.79577322247542\n",
            "Mean Reward in Episode 49: -2.0298360477563544\n",
            "Mean Reward in Episode 50: -4.491230919375197\n",
            "Mean Reward in Episode 51: -1.9967729922540627\n",
            "Mean Reward in Episode 52: -0.2797798779464899\n",
            "Mean Reward in Episode 53: -1.0066919041083189\n",
            "Mean Reward in Episode 54: -1.1928698528397041\n",
            "Mean Reward in Episode 55: -5.204489747605907\n",
            "Mean Reward in Episode 56: -1.5968099292017472\n",
            "Mean Reward in Episode 57: -1.3945905152192466\n",
            "Mean Reward in Episode 58: -2.5913314616652054\n",
            "Mean Reward in Episode 59: -1.4775001112603094\n",
            "Mean Reward in Episode 60: -1.0841136756687837\n",
            "Mean Reward in Episode 61: -2.2994554206462343\n",
            "Mean Reward in Episode 62: -3.0784019954003914\n",
            "Mean Reward in Episode 63: -2.142763500254105\n",
            "Mean Reward in Episode 64: -3.46944496001273\n",
            "Mean Reward in Episode 65: -3.521900493637295\n",
            "Mean Reward in Episode 66: -3.144450361970773\n",
            "Mean Reward in Episode 67: -1.297961890780381\n",
            "Mean Reward in Episode 68: -2.0824991246639923\n",
            "Mean Reward in Episode 69: -4.877812168236639\n",
            "Mean Reward in Episode 70: -2.4117062106190934\n",
            "Mean Reward in Episode 71: -1.2292911264614221\n",
            "Mean Reward in Episode 72: -1.517298518368987\n",
            "Mean Reward in Episode 73: -1.910571625353892\n",
            "Mean Reward in Episode 74: -1.1764994575102143\n",
            "Mean Reward in Episode 75: -2.791920291970873\n",
            "Mean Reward in Episode 76: -3.424651564170913\n",
            "Mean Reward in Episode 77: -1.746923342808236\n",
            "Mean Reward in Episode 78: -1.5183112373202414\n",
            "Mean Reward in Episode 79: -1.0008510373588921\n",
            "Mean Reward in Episode 80: -2.251205102073749\n",
            "Mean Reward in Episode 81: -5.041117419707566\n",
            "Mean Reward in Episode 82: -1.4590974776827987\n",
            "Mean Reward in Episode 83: -1.436008626779638\n",
            "Mean Reward in Episode 84: -1.6319565569872245\n",
            "Mean Reward in Episode 85: -1.2589985358611286\n",
            "Mean Reward in Episode 86: -1.7147178131568546\n",
            "Mean Reward in Episode 87: -4.468286763136835\n",
            "Mean Reward in Episode 88: -1.134228806065401\n",
            "Mean Reward in Episode 89: -1.4888913324141109\n",
            "Mean Reward in Episode 90: -0.646728441417609\n",
            "Mean Reward in Episode 91: -4.099363707508094\n",
            "Mean Reward in Episode 92: -3.5292488179991115\n",
            "Mean Reward in Episode 93: -1.0924814582156612\n",
            "Mean Reward in Episode 94: -2.5859630413322967\n",
            "Mean Reward in Episode 95: -2.1172898243282665\n",
            "Mean Reward in Episode 96: -1.8089957635144875\n",
            "Mean Reward in Episode 97: -2.4198143274263035\n",
            "Mean Reward in Episode 98: -2.3738681581342083\n",
            "Mean Reward in Episode 99: -1.7489602274552674\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2196, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2125, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2220, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2471, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3044, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2377, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3206, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2357, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2295, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2134, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2123, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2293, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3030, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2483, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2524, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2187, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2315, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3063, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2429, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3038, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.2706952163038627\n",
            "Mean Reward in Episode 1: -2.5027133330603495\n",
            "Mean Reward in Episode 2: -3.6462627552218834\n",
            "Mean Reward in Episode 3: -1.7790959318333173\n",
            "Mean Reward in Episode 4: -1.1563764806496744\n",
            "Mean Reward in Episode 5: -2.521843228814805\n",
            "Mean Reward in Episode 6: -0.9036457102092634\n",
            "Mean Reward in Episode 7: -3.035138623753938\n",
            "Mean Reward in Episode 8: -1.3191588086814507\n",
            "Mean Reward in Episode 9: -1.8056939100543126\n",
            "Mean Reward in Episode 10: -2.6945974954896568\n",
            "Mean Reward in Episode 11: -1.2642157260082914\n",
            "Mean Reward in Episode 12: -0.42659027602476873\n",
            "Mean Reward in Episode 13: -3.5262923735634613\n",
            "Mean Reward in Episode 14: -0.8776650206532998\n",
            "Mean Reward in Episode 15: -3.0086955386853402\n",
            "Mean Reward in Episode 16: -4.155966638042414\n",
            "Mean Reward in Episode 17: -5.235185133661039\n",
            "Mean Reward in Episode 18: -2.7985837218318275\n",
            "Mean Reward in Episode 19: -0.9821930298981056\n",
            "Mean Reward in Episode 20: -3.007860673335176\n",
            "Mean Reward in Episode 21: -1.641545406691167\n",
            "Mean Reward in Episode 22: -1.4295144523373549\n",
            "Mean Reward in Episode 23: -1.5555710418242237\n",
            "Mean Reward in Episode 24: -1.173878010185952\n",
            "Mean Reward in Episode 25: -2.432597941292843\n",
            "Mean Reward in Episode 26: -0.17298258236847044\n",
            "Mean Reward in Episode 27: -1.0465845124683184\n",
            "Mean Reward in Episode 28: -2.04136913398263\n",
            "Mean Reward in Episode 29: -1.3815386026968666\n",
            "Mean Reward in Episode 30: -1.7612428306939263\n",
            "Mean Reward in Episode 31: -3.035440539428052\n",
            "Mean Reward in Episode 32: -3.004345555176587\n",
            "Mean Reward in Episode 33: -1.9758582641041575\n",
            "Mean Reward in Episode 34: -1.7133472360627535\n",
            "Mean Reward in Episode 35: -1.0960907939612892\n",
            "Mean Reward in Episode 36: -0.5769161266742799\n",
            "Mean Reward in Episode 37: -1.2330225083804354\n",
            "Mean Reward in Episode 38: -2.3991192589892507\n",
            "Mean Reward in Episode 39: -1.1548988481208886\n",
            "Mean Reward in Episode 40: -1.1912106349710214\n",
            "Mean Reward in Episode 41: -3.5001243280925896\n",
            "Mean Reward in Episode 42: -2.8936906753822162\n",
            "Mean Reward in Episode 43: -1.9083922022035835\n",
            "Mean Reward in Episode 44: -1.4170066928415606\n",
            "Mean Reward in Episode 45: -1.2949237978160852\n",
            "Mean Reward in Episode 46: -3.7529797496565656\n",
            "Mean Reward in Episode 47: 0.02905621519182978\n",
            "Mean Reward in Episode 48: -1.9603302704988301\n",
            "Mean Reward in Episode 49: -2.3089210236829145\n",
            "Mean Reward in Episode 50: -0.9408522866749887\n",
            "Mean Reward in Episode 51: -1.6143577357673613\n",
            "Mean Reward in Episode 52: -2.743408062887894\n",
            "Mean Reward in Episode 53: -1.5840600142833414\n",
            "Mean Reward in Episode 54: -2.944777696697841\n",
            "Mean Reward in Episode 55: -1.4621139846398137\n",
            "Mean Reward in Episode 56: -1.1801086831400014\n",
            "Mean Reward in Episode 57: -2.121703751294089\n",
            "Mean Reward in Episode 58: -2.452892031370824\n",
            "Mean Reward in Episode 59: -1.3705404341284848\n",
            "Mean Reward in Episode 60: -3.2273412841961844\n",
            "Mean Reward in Episode 61: -1.6265786077934592\n",
            "Mean Reward in Episode 62: -2.419788434073129\n",
            "Mean Reward in Episode 63: -1.2986121895528895\n",
            "Mean Reward in Episode 64: 0.042699623527338526\n",
            "Mean Reward in Episode 65: -2.890674091393345\n",
            "Mean Reward in Episode 66: -1.0941788940057162\n",
            "Mean Reward in Episode 67: -1.1731635994131926\n",
            "Mean Reward in Episode 68: -1.6855752483265722\n",
            "Mean Reward in Episode 69: -2.111475241745699\n",
            "Mean Reward in Episode 70: -1.903801389085314\n",
            "Mean Reward in Episode 71: -1.4912152137748345\n",
            "Mean Reward in Episode 72: -1.822591884071969\n",
            "Mean Reward in Episode 73: -2.2891552618348188\n",
            "Mean Reward in Episode 74: -2.497693678490598\n",
            "Mean Reward in Episode 75: -1.5074822240595211\n",
            "Mean Reward in Episode 76: -1.4009510655936297\n",
            "Mean Reward in Episode 77: -3.886443357349137\n",
            "Mean Reward in Episode 78: -1.7196051335901748\n",
            "Mean Reward in Episode 79: -1.6522942727490868\n",
            "Mean Reward in Episode 80: -2.101239958476357\n",
            "Mean Reward in Episode 81: -3.1980833278016245\n",
            "Mean Reward in Episode 82: -2.9670793103843147\n",
            "Mean Reward in Episode 83: -1.730497234370899\n",
            "Mean Reward in Episode 84: -2.7008125573303\n",
            "Mean Reward in Episode 85: -1.783943795199696\n",
            "Mean Reward in Episode 86: -1.2518605321572307\n",
            "Mean Reward in Episode 87: -1.0811924587899313\n",
            "Mean Reward in Episode 88: -0.7548873752907541\n",
            "Mean Reward in Episode 89: -2.8812068270744393\n",
            "Mean Reward in Episode 90: -1.3835129530529247\n",
            "Mean Reward in Episode 91: -1.8878770604043515\n",
            "Mean Reward in Episode 92: -1.956353938404017\n",
            "Mean Reward in Episode 93: -2.5837564701593205\n",
            "Mean Reward in Episode 94: -1.8970844608386845\n",
            "Mean Reward in Episode 95: -1.3455581308472369\n",
            "Mean Reward in Episode 96: -3.427373845123133\n",
            "Mean Reward in Episode 97: -3.4199265780404486\n",
            "Mean Reward in Episode 98: -2.445983569917591\n",
            "Mean Reward in Episode 99: -2.346510493337745\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3123, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2207, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3285, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2457, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3087, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2157, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2227, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2403, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3117, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2133, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2526, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3047, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3027, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2377, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2192, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2070, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2240, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2131, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3140, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2231, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.0277923499147374\n",
            "Mean Reward in Episode 1: -1.6655775435513254\n",
            "Mean Reward in Episode 2: -0.7990085534559054\n",
            "Mean Reward in Episode 3: -1.7949247036791438\n",
            "Mean Reward in Episode 4: -1.2364691075927838\n",
            "Mean Reward in Episode 5: -1.508254341416893\n",
            "Mean Reward in Episode 6: -6.594430321089317\n",
            "Mean Reward in Episode 7: -0.9281364191549326\n",
            "Mean Reward in Episode 8: -1.2983798074788642\n",
            "Mean Reward in Episode 9: -1.5512201458929706\n",
            "Mean Reward in Episode 10: -1.386076326818093\n",
            "Mean Reward in Episode 11: -2.4683002461819874\n",
            "Mean Reward in Episode 12: -2.5139733246298492\n",
            "Mean Reward in Episode 13: -1.0352061727664104\n",
            "Mean Reward in Episode 14: -1.0018191841546\n",
            "Mean Reward in Episode 15: -1.6264483452176224\n",
            "Mean Reward in Episode 16: -2.3425999763628598\n",
            "Mean Reward in Episode 17: -1.5210177781020675\n",
            "Mean Reward in Episode 18: -1.0948717457808184\n",
            "Mean Reward in Episode 19: -3.242001704047505\n",
            "Mean Reward in Episode 20: -0.8667547899001731\n",
            "Mean Reward in Episode 21: -1.6442309305489362\n",
            "Mean Reward in Episode 22: -2.3273322474652165\n",
            "Mean Reward in Episode 23: -1.3627375133086554\n",
            "Mean Reward in Episode 24: -2.1900211704368453\n",
            "Mean Reward in Episode 25: -1.2743652884166308\n",
            "Mean Reward in Episode 26: -3.7799848197285786\n",
            "Mean Reward in Episode 27: 0.5356389804268616\n",
            "Mean Reward in Episode 28: -1.2178231895556815\n",
            "Mean Reward in Episode 29: -1.577744725770545\n",
            "Mean Reward in Episode 30: -1.6150965242502335\n",
            "Mean Reward in Episode 31: -3.278875514108169\n",
            "Mean Reward in Episode 32: -1.5933424224746562\n",
            "Mean Reward in Episode 33: -0.6788367436166751\n",
            "Mean Reward in Episode 34: -2.325432529698713\n",
            "Mean Reward in Episode 35: -1.3671251754529399\n",
            "Mean Reward in Episode 36: -1.6543186676333927\n",
            "Mean Reward in Episode 37: -2.3004680768908976\n",
            "Mean Reward in Episode 38: -0.6394655006268314\n",
            "Mean Reward in Episode 39: -4.098643902615417\n",
            "Mean Reward in Episode 40: -1.353708820324044\n",
            "Mean Reward in Episode 41: -1.9736468533342708\n",
            "Mean Reward in Episode 42: -1.4263212987761216\n",
            "Mean Reward in Episode 43: -0.4673738003386927\n",
            "Mean Reward in Episode 44: -4.914600298791488\n",
            "Mean Reward in Episode 45: -1.4781028168911643\n",
            "Mean Reward in Episode 46: -1.8495501223975739\n",
            "Mean Reward in Episode 47: -2.3198336051332222\n",
            "Mean Reward in Episode 48: -0.8036680162131172\n",
            "Mean Reward in Episode 49: -1.2096451071209153\n",
            "Mean Reward in Episode 50: -1.815207958975687\n",
            "Mean Reward in Episode 51: -1.5614426778442176\n",
            "Mean Reward in Episode 52: -2.1466511746465846\n",
            "Mean Reward in Episode 53: -1.8791855879324642\n",
            "Mean Reward in Episode 54: -1.2197239273809626\n",
            "Mean Reward in Episode 55: -3.6211026450636865\n",
            "Mean Reward in Episode 56: -1.2039748581638399\n",
            "Mean Reward in Episode 57: -1.1237719973785374\n",
            "Mean Reward in Episode 58: -0.6240984980107369\n",
            "Mean Reward in Episode 59: -3.5397833434621564\n",
            "Mean Reward in Episode 60: -1.2396674932333052\n",
            "Mean Reward in Episode 61: -2.5291598786044815\n",
            "Mean Reward in Episode 62: -2.075965481894398\n",
            "Mean Reward in Episode 63: -1.1248299466936527\n",
            "Mean Reward in Episode 64: -1.5040481786179012\n",
            "Mean Reward in Episode 65: -1.2954130761697784\n",
            "Mean Reward in Episode 66: -3.130697740698447\n",
            "Mean Reward in Episode 67: -4.752357632484299\n",
            "Mean Reward in Episode 68: -0.997330212849632\n",
            "Mean Reward in Episode 69: -1.4477029812885893\n",
            "Mean Reward in Episode 70: -2.2736290937500323\n",
            "Mean Reward in Episode 71: -3.214704755765727\n",
            "Mean Reward in Episode 72: -1.87792383197485\n",
            "Mean Reward in Episode 73: -1.1909103799227285\n",
            "Mean Reward in Episode 74: -1.2022805210147762\n",
            "Mean Reward in Episode 75: -3.842256486968174\n",
            "Mean Reward in Episode 76: -1.6119781753046831\n",
            "Mean Reward in Episode 77: -1.5619118480697172\n",
            "Mean Reward in Episode 78: -0.5916251342530348\n",
            "Mean Reward in Episode 79: -2.263232305680183\n",
            "Mean Reward in Episode 80: -0.8656493141054838\n",
            "Mean Reward in Episode 81: -1.74633058289518\n",
            "Mean Reward in Episode 82: -3.2747271500614845\n",
            "Mean Reward in Episode 83: -3.9539739693689255\n",
            "Mean Reward in Episode 84: -1.73256208563365\n",
            "Mean Reward in Episode 85: -1.8222131766677858\n",
            "Mean Reward in Episode 86: -1.1486853341619094\n",
            "Mean Reward in Episode 87: -1.1854251839391337\n",
            "Mean Reward in Episode 88: -1.6739406497957838\n",
            "Mean Reward in Episode 89: -3.2990652188200356\n",
            "Mean Reward in Episode 90: -3.605156007684603\n",
            "Mean Reward in Episode 91: -1.0798283721906912\n",
            "Mean Reward in Episode 92: -4.430726867239662\n",
            "Mean Reward in Episode 93: -1.0070276583815683\n",
            "Mean Reward in Episode 94: -3.204679916526234\n",
            "Mean Reward in Episode 95: -5.953498808672333\n",
            "Mean Reward in Episode 96: -2.6024272028176494\n",
            "Mean Reward in Episode 97: -3.9347607606399317\n",
            "Mean Reward in Episode 98: -4.627470474672463\n",
            "Mean Reward in Episode 99: -1.3727121157037654\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3139, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3138, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2303, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3114, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3109, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2369, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3120, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2157, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3049, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2350, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3032, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3006, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2494, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3061, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2215, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3052, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3029, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2387, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3044, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2213, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.3228492425764098\n",
            "Mean Reward in Episode 1: -1.218395730670149\n",
            "Mean Reward in Episode 2: -0.9572765588711785\n",
            "Mean Reward in Episode 3: -2.598655829076895\n",
            "Mean Reward in Episode 4: -1.8951951240358837\n",
            "Mean Reward in Episode 5: -3.2566351819796093\n",
            "Mean Reward in Episode 6: -1.328739958883927\n",
            "Mean Reward in Episode 7: -1.1550029398715425\n",
            "Mean Reward in Episode 8: -4.6075466620243954\n",
            "Mean Reward in Episode 9: -1.5929581161557849\n",
            "Mean Reward in Episode 10: -2.7376033248028357\n",
            "Mean Reward in Episode 11: -0.9648276461490117\n",
            "Mean Reward in Episode 12: -2.645680544180326\n",
            "Mean Reward in Episode 13: -1.7406667355127183\n",
            "Mean Reward in Episode 14: -1.0018621824928347\n",
            "Mean Reward in Episode 15: -3.960163134146959\n",
            "Mean Reward in Episode 16: -2.967446423764404\n",
            "Mean Reward in Episode 17: -2.584573995616545\n",
            "Mean Reward in Episode 18: -2.3910376591553506\n",
            "Mean Reward in Episode 19: -2.563128321536226\n",
            "Mean Reward in Episode 20: -1.971278193556226\n",
            "Mean Reward in Episode 21: -3.7162248154859574\n",
            "Mean Reward in Episode 22: -2.419821890064833\n",
            "Mean Reward in Episode 23: -4.960765106061891\n",
            "Mean Reward in Episode 24: -1.2098656576691496\n",
            "Mean Reward in Episode 25: 0.017711533373620875\n",
            "Mean Reward in Episode 26: -2.6130580598384867\n",
            "Mean Reward in Episode 27: -2.9899630433497326\n",
            "Mean Reward in Episode 28: -1.0488995868066129\n",
            "Mean Reward in Episode 29: -3.040600495702364\n",
            "Mean Reward in Episode 30: -2.0242808404720583\n",
            "Mean Reward in Episode 31: -5.090189195730915\n",
            "Mean Reward in Episode 32: -1.8828406971984013\n",
            "Mean Reward in Episode 33: -2.9601582867297287\n",
            "Mean Reward in Episode 34: -2.889617959769385\n",
            "Mean Reward in Episode 35: -2.1158215635962683\n",
            "Mean Reward in Episode 36: -1.2291848175141904\n",
            "Mean Reward in Episode 37: -1.9361401385670516\n",
            "Mean Reward in Episode 38: -2.0913068740686995\n",
            "Mean Reward in Episode 39: -2.0894925207356883\n",
            "Mean Reward in Episode 40: -5.171409540868708\n",
            "Mean Reward in Episode 41: -2.6956239504074047\n",
            "Mean Reward in Episode 42: -3.5299143158871344\n",
            "Mean Reward in Episode 43: -1.638988990152397\n",
            "Mean Reward in Episode 44: -1.1544701595501305\n",
            "Mean Reward in Episode 45: -3.5331630666868246\n",
            "Mean Reward in Episode 46: -1.449415268846135\n",
            "Mean Reward in Episode 47: -3.0314815429699142\n",
            "Mean Reward in Episode 48: -2.1082017520351273\n",
            "Mean Reward in Episode 49: -3.3204966098065047\n",
            "Mean Reward in Episode 50: -3.418717581556983\n",
            "Mean Reward in Episode 51: -2.3498201581121148\n",
            "Mean Reward in Episode 52: -0.5670377098241918\n",
            "Mean Reward in Episode 53: -3.0270135710117394\n",
            "Mean Reward in Episode 54: -2.2927043968412435\n",
            "Mean Reward in Episode 55: -1.0647829483526694\n",
            "Mean Reward in Episode 56: -2.7548862483078036\n",
            "Mean Reward in Episode 57: -1.930229764910202\n",
            "Mean Reward in Episode 58: -3.151015526254505\n",
            "Mean Reward in Episode 59: -2.173034994235287\n",
            "Mean Reward in Episode 60: -1.6002422667014973\n",
            "Mean Reward in Episode 61: -1.4639616792722088\n",
            "Mean Reward in Episode 62: -1.6432076660716357\n",
            "Mean Reward in Episode 63: -2.6243573633615256\n",
            "Mean Reward in Episode 64: -1.3759997955719998\n",
            "Mean Reward in Episode 65: -4.373802497517578\n",
            "Mean Reward in Episode 66: -4.554960703122321\n",
            "Mean Reward in Episode 67: -4.1970893453633025\n",
            "Mean Reward in Episode 68: -0.9341841593313979\n",
            "Mean Reward in Episode 69: -2.0025546062893897\n",
            "Mean Reward in Episode 70: -1.658747805260779\n",
            "Mean Reward in Episode 71: -3.8995605213113618\n",
            "Mean Reward in Episode 72: -0.43507915862971286\n",
            "Mean Reward in Episode 73: -3.0707482687266983\n",
            "Mean Reward in Episode 74: -2.5097280776583744\n",
            "Mean Reward in Episode 75: -1.2201532079809858\n",
            "Mean Reward in Episode 76: -2.8664437267979945\n",
            "Mean Reward in Episode 77: -1.39387489257804\n",
            "Mean Reward in Episode 78: -4.131441864402111\n",
            "Mean Reward in Episode 79: -1.20549745168139\n",
            "Mean Reward in Episode 80: -5.216345798577743\n",
            "Mean Reward in Episode 81: -3.551194165835626\n",
            "Mean Reward in Episode 82: -0.9715262682613095\n",
            "Mean Reward in Episode 83: -1.3753873756980686\n",
            "Mean Reward in Episode 84: -0.8637644536625354\n",
            "Mean Reward in Episode 85: -1.4823751203920978\n",
            "Mean Reward in Episode 86: -3.8776905592309956\n",
            "Mean Reward in Episode 87: -4.6409039620323185\n",
            "Mean Reward in Episode 88: -1.1022962498627722\n",
            "Mean Reward in Episode 89: -0.8612809541761526\n",
            "Mean Reward in Episode 90: 0.33369044096211553\n",
            "Mean Reward in Episode 91: -0.8604718577624376\n",
            "Mean Reward in Episode 92: -1.344713840196618\n",
            "Mean Reward in Episode 93: -2.4245861025772375\n",
            "Mean Reward in Episode 94: -3.995128407378402\n",
            "Mean Reward in Episode 95: -1.0812499485992464\n",
            "Mean Reward in Episode 96: -3.1308998254954505\n",
            "Mean Reward in Episode 97: -3.2888859219714996\n",
            "Mean Reward in Episode 98: -0.11491252133199738\n",
            "Mean Reward in Episode 99: -1.0394901666438212\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3072, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2315, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3072, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3145, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2157, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2420, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3114, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3107, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3311, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3020, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2181, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2382, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3097, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2285, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3244, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2465, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2368, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2211, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3069, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3120, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -0.822898384969595\n",
            "Mean Reward in Episode 1: -1.100308010322521\n",
            "Mean Reward in Episode 2: -1.720211696831563\n",
            "Mean Reward in Episode 3: 0.3806940907010612\n",
            "Mean Reward in Episode 4: -2.0345281774553188\n",
            "Mean Reward in Episode 5: -0.7089539387341453\n",
            "Mean Reward in Episode 6: -1.4084644701048226\n",
            "Mean Reward in Episode 7: -1.164542433619856\n",
            "Mean Reward in Episode 8: -1.2537192358642166\n",
            "Mean Reward in Episode 9: -0.12422324724933755\n",
            "Mean Reward in Episode 10: -1.705208734750927\n",
            "Mean Reward in Episode 11: -0.2350026052544006\n",
            "Mean Reward in Episode 12: -2.95501381325507\n",
            "Mean Reward in Episode 13: -1.3284558457741684\n",
            "Mean Reward in Episode 14: -1.6101778990189126\n",
            "Mean Reward in Episode 15: -1.5840465410452123\n",
            "Mean Reward in Episode 16: -0.6082624306952239\n",
            "Mean Reward in Episode 17: -2.4900979190695223\n",
            "Mean Reward in Episode 18: -2.8377619815447956\n",
            "Mean Reward in Episode 19: -1.4080925537379068\n",
            "Mean Reward in Episode 20: -0.5541574802924303\n",
            "Mean Reward in Episode 21: -2.3756013888424286\n",
            "Mean Reward in Episode 22: -1.97882623506424\n",
            "Mean Reward in Episode 23: -1.5536534513336064\n",
            "Mean Reward in Episode 24: -1.5097729511526818\n",
            "Mean Reward in Episode 25: -2.082090810463751\n",
            "Mean Reward in Episode 26: -2.6226313079049484\n",
            "Mean Reward in Episode 27: -2.409896928117464\n",
            "Mean Reward in Episode 28: -1.459589070239164\n",
            "Mean Reward in Episode 29: -3.170986185193083\n",
            "Mean Reward in Episode 30: -1.3145620444744486\n",
            "Mean Reward in Episode 31: -2.6259768449072367\n",
            "Mean Reward in Episode 32: -1.7809587211970632\n",
            "Mean Reward in Episode 33: -1.6090018295928064\n",
            "Mean Reward in Episode 34: -2.439603232040167\n",
            "Mean Reward in Episode 35: -1.382544435671142\n",
            "Mean Reward in Episode 36: -2.164731763341445\n",
            "Mean Reward in Episode 37: -4.0631654239439845\n",
            "Mean Reward in Episode 38: -3.872980446588571\n",
            "Mean Reward in Episode 39: -2.038595556550505\n",
            "Mean Reward in Episode 40: -3.301029135505315\n",
            "Mean Reward in Episode 41: -2.0663783417702417\n",
            "Mean Reward in Episode 42: -1.304593484706574\n",
            "Mean Reward in Episode 43: -1.0062387375817574\n",
            "Mean Reward in Episode 44: -1.9778191584077207\n",
            "Mean Reward in Episode 45: -4.516660532077869\n",
            "Mean Reward in Episode 46: -1.3397716672481887\n",
            "Mean Reward in Episode 47: -2.7881023897976154\n",
            "Mean Reward in Episode 48: -3.0986944817870783\n",
            "Mean Reward in Episode 49: -1.4846988872035842\n",
            "Mean Reward in Episode 50: -1.2666601406350235\n",
            "Mean Reward in Episode 51: -3.2842503776161722\n",
            "Mean Reward in Episode 52: -1.9921616601488843\n",
            "Mean Reward in Episode 53: -0.06439993406046883\n",
            "Mean Reward in Episode 54: -1.0394008383855917\n",
            "Mean Reward in Episode 55: -1.375514094176177\n",
            "Mean Reward in Episode 56: -2.135536918245368\n",
            "Mean Reward in Episode 57: -3.0413826157187613\n",
            "Mean Reward in Episode 58: -3.3074835473510804\n",
            "Mean Reward in Episode 59: -1.0179401918151878\n",
            "Mean Reward in Episode 60: -1.7788371875622424\n",
            "Mean Reward in Episode 61: -1.1427101538607887\n",
            "Mean Reward in Episode 62: -3.237767167904238\n",
            "Mean Reward in Episode 63: -1.0822036707049338\n",
            "Mean Reward in Episode 64: -1.2117318083078912\n",
            "Mean Reward in Episode 65: -1.4826532769641023\n",
            "Mean Reward in Episode 66: -1.4504896006326804\n",
            "Mean Reward in Episode 67: -3.2427657907679093\n",
            "Mean Reward in Episode 68: -1.952610876220587\n",
            "Mean Reward in Episode 69: -1.8556702496727508\n",
            "Mean Reward in Episode 70: -1.3391442587222868\n",
            "Mean Reward in Episode 71: -1.9092918196899575\n",
            "Mean Reward in Episode 72: -3.3932090782330997\n",
            "Mean Reward in Episode 73: -2.1081192765708976\n",
            "Mean Reward in Episode 74: -3.8158027340767986\n",
            "Mean Reward in Episode 75: -1.5013465957615373\n",
            "Mean Reward in Episode 76: -4.081855404505356\n",
            "Mean Reward in Episode 77: -0.9692131361374536\n",
            "Mean Reward in Episode 78: -3.686574777038053\n",
            "Mean Reward in Episode 79: -1.6571707992003193\n",
            "Mean Reward in Episode 80: -4.086678790587341\n",
            "Mean Reward in Episode 81: -1.6509317027100652\n",
            "Mean Reward in Episode 82: -1.269922378984997\n",
            "Mean Reward in Episode 83: -2.9041881795596707\n",
            "Mean Reward in Episode 84: -1.6299937250336942\n",
            "Mean Reward in Episode 85: -1.0576183146242402\n",
            "Mean Reward in Episode 86: -1.9664584695882852\n",
            "Mean Reward in Episode 87: -2.06730159590906\n",
            "Mean Reward in Episode 88: -1.6029669163814755\n",
            "Mean Reward in Episode 89: -1.252293188628494\n",
            "Mean Reward in Episode 90: -4.946984282119861\n",
            "Mean Reward in Episode 91: -0.899621827000802\n",
            "Mean Reward in Episode 92: -0.886834295066151\n",
            "Mean Reward in Episode 93: -2.5268433306828992\n",
            "Mean Reward in Episode 94: -1.8267710845745688\n",
            "Mean Reward in Episode 95: -1.1134047916288041\n",
            "Mean Reward in Episode 96: -1.5251460505303847\n",
            "Mean Reward in Episode 97: -3.517484738148737\n",
            "Mean Reward in Episode 98: -3.761606856612758\n",
            "Mean Reward in Episode 99: -2.31595375683222\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2142, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2295, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2394, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2327, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2405, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2214, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2332, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3113, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2434, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3247, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2167, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2472, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2493, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3037, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2546, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3023, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3042, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2297, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2345, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2211, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.3357257353534806\n",
            "Mean Reward in Episode 1: -2.852889411548375\n",
            "Mean Reward in Episode 2: -1.4671898833531896\n",
            "Mean Reward in Episode 3: -2.8231977409823825\n",
            "Mean Reward in Episode 4: -3.2832185550468664\n",
            "Mean Reward in Episode 5: -0.008493718160040005\n",
            "Mean Reward in Episode 6: -1.897280788196376\n",
            "Mean Reward in Episode 7: -1.2813782470366188\n",
            "Mean Reward in Episode 8: -1.6313359051413356\n",
            "Mean Reward in Episode 9: -4.43970261192966\n",
            "Mean Reward in Episode 10: -3.5192301824565675\n",
            "Mean Reward in Episode 11: -4.06175699729686\n",
            "Mean Reward in Episode 12: -1.2239193233473429\n",
            "Mean Reward in Episode 13: -1.112239143672495\n",
            "Mean Reward in Episode 14: -1.4161966846941727\n",
            "Mean Reward in Episode 15: -1.3354866452491798\n",
            "Mean Reward in Episode 16: -3.3445808932962358\n",
            "Mean Reward in Episode 17: -3.0147880908749416\n",
            "Mean Reward in Episode 18: -1.3023989578332533\n",
            "Mean Reward in Episode 19: -0.03212376058221808\n",
            "Mean Reward in Episode 20: -3.1498759014823485\n",
            "Mean Reward in Episode 21: 0.004018842579318094\n",
            "Mean Reward in Episode 22: -1.9225810391837914\n",
            "Mean Reward in Episode 23: -2.0754519301669125\n",
            "Mean Reward in Episode 24: 0.05174258241224809\n",
            "Mean Reward in Episode 25: -1.3603893687286983\n",
            "Mean Reward in Episode 26: -5.814077696042282\n",
            "Mean Reward in Episode 27: -1.3994018086671962\n",
            "Mean Reward in Episode 28: -3.5101158699700044\n",
            "Mean Reward in Episode 29: -3.6532849950053143\n",
            "Mean Reward in Episode 30: -3.749356981885094\n",
            "Mean Reward in Episode 31: -4.152101823897427\n",
            "Mean Reward in Episode 32: -3.769300153690029\n",
            "Mean Reward in Episode 33: -1.236091190885327\n",
            "Mean Reward in Episode 34: -0.8533856288268777\n",
            "Mean Reward in Episode 35: -2.067594390696703\n",
            "Mean Reward in Episode 36: -1.5133642230853792\n",
            "Mean Reward in Episode 37: -1.5364410537694022\n",
            "Mean Reward in Episode 38: -2.792588967360429\n",
            "Mean Reward in Episode 39: -2.9349330040563864\n",
            "Mean Reward in Episode 40: -2.0796827838547443\n",
            "Mean Reward in Episode 41: -1.4439002966780323\n",
            "Mean Reward in Episode 42: -1.6046801693090522\n",
            "Mean Reward in Episode 43: -5.429598112079472\n",
            "Mean Reward in Episode 44: -3.779131164622098\n",
            "Mean Reward in Episode 45: -1.8694993645785318\n",
            "Mean Reward in Episode 46: -2.1546413105980045\n",
            "Mean Reward in Episode 47: -0.9975670896283265\n",
            "Mean Reward in Episode 48: -1.0321684948400982\n",
            "Mean Reward in Episode 49: -1.8981859606316192\n",
            "Mean Reward in Episode 50: -2.0558989017651905\n",
            "Mean Reward in Episode 51: -0.8263462037829813\n",
            "Mean Reward in Episode 52: 0.18184840854244083\n",
            "Mean Reward in Episode 53: -1.3852458923697577\n",
            "Mean Reward in Episode 54: -0.7132306346617489\n",
            "Mean Reward in Episode 55: -2.7076349216178976\n",
            "Mean Reward in Episode 56: -2.5201658491488046\n",
            "Mean Reward in Episode 57: -0.9035329452736195\n",
            "Mean Reward in Episode 58: -2.1229650314545436\n",
            "Mean Reward in Episode 59: -0.1718680551144924\n",
            "Mean Reward in Episode 60: -2.936884332778379\n",
            "Mean Reward in Episode 61: -1.767184077182322\n",
            "Mean Reward in Episode 62: -3.395034320771521\n",
            "Mean Reward in Episode 63: -1.9401720753638503\n",
            "Mean Reward in Episode 64: 0.20060477989293438\n",
            "Mean Reward in Episode 65: -3.4895379698912707\n",
            "Mean Reward in Episode 66: -2.0298075185920235\n",
            "Mean Reward in Episode 67: -1.795065156969473\n",
            "Mean Reward in Episode 68: -1.1773375451530392\n",
            "Mean Reward in Episode 69: -1.3284729681454963\n",
            "Mean Reward in Episode 70: -2.119574675324274\n",
            "Mean Reward in Episode 71: -2.1064421956607884\n",
            "Mean Reward in Episode 72: -1.8558226242349736\n",
            "Mean Reward in Episode 73: -0.8936050478897396\n",
            "Mean Reward in Episode 74: -1.1056876918408023\n",
            "Mean Reward in Episode 75: -1.6442991049471036\n",
            "Mean Reward in Episode 76: -1.2764251819944379\n",
            "Mean Reward in Episode 77: -1.206635282751136\n",
            "Mean Reward in Episode 78: -1.3772270267223734\n",
            "Mean Reward in Episode 79: -2.811550064264126\n",
            "Mean Reward in Episode 80: -5.045213250128377\n",
            "Mean Reward in Episode 81: -1.6460881125966027\n",
            "Mean Reward in Episode 82: -1.1796406494290572\n",
            "Mean Reward in Episode 83: -4.663531913381288\n",
            "Mean Reward in Episode 84: -1.1913686946917534\n",
            "Mean Reward in Episode 85: -3.1422590266354407\n",
            "Mean Reward in Episode 86: -3.1798977960437718\n",
            "Mean Reward in Episode 87: -1.7160137752598013\n",
            "Mean Reward in Episode 88: -2.2448739391302115\n",
            "Mean Reward in Episode 89: -0.7389684779632298\n",
            "Mean Reward in Episode 90: -2.3969362706016564\n",
            "Mean Reward in Episode 91: -1.769303082884113\n",
            "Mean Reward in Episode 92: -3.2514368849564863\n",
            "Mean Reward in Episode 93: -2.822230482137578\n",
            "Mean Reward in Episode 94: -4.690943840990445\n",
            "Mean Reward in Episode 95: -5.609974989259544\n",
            "Mean Reward in Episode 96: -0.9698244917949727\n",
            "Mean Reward in Episode 97: -1.4500373465029521\n",
            "Mean Reward in Episode 98: -1.6774147108838646\n",
            "Mean Reward in Episode 99: -1.5504298533113403\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2220, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3073, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3098, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2129, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3053, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3068, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2216, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3080, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2187, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2166, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2479, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2425, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2225, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3335, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3013, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3028, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3078, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2329, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2150, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3072, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.590364566049224\n",
            "Mean Reward in Episode 1: -3.1321273561542236\n",
            "Mean Reward in Episode 2: -1.589018268235066\n",
            "Mean Reward in Episode 3: -1.862261883874838\n",
            "Mean Reward in Episode 4: -1.5605148115135326\n",
            "Mean Reward in Episode 5: -1.2834088362551164\n",
            "Mean Reward in Episode 6: -1.6587457061245678\n",
            "Mean Reward in Episode 7: -1.8199722609620246\n",
            "Mean Reward in Episode 8: -5.616505644710049\n",
            "Mean Reward in Episode 9: -0.9102448557311417\n",
            "Mean Reward in Episode 10: -0.9881485247265374\n",
            "Mean Reward in Episode 11: -1.6383758889576667\n",
            "Mean Reward in Episode 12: -2.479598693079938\n",
            "Mean Reward in Episode 13: -1.1989505476187632\n",
            "Mean Reward in Episode 14: -1.7280535412771985\n",
            "Mean Reward in Episode 15: -0.961534448773477\n",
            "Mean Reward in Episode 16: -0.26400599058735974\n",
            "Mean Reward in Episode 17: -3.261580024739148\n",
            "Mean Reward in Episode 18: -4.088383731344559\n",
            "Mean Reward in Episode 19: -4.3727131479645704\n",
            "Mean Reward in Episode 20: -2.7076753486866108\n",
            "Mean Reward in Episode 21: -2.111752550819332\n",
            "Mean Reward in Episode 22: -3.1041119463274884\n",
            "Mean Reward in Episode 23: -3.7644028873425985\n",
            "Mean Reward in Episode 24: -3.2185424164164043\n",
            "Mean Reward in Episode 25: -1.1805967441042686\n",
            "Mean Reward in Episode 26: -0.5858864292465933\n",
            "Mean Reward in Episode 27: -1.2428809551678974\n",
            "Mean Reward in Episode 28: -4.158361829160843\n",
            "Mean Reward in Episode 29: -1.4638167275039395\n",
            "Mean Reward in Episode 30: -4.980565110231452\n",
            "Mean Reward in Episode 31: -1.5134193384408523\n",
            "Mean Reward in Episode 32: -1.3074768290786607\n",
            "Mean Reward in Episode 33: -3.1612735803592154\n",
            "Mean Reward in Episode 34: -0.7377833135264247\n",
            "Mean Reward in Episode 35: -2.7568178362629503\n",
            "Mean Reward in Episode 36: -2.003193190780568\n",
            "Mean Reward in Episode 37: -2.985416785351407\n",
            "Mean Reward in Episode 38: -1.340880244980466\n",
            "Mean Reward in Episode 39: -1.6736067920205733\n",
            "Mean Reward in Episode 40: -2.2219228762705927\n",
            "Mean Reward in Episode 41: -2.634439726478992\n",
            "Mean Reward in Episode 42: -2.496868644142376\n",
            "Mean Reward in Episode 43: -1.8476188198789125\n",
            "Mean Reward in Episode 44: -0.6358037913832739\n",
            "Mean Reward in Episode 45: -1.1309126001241494\n",
            "Mean Reward in Episode 46: -1.5966118211970468\n",
            "Mean Reward in Episode 47: -3.8170635699965043\n",
            "Mean Reward in Episode 48: -1.4342473019085307\n",
            "Mean Reward in Episode 49: -1.3323398975095977\n",
            "Mean Reward in Episode 50: -1.2537808593042061\n",
            "Mean Reward in Episode 51: -0.9795767699383938\n",
            "Mean Reward in Episode 52: -2.826859455504526\n",
            "Mean Reward in Episode 53: -3.80561637412281\n",
            "Mean Reward in Episode 54: -1.9441674711533652\n",
            "Mean Reward in Episode 55: 0.05793348591099123\n",
            "Mean Reward in Episode 56: -1.5952384846065313\n",
            "Mean Reward in Episode 57: -3.0474093670424245\n",
            "Mean Reward in Episode 58: -4.1154778114644985\n",
            "Mean Reward in Episode 59: -1.4703825775286434\n",
            "Mean Reward in Episode 60: -2.071606194503579\n",
            "Mean Reward in Episode 61: 0.5267419686193564\n",
            "Mean Reward in Episode 62: -1.4901711022137483\n",
            "Mean Reward in Episode 63: -2.9383465528985284\n",
            "Mean Reward in Episode 64: -1.9863277189282218\n",
            "Mean Reward in Episode 65: -1.8336183567846747\n",
            "Mean Reward in Episode 66: -3.1831152351999723\n",
            "Mean Reward in Episode 67: -1.4329517130527518\n",
            "Mean Reward in Episode 68: -1.051580877761246\n",
            "Mean Reward in Episode 69: -5.0358208132255164\n",
            "Mean Reward in Episode 70: -1.7961917091601094\n",
            "Mean Reward in Episode 71: -1.3133454787694205\n",
            "Mean Reward in Episode 72: -0.9444827113179459\n",
            "Mean Reward in Episode 73: -1.4775568687855758\n",
            "Mean Reward in Episode 74: -1.6331377157868068\n",
            "Mean Reward in Episode 75: -1.2857602606476386\n",
            "Mean Reward in Episode 76: -2.7159758739502475\n",
            "Mean Reward in Episode 77: -3.9002963499470735\n",
            "Mean Reward in Episode 78: -3.7535386609803916\n",
            "Mean Reward in Episode 79: -3.756900752059077\n",
            "Mean Reward in Episode 80: 0.3599003944152617\n",
            "Mean Reward in Episode 81: -2.013332727900104\n",
            "Mean Reward in Episode 82: -3.188237855150253\n",
            "Mean Reward in Episode 83: -2.918167037260876\n",
            "Mean Reward in Episode 84: -0.9308650806962221\n",
            "Mean Reward in Episode 85: -1.2378270921332166\n",
            "Mean Reward in Episode 86: -1.49713201357752\n",
            "Mean Reward in Episode 87: -1.1163342189477923\n",
            "Mean Reward in Episode 88: -1.6704570865140642\n",
            "Mean Reward in Episode 89: -2.372917880452711\n",
            "Mean Reward in Episode 90: -5.134339151857739\n",
            "Mean Reward in Episode 91: -3.428250113550743\n",
            "Mean Reward in Episode 92: -1.946357592398729\n",
            "Mean Reward in Episode 93: -1.3792771195667899\n",
            "Mean Reward in Episode 94: -1.6352201416430754\n",
            "Mean Reward in Episode 95: -2.260582500608052\n",
            "Mean Reward in Episode 96: -1.3141102391152257\n",
            "Mean Reward in Episode 97: -2.8348349538561415\n",
            "Mean Reward in Episode 98: -1.4720406079239508\n",
            "Mean Reward in Episode 99: -1.3233228563771822\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2297, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2347, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3082, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2355, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2311, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2151, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3025, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2164, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2132, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2158, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2228, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2164, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2200, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2193, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3051, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2213, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2156, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2391, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2316, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3105, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.1652274278245613\n",
            "Mean Reward in Episode 1: -1.392056354971427\n",
            "Mean Reward in Episode 2: -4.095376076067965\n",
            "Mean Reward in Episode 3: -1.178961784734815\n",
            "Mean Reward in Episode 4: -1.1405432974073066\n",
            "Mean Reward in Episode 5: -1.8723442501928587\n",
            "Mean Reward in Episode 6: -4.692465563575786\n",
            "Mean Reward in Episode 7: -1.2511136749511513\n",
            "Mean Reward in Episode 8: -1.6011494589870385\n",
            "Mean Reward in Episode 9: -3.4172222303442172\n",
            "Mean Reward in Episode 10: -3.97909801891869\n",
            "Mean Reward in Episode 11: -1.365136129755762\n",
            "Mean Reward in Episode 12: -3.252262969071811\n",
            "Mean Reward in Episode 13: -1.14043800420772\n",
            "Mean Reward in Episode 14: -1.942317484414614\n",
            "Mean Reward in Episode 15: -2.2081722272643494\n",
            "Mean Reward in Episode 16: -1.3802521662583427\n",
            "Mean Reward in Episode 17: -2.00040778438237\n",
            "Mean Reward in Episode 18: -3.735178696414928\n",
            "Mean Reward in Episode 19: -3.528898042683659\n",
            "Mean Reward in Episode 20: -1.7478743058054313\n",
            "Mean Reward in Episode 21: -1.9385624352580262\n",
            "Mean Reward in Episode 22: -3.2557148976993213\n",
            "Mean Reward in Episode 23: -1.2259264295017542\n",
            "Mean Reward in Episode 24: -2.878306429431345\n",
            "Mean Reward in Episode 25: -1.7073052514155422\n",
            "Mean Reward in Episode 26: -1.770371243445406\n",
            "Mean Reward in Episode 27: -1.4181605090952765\n",
            "Mean Reward in Episode 28: -4.851753726910117\n",
            "Mean Reward in Episode 29: -1.294323232582347\n",
            "Mean Reward in Episode 30: -3.300407082725559\n",
            "Mean Reward in Episode 31: -5.0091962415505\n",
            "Mean Reward in Episode 32: -1.732846097468288\n",
            "Mean Reward in Episode 33: -0.9476861518454096\n",
            "Mean Reward in Episode 34: -1.2585297274382592\n",
            "Mean Reward in Episode 35: -3.344417276399654\n",
            "Mean Reward in Episode 36: -1.7099086683404172\n",
            "Mean Reward in Episode 37: -0.6466524673892633\n",
            "Mean Reward in Episode 38: -2.328393618110461\n",
            "Mean Reward in Episode 39: -2.1764644682379504\n",
            "Mean Reward in Episode 40: -2.880109690910892\n",
            "Mean Reward in Episode 41: -4.418679263769672\n",
            "Mean Reward in Episode 42: -3.126057567846087\n",
            "Mean Reward in Episode 43: -1.1482193410373212\n",
            "Mean Reward in Episode 44: -1.345761100201084\n",
            "Mean Reward in Episode 45: -4.752353861445173\n",
            "Mean Reward in Episode 46: -1.8301493987022213\n",
            "Mean Reward in Episode 47: -1.8331296469575893\n",
            "Mean Reward in Episode 48: -3.235773744764249\n",
            "Mean Reward in Episode 49: -2.4655341166461495\n",
            "Mean Reward in Episode 50: -1.3010879530419288\n",
            "Mean Reward in Episode 51: -2.1208241293556545\n",
            "Mean Reward in Episode 52: -1.2885383848431975\n",
            "Mean Reward in Episode 53: -1.4424917445401235\n",
            "Mean Reward in Episode 54: -4.629846116428516\n",
            "Mean Reward in Episode 55: -1.94000290698267\n",
            "Mean Reward in Episode 56: -3.6244212417456994\n",
            "Mean Reward in Episode 57: -1.602666913958851\n",
            "Mean Reward in Episode 58: -1.2684852028392006\n",
            "Mean Reward in Episode 59: -3.9958252635433897\n",
            "Mean Reward in Episode 60: -1.0941430703560115\n",
            "Mean Reward in Episode 61: -3.1399287004862186\n",
            "Mean Reward in Episode 62: -4.302957561586623\n",
            "Mean Reward in Episode 63: -3.1065609686868707\n",
            "Mean Reward in Episode 64: -1.7778806189710439\n",
            "Mean Reward in Episode 65: -1.9314932573163082\n",
            "Mean Reward in Episode 66: -5.0371342621875455\n",
            "Mean Reward in Episode 67: -2.4139385246595575\n",
            "Mean Reward in Episode 68: -3.4006178977773507\n",
            "Mean Reward in Episode 69: -2.812131698329463\n",
            "Mean Reward in Episode 70: -3.7184508242943415\n",
            "Mean Reward in Episode 71: -3.113924255178972\n",
            "Mean Reward in Episode 72: -3.604358874834296\n",
            "Mean Reward in Episode 73: -4.570686633093546\n",
            "Mean Reward in Episode 74: -1.8004555177590376\n",
            "Mean Reward in Episode 75: -1.109981053397774\n",
            "Mean Reward in Episode 76: -2.9581657434585313\n",
            "Mean Reward in Episode 77: -0.23075055047405335\n",
            "Mean Reward in Episode 78: -2.515072841112346\n",
            "Mean Reward in Episode 79: -1.5193504182430528\n",
            "Mean Reward in Episode 80: -1.4556028607492542\n",
            "Mean Reward in Episode 81: -0.9983198602743787\n",
            "Mean Reward in Episode 82: -1.144251692082757\n",
            "Mean Reward in Episode 83: -1.724534536829367\n",
            "Mean Reward in Episode 84: -1.2156690879850527\n",
            "Mean Reward in Episode 85: -0.7593983112820478\n",
            "Mean Reward in Episode 86: -2.9161584544178587\n",
            "Mean Reward in Episode 87: -1.8255927969787626\n",
            "Mean Reward in Episode 88: -4.058232398899364\n",
            "Mean Reward in Episode 89: 0.003618401891763221\n",
            "Mean Reward in Episode 90: -1.1505255541466648\n",
            "Mean Reward in Episode 91: -5.130709444583481\n",
            "Mean Reward in Episode 92: -1.5761838279206295\n",
            "Mean Reward in Episode 93: -1.4639906488111778\n",
            "Mean Reward in Episode 94: -1.781425887559072\n",
            "Mean Reward in Episode 95: -1.4655197544624845\n",
            "Mean Reward in Episode 96: -1.7470939461100106\n",
            "Mean Reward in Episode 97: -1.9205525123525515\n",
            "Mean Reward in Episode 98: -3.103605900333194\n",
            "Mean Reward in Episode 99: -3.894361998018913\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2216, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2227, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3083, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2191, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2155, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2415, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3145, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3290, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2195, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3138, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2248, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2389, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2463, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3049, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3045, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2197, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3325, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3032, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2067, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2182, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -2.1830729062896057\n",
            "Mean Reward in Episode 1: -1.8320423722373127\n",
            "Mean Reward in Episode 2: -5.298996066961684\n",
            "Mean Reward in Episode 3: -1.2861224735082641\n",
            "Mean Reward in Episode 4: -2.14813644857612\n",
            "Mean Reward in Episode 5: -1.3082161008364033\n",
            "Mean Reward in Episode 6: -0.4681850971684605\n",
            "Mean Reward in Episode 7: -0.913547861337495\n",
            "Mean Reward in Episode 8: -1.97734716835511\n",
            "Mean Reward in Episode 9: -3.5838590379052273\n",
            "Mean Reward in Episode 10: -3.9585556366599004\n",
            "Mean Reward in Episode 11: -0.7571078576224175\n",
            "Mean Reward in Episode 12: -3.195355794823349\n",
            "Mean Reward in Episode 13: -1.5100243215875349\n",
            "Mean Reward in Episode 14: -0.9089263334927824\n",
            "Mean Reward in Episode 15: -2.2434144933219105\n",
            "Mean Reward in Episode 16: -2.3560226459174523\n",
            "Mean Reward in Episode 17: -3.276046825913982\n",
            "Mean Reward in Episode 18: -1.7931178370014726\n",
            "Mean Reward in Episode 19: -1.4616983578085398\n",
            "Mean Reward in Episode 20: -1.6946534406282254\n",
            "Mean Reward in Episode 21: -3.3091639430822144\n",
            "Mean Reward in Episode 22: -4.335149362509463\n",
            "Mean Reward in Episode 23: -2.1152148399540716\n",
            "Mean Reward in Episode 24: -0.3611185099943108\n",
            "Mean Reward in Episode 25: -3.4629073409798132\n",
            "Mean Reward in Episode 26: -3.942423782308991\n",
            "Mean Reward in Episode 27: -1.7163994764051556\n",
            "Mean Reward in Episode 28: -1.6821951169063503\n",
            "Mean Reward in Episode 29: -3.640925646863076\n",
            "Mean Reward in Episode 30: -0.5626558917928796\n",
            "Mean Reward in Episode 31: -3.290728006899658\n",
            "Mean Reward in Episode 32: -1.2250757181422918\n",
            "Mean Reward in Episode 33: -1.6321896756274434\n",
            "Mean Reward in Episode 34: -1.285170456627399\n",
            "Mean Reward in Episode 35: -1.636046393538874\n",
            "Mean Reward in Episode 36: -4.210316299748875\n",
            "Mean Reward in Episode 37: -1.8607826820572302\n",
            "Mean Reward in Episode 38: -2.0855043950158687\n",
            "Mean Reward in Episode 39: -0.8459196974139616\n",
            "Mean Reward in Episode 40: 0.09841191351177889\n",
            "Mean Reward in Episode 41: -3.177818812016725\n",
            "Mean Reward in Episode 42: -2.5356199374399018\n",
            "Mean Reward in Episode 43: -1.1128902367641016\n",
            "Mean Reward in Episode 44: -2.398764754709529\n",
            "Mean Reward in Episode 45: -1.5144504910904852\n",
            "Mean Reward in Episode 46: -1.641702275477888\n",
            "Mean Reward in Episode 47: -3.959376794448364\n",
            "Mean Reward in Episode 48: -3.158605112460526\n",
            "Mean Reward in Episode 49: -0.8530778286568177\n",
            "Mean Reward in Episode 50: -4.158549433747605\n",
            "Mean Reward in Episode 51: -2.033359971543774\n",
            "Mean Reward in Episode 52: -4.177850349317658\n",
            "Mean Reward in Episode 53: -0.5859802141401156\n",
            "Mean Reward in Episode 54: -1.7263152541858915\n",
            "Mean Reward in Episode 55: -2.6165028749939174\n",
            "Mean Reward in Episode 56: -3.172723756183568\n",
            "Mean Reward in Episode 57: -1.8718622695643423\n",
            "Mean Reward in Episode 58: -3.854882963814205\n",
            "Mean Reward in Episode 59: -1.6092996376168986\n",
            "Mean Reward in Episode 60: -1.6795586610363333\n",
            "Mean Reward in Episode 61: -0.13910474870672002\n",
            "Mean Reward in Episode 62: -3.979438620028951\n",
            "Mean Reward in Episode 63: -4.451776185919418\n",
            "Mean Reward in Episode 64: -1.4248204237559174\n",
            "Mean Reward in Episode 65: -2.128008328613364\n",
            "Mean Reward in Episode 66: -2.455416010168789\n",
            "Mean Reward in Episode 67: -3.8679458532454603\n",
            "Mean Reward in Episode 68: -2.341608550705946\n",
            "Mean Reward in Episode 69: -2.384353191418267\n",
            "Mean Reward in Episode 70: -2.1905750335434395\n",
            "Mean Reward in Episode 71: -0.9674896313404321\n",
            "Mean Reward in Episode 72: -3.4740000361474572\n",
            "Mean Reward in Episode 73: -1.3130539659280052\n",
            "Mean Reward in Episode 74: -2.548480356356595\n",
            "Mean Reward in Episode 75: -1.3566833069903486\n",
            "Mean Reward in Episode 76: -1.2982758241594945\n",
            "Mean Reward in Episode 77: -0.9075767570394826\n",
            "Mean Reward in Episode 78: -3.049048864377155\n",
            "Mean Reward in Episode 79: -2.590726654562742\n",
            "Mean Reward in Episode 80: -1.408936942255509\n",
            "Mean Reward in Episode 81: -1.3127663302396249\n",
            "Mean Reward in Episode 82: -1.0578165930035357\n",
            "Mean Reward in Episode 83: -1.8901959397340378\n",
            "Mean Reward in Episode 84: -4.613697988748777\n",
            "Mean Reward in Episode 85: -4.922818933060701\n",
            "Mean Reward in Episode 86: -1.1193697136045793\n",
            "Mean Reward in Episode 87: -5.693006713301149\n",
            "Mean Reward in Episode 88: -4.937103294333878\n",
            "Mean Reward in Episode 89: -1.0024533515211678\n",
            "Mean Reward in Episode 90: -1.31202251886229\n",
            "Mean Reward in Episode 91: -1.4689664321586586\n",
            "Mean Reward in Episode 92: -1.8896903572834414\n",
            "Mean Reward in Episode 93: -4.809506722294642\n",
            "Mean Reward in Episode 94: -1.6776930350880752\n",
            "Mean Reward in Episode 95: -0.8127793994132806\n",
            "Mean Reward in Episode 96: -0.7203207070354112\n",
            "Mean Reward in Episode 97: -2.716772251688433\n",
            "Mean Reward in Episode 98: -0.7483301909448908\n",
            "Mean Reward in Episode 99: -1.0561930911438586\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2183, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2445, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2409, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2214, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2421, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2534, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3057, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2530, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3061, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2435, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3052, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2313, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2214, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3063, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2495, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2391, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2299, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2387, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2432, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2548, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.297181473005512\n",
            "Mean Reward in Episode 1: -2.9875337171178176\n",
            "Mean Reward in Episode 2: -2.173171210739977\n",
            "Mean Reward in Episode 3: -1.564280185748986\n",
            "Mean Reward in Episode 4: -1.3853807028429896\n",
            "Mean Reward in Episode 5: -0.18275192855172712\n",
            "Mean Reward in Episode 6: -1.043410928554012\n",
            "Mean Reward in Episode 7: -1.75724061131798\n",
            "Mean Reward in Episode 8: -2.1186344471916017\n",
            "Mean Reward in Episode 9: -1.6043709478341401\n",
            "Mean Reward in Episode 10: -1.2789495338274512\n",
            "Mean Reward in Episode 11: -1.926250922931925\n",
            "Mean Reward in Episode 12: -0.9731838271138166\n",
            "Mean Reward in Episode 13: -1.397610436768815\n",
            "Mean Reward in Episode 14: -1.4638386031579722\n",
            "Mean Reward in Episode 15: -2.5103624084222513\n",
            "Mean Reward in Episode 16: -1.2447142888381344\n",
            "Mean Reward in Episode 17: -3.147962090185299\n",
            "Mean Reward in Episode 18: -2.085809846504663\n",
            "Mean Reward in Episode 19: -1.5730991836577735\n",
            "Mean Reward in Episode 20: -1.83403326421179\n",
            "Mean Reward in Episode 21: -3.199138037057581\n",
            "Mean Reward in Episode 22: -1.595548949621912\n",
            "Mean Reward in Episode 23: -3.674301585398043\n",
            "Mean Reward in Episode 24: -1.1786376456531944\n",
            "Mean Reward in Episode 25: -1.8364184457412027\n",
            "Mean Reward in Episode 26: -2.842241693058709\n",
            "Mean Reward in Episode 27: -1.058029281284096\n",
            "Mean Reward in Episode 28: -2.901833573124841\n",
            "Mean Reward in Episode 29: -1.3211962460755489\n",
            "Mean Reward in Episode 30: -3.3619239152790636\n",
            "Mean Reward in Episode 31: -1.1482706100433808\n",
            "Mean Reward in Episode 32: -0.9982321295976564\n",
            "Mean Reward in Episode 33: -1.1134043885941718\n",
            "Mean Reward in Episode 34: -1.05159242906244\n",
            "Mean Reward in Episode 35: -2.521782331406454\n",
            "Mean Reward in Episode 36: -3.1983310886568717\n",
            "Mean Reward in Episode 37: -1.6071125094689311\n",
            "Mean Reward in Episode 38: -4.189423316120853\n",
            "Mean Reward in Episode 39: -2.0870622943909387\n",
            "Mean Reward in Episode 40: -1.902246062414008\n",
            "Mean Reward in Episode 41: -3.8210874643249233\n",
            "Mean Reward in Episode 42: -1.4683918317387037\n",
            "Mean Reward in Episode 43: -2.190556928407972\n",
            "Mean Reward in Episode 44: -1.1212180074362315\n",
            "Mean Reward in Episode 45: -0.9910899628822528\n",
            "Mean Reward in Episode 46: -0.97026636314014\n",
            "Mean Reward in Episode 47: -3.208897908110943\n",
            "Mean Reward in Episode 48: -1.5201515207705658\n",
            "Mean Reward in Episode 49: -0.926799592408137\n",
            "Mean Reward in Episode 50: -1.0084923288263656\n",
            "Mean Reward in Episode 51: -3.8378057639487295\n",
            "Mean Reward in Episode 52: -3.8480605650973105\n",
            "Mean Reward in Episode 53: -3.2128662322463906\n",
            "Mean Reward in Episode 54: -3.9031217997504726\n",
            "Mean Reward in Episode 55: -4.275640281816319\n",
            "Mean Reward in Episode 56: -1.239309769866622\n",
            "Mean Reward in Episode 57: -1.4093853786872137\n",
            "Mean Reward in Episode 58: -1.6931141306278799\n",
            "Mean Reward in Episode 59: -4.261532967484851\n",
            "Mean Reward in Episode 60: -4.845322097392595\n",
            "Mean Reward in Episode 61: -1.2136721059586326\n",
            "Mean Reward in Episode 62: -3.324693711269638\n",
            "Mean Reward in Episode 63: -2.1039977887805024\n",
            "Mean Reward in Episode 64: -4.480938915099183\n",
            "Mean Reward in Episode 65: -1.0517754509155877\n",
            "Mean Reward in Episode 66: -1.0626521030817822\n",
            "Mean Reward in Episode 67: -1.8952825314655548\n",
            "Mean Reward in Episode 68: -1.5476044958686588\n",
            "Mean Reward in Episode 69: -4.439648401336584\n",
            "Mean Reward in Episode 70: -1.385323403571726\n",
            "Mean Reward in Episode 71: -1.921751448500789\n",
            "Mean Reward in Episode 72: -1.741265928882147\n",
            "Mean Reward in Episode 73: -1.8776802286322893\n",
            "Mean Reward in Episode 74: -1.9679372208388037\n",
            "Mean Reward in Episode 75: -2.5128621258657455\n",
            "Mean Reward in Episode 76: -2.335550884225994\n",
            "Mean Reward in Episode 77: -1.7242755702805652\n",
            "Mean Reward in Episode 78: -3.7423094720248398\n",
            "Mean Reward in Episode 79: -2.2476918460054742\n",
            "Mean Reward in Episode 80: -1.8317135987181246\n",
            "Mean Reward in Episode 81: -1.7656596638731592\n",
            "Mean Reward in Episode 82: -3.1829024888647606\n",
            "Mean Reward in Episode 83: -0.8605127098301852\n",
            "Mean Reward in Episode 84: -1.7408052815080621\n",
            "Mean Reward in Episode 85: -1.3194771939794514\n",
            "Mean Reward in Episode 86: -4.48246709066748\n",
            "Mean Reward in Episode 87: -1.4896067848663677\n",
            "Mean Reward in Episode 88: -3.2847409321765104\n",
            "Mean Reward in Episode 89: -1.3136184407013696\n",
            "Mean Reward in Episode 90: -2.6313948769499484\n",
            "Mean Reward in Episode 91: -2.596854423850252\n",
            "Mean Reward in Episode 92: -1.2084861107656435\n",
            "Mean Reward in Episode 93: -3.8600090432655993\n",
            "Mean Reward in Episode 94: -1.4224963599817653\n",
            "Mean Reward in Episode 95: -1.0359730836705268\n",
            "Mean Reward in Episode 96: -2.9411209666215816\n",
            "Mean Reward in Episode 97: -3.6442293494137514\n",
            "Mean Reward in Episode 98: -1.923961908889843\n",
            "Mean Reward in Episode 99: -1.1894429225006624\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3064, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2447, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3079, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2084, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2486, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2476, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2515, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2325, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2451, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2222, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2457, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2385, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2350, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3087, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3059, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2376, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2223, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2448, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2121, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3366, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -2.260591136169921\n",
            "Mean Reward in Episode 1: -0.8127687248663412\n",
            "Mean Reward in Episode 2: -2.0932825781556033\n",
            "Mean Reward in Episode 3: -2.5437603791191927\n",
            "Mean Reward in Episode 4: -1.8108766639729605\n",
            "Mean Reward in Episode 5: -1.2889749394356784\n",
            "Mean Reward in Episode 6: -1.4183538852994302\n",
            "Mean Reward in Episode 7: -0.9884264848788905\n",
            "Mean Reward in Episode 8: -3.627716940595215\n",
            "Mean Reward in Episode 9: -2.725334577585065\n",
            "Mean Reward in Episode 10: -2.8142073074368903\n",
            "Mean Reward in Episode 11: -1.6562414992438161\n",
            "Mean Reward in Episode 12: -0.9617086583640964\n",
            "Mean Reward in Episode 13: -0.9224039990306921\n",
            "Mean Reward in Episode 14: -2.8522937362964425\n",
            "Mean Reward in Episode 15: -1.9745965692249858\n",
            "Mean Reward in Episode 16: -1.6852787579741304\n",
            "Mean Reward in Episode 17: -4.055381358360053\n",
            "Mean Reward in Episode 18: -1.616305488008728\n",
            "Mean Reward in Episode 19: -1.385671093058879\n",
            "Mean Reward in Episode 20: -2.9779237721817777\n",
            "Mean Reward in Episode 21: -1.9598371469532714\n",
            "Mean Reward in Episode 22: -2.6700545066355086\n",
            "Mean Reward in Episode 23: -1.6909175529004326\n",
            "Mean Reward in Episode 24: -1.2452110300529744\n",
            "Mean Reward in Episode 25: -1.4384689549045926\n",
            "Mean Reward in Episode 26: -1.3517072611398235\n",
            "Mean Reward in Episode 27: -5.582687588628884\n",
            "Mean Reward in Episode 28: -2.268724953381747\n",
            "Mean Reward in Episode 29: -2.658453956944443\n",
            "Mean Reward in Episode 30: -3.548004555495975\n",
            "Mean Reward in Episode 31: -1.7932982325280598\n",
            "Mean Reward in Episode 32: -1.9086214671979083\n",
            "Mean Reward in Episode 33: 0.29935873920882045\n",
            "Mean Reward in Episode 34: -5.145463070969101\n",
            "Mean Reward in Episode 35: -1.4371735499242921\n",
            "Mean Reward in Episode 36: -1.734123403158491\n",
            "Mean Reward in Episode 37: -3.471543550115256\n",
            "Mean Reward in Episode 38: -0.033999012176808646\n",
            "Mean Reward in Episode 39: -3.9735464110281877\n",
            "Mean Reward in Episode 40: -1.8933949962208292\n",
            "Mean Reward in Episode 41: -0.38276006753747666\n",
            "Mean Reward in Episode 42: -1.5643249194087563\n",
            "Mean Reward in Episode 43: -4.654756779430699\n",
            "Mean Reward in Episode 44: -2.5833517923753204\n",
            "Mean Reward in Episode 45: -2.4842682274735255\n",
            "Mean Reward in Episode 46: -4.312252202692435\n",
            "Mean Reward in Episode 47: -2.4875562023587223\n",
            "Mean Reward in Episode 48: -1.4667616323230717\n",
            "Mean Reward in Episode 49: -2.3911913631559254\n",
            "Mean Reward in Episode 50: -1.1685256003427045\n",
            "Mean Reward in Episode 51: -2.489549830769399\n",
            "Mean Reward in Episode 52: -1.1112229286794963\n",
            "Mean Reward in Episode 53: -1.2178499463822692\n",
            "Mean Reward in Episode 54: -1.494192552617651\n",
            "Mean Reward in Episode 55: -0.9138095184984864\n",
            "Mean Reward in Episode 56: -1.3758190887382833\n",
            "Mean Reward in Episode 57: 0.148984145903929\n",
            "Mean Reward in Episode 58: -4.564354722529712\n",
            "Mean Reward in Episode 59: -1.198973083287964\n",
            "Mean Reward in Episode 60: -1.675929879093035\n",
            "Mean Reward in Episode 61: -0.952718459807676\n",
            "Mean Reward in Episode 62: -3.4199602761176027\n",
            "Mean Reward in Episode 63: -1.1318905593343456\n",
            "Mean Reward in Episode 64: -4.084812226409113\n",
            "Mean Reward in Episode 65: -1.5777545812811855\n",
            "Mean Reward in Episode 66: -2.797411633438411\n",
            "Mean Reward in Episode 67: -3.1984870564845913\n",
            "Mean Reward in Episode 68: -1.5021568373999068\n",
            "Mean Reward in Episode 69: -3.4778466426487835\n",
            "Mean Reward in Episode 70: -2.467321218816524\n",
            "Mean Reward in Episode 71: -1.5225640446331028\n",
            "Mean Reward in Episode 72: -1.369802889734575\n",
            "Mean Reward in Episode 73: -4.776379124428899\n",
            "Mean Reward in Episode 74: -3.9525886574313995\n",
            "Mean Reward in Episode 75: -3.5860068535424894\n",
            "Mean Reward in Episode 76: -1.3305403142700458\n",
            "Mean Reward in Episode 77: -1.8011035782744844\n",
            "Mean Reward in Episode 78: -0.8815370524695745\n",
            "Mean Reward in Episode 79: -2.796733152287805\n",
            "Mean Reward in Episode 80: -3.0948317478116087\n",
            "Mean Reward in Episode 81: -1.2552950656925415\n",
            "Mean Reward in Episode 82: -2.0452376589563093\n",
            "Mean Reward in Episode 83: -1.8480281483640526\n",
            "Mean Reward in Episode 84: -1.3282325507109276\n",
            "Mean Reward in Episode 85: -0.8804914212745787\n",
            "Mean Reward in Episode 86: -1.1921785569308103\n",
            "Mean Reward in Episode 87: -2.5268940890709692\n",
            "Mean Reward in Episode 88: -3.7346751344887323\n",
            "Mean Reward in Episode 89: 0.19435257308674905\n",
            "Mean Reward in Episode 90: -1.0903144141539511\n",
            "Mean Reward in Episode 91: -2.0852468853084725\n",
            "Mean Reward in Episode 92: -1.3696897622724855\n",
            "Mean Reward in Episode 93: -1.9832471527026714\n",
            "Mean Reward in Episode 94: -1.8941014669608953\n",
            "Mean Reward in Episode 95: -3.54603817855891\n",
            "Mean Reward in Episode 96: -2.81851526186704\n",
            "Mean Reward in Episode 97: -1.848015215215767\n",
            "Mean Reward in Episode 98: -2.60827084046536\n",
            "Mean Reward in Episode 99: -2.0550333443965534\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2423, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2195, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3096, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2304, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3109, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2298, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3108, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3143, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2318, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2323, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3115, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2183, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2140, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3052, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2575, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3012, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3032, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2095, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2140, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2299, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.31075538295589\n",
            "Mean Reward in Episode 1: -3.7332887289659973\n",
            "Mean Reward in Episode 2: -3.431539643383351\n",
            "Mean Reward in Episode 3: -1.9431952943655801\n",
            "Mean Reward in Episode 4: -4.2082069293319755\n",
            "Mean Reward in Episode 5: -3.449762048918957\n",
            "Mean Reward in Episode 6: -1.0823718589759488\n",
            "Mean Reward in Episode 7: -2.213499810370245\n",
            "Mean Reward in Episode 8: -1.7051133797720281\n",
            "Mean Reward in Episode 9: -1.3727387840435226\n",
            "Mean Reward in Episode 10: -4.186610629081771\n",
            "Mean Reward in Episode 11: -1.2039581774876926\n",
            "Mean Reward in Episode 12: -1.9802334085468443\n",
            "Mean Reward in Episode 13: -4.79907505844621\n",
            "Mean Reward in Episode 14: 0.3958760082141287\n",
            "Mean Reward in Episode 15: -1.781376796963751\n",
            "Mean Reward in Episode 16: -2.0297014846060013\n",
            "Mean Reward in Episode 17: -1.866953462478475\n",
            "Mean Reward in Episode 18: -0.8917282860151269\n",
            "Mean Reward in Episode 19: -1.9625791312455705\n",
            "Mean Reward in Episode 20: -4.615808437826213\n",
            "Mean Reward in Episode 21: -0.9114741685203787\n",
            "Mean Reward in Episode 22: -3.942232719681989\n",
            "Mean Reward in Episode 23: -2.0797136162502206\n",
            "Mean Reward in Episode 24: -0.89225713090222\n",
            "Mean Reward in Episode 25: -3.761451237070983\n",
            "Mean Reward in Episode 26: -1.2674404474003287\n",
            "Mean Reward in Episode 27: 0.17468276547005818\n",
            "Mean Reward in Episode 28: -1.1387110373711418\n",
            "Mean Reward in Episode 29: -2.8141565036436744\n",
            "Mean Reward in Episode 30: -3.0034213418735196\n",
            "Mean Reward in Episode 31: -1.6292632727639904\n",
            "Mean Reward in Episode 32: -1.7833730345087857\n",
            "Mean Reward in Episode 33: -1.4235077793638244\n",
            "Mean Reward in Episode 34: -1.7329381986894927\n",
            "Mean Reward in Episode 35: -4.369143138994252\n",
            "Mean Reward in Episode 36: -0.8870542609322339\n",
            "Mean Reward in Episode 37: -3.3194722114270703\n",
            "Mean Reward in Episode 38: -2.630847599481666\n",
            "Mean Reward in Episode 39: -3.8103595364607026\n",
            "Mean Reward in Episode 40: -3.44185192646354\n",
            "Mean Reward in Episode 41: -1.1634948820589537\n",
            "Mean Reward in Episode 42: -1.5113236915685244\n",
            "Mean Reward in Episode 43: -1.4297978238897773\n",
            "Mean Reward in Episode 44: -1.4958659171902793\n",
            "Mean Reward in Episode 45: -1.3043672716788794\n",
            "Mean Reward in Episode 46: -3.5333913879785306\n",
            "Mean Reward in Episode 47: -2.5208650553821923\n",
            "Mean Reward in Episode 48: -1.026598085552061\n",
            "Mean Reward in Episode 49: -5.127324034317598\n",
            "Mean Reward in Episode 50: -1.6395164879138768\n",
            "Mean Reward in Episode 51: -1.2713943185842276\n",
            "Mean Reward in Episode 52: -2.3234745111358976\n",
            "Mean Reward in Episode 53: 0.20819711891260384\n",
            "Mean Reward in Episode 54: -2.4670795035846638\n",
            "Mean Reward in Episode 55: -1.458191778246842\n",
            "Mean Reward in Episode 56: -1.1812456440757737\n",
            "Mean Reward in Episode 57: -1.2034997076380725\n",
            "Mean Reward in Episode 58: -3.925679846205497\n",
            "Mean Reward in Episode 59: -1.6138845628269387\n",
            "Mean Reward in Episode 60: -2.222487918205284\n",
            "Mean Reward in Episode 61: -0.8824509848917723\n",
            "Mean Reward in Episode 62: -1.2137638669651922\n",
            "Mean Reward in Episode 63: -1.3261871776456255\n",
            "Mean Reward in Episode 64: -1.9126640343349657\n",
            "Mean Reward in Episode 65: -2.424312327260972\n",
            "Mean Reward in Episode 66: -1.6583924480275591\n",
            "Mean Reward in Episode 67: -2.5469402630141804\n",
            "Mean Reward in Episode 68: -1.6480454761141015\n",
            "Mean Reward in Episode 69: -4.038314859166065\n",
            "Mean Reward in Episode 70: -2.6258710679494084\n",
            "Mean Reward in Episode 71: -5.356954408369381\n",
            "Mean Reward in Episode 72: -1.5121895391610913\n",
            "Mean Reward in Episode 73: -5.21700106523736\n",
            "Mean Reward in Episode 74: -1.472164991406811\n",
            "Mean Reward in Episode 75: -1.4451240221478978\n",
            "Mean Reward in Episode 76: -1.3930770784668138\n",
            "Mean Reward in Episode 77: -3.064504976207183\n",
            "Mean Reward in Episode 78: -1.5039902069722317\n",
            "Mean Reward in Episode 79: -1.8899127880331208\n",
            "Mean Reward in Episode 80: -1.1292318346058152\n",
            "Mean Reward in Episode 81: -1.5892453219500604\n",
            "Mean Reward in Episode 82: -4.5143781487751395\n",
            "Mean Reward in Episode 83: -3.8115556119856513\n",
            "Mean Reward in Episode 84: -3.3884811371384007\n",
            "Mean Reward in Episode 85: -1.3659201740479774\n",
            "Mean Reward in Episode 86: -1.9273098008366036\n",
            "Mean Reward in Episode 87: -4.02561859493\n",
            "Mean Reward in Episode 88: -0.9872222323629226\n",
            "Mean Reward in Episode 89: -1.2825791907380097\n",
            "Mean Reward in Episode 90: -3.509473191627892\n",
            "Mean Reward in Episode 91: -1.4299901588345176\n",
            "Mean Reward in Episode 92: -0.7041003015921481\n",
            "Mean Reward in Episode 93: -3.1042429313606115\n",
            "Mean Reward in Episode 94: -0.730398809760571\n",
            "Mean Reward in Episode 95: -1.4714687623066092\n",
            "Mean Reward in Episode 96: -3.5200380056111973\n",
            "Mean Reward in Episode 97: -0.9007921507575708\n",
            "Mean Reward in Episode 98: -3.008385204091275\n",
            "Mean Reward in Episode 99: -1.3618293215093098\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2286, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3005, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3042, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3121, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2187, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2194, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2297, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2217, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3066, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3143, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2365, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3035, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2448, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2300, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2399, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2204, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3054, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3045, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2071, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3006, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -2.805382749094448\n",
            "Mean Reward in Episode 1: -1.1692580831393198\n",
            "Mean Reward in Episode 2: -4.5787512948073426\n",
            "Mean Reward in Episode 3: -1.380842472185184\n",
            "Mean Reward in Episode 4: -1.204509019659759\n",
            "Mean Reward in Episode 5: -1.4846856894001572\n",
            "Mean Reward in Episode 6: -1.0483843516830074\n",
            "Mean Reward in Episode 7: -4.050507861116776\n",
            "Mean Reward in Episode 8: -2.747680888940035\n",
            "Mean Reward in Episode 9: -2.5976804131849676\n",
            "Mean Reward in Episode 10: -1.1671114735408685\n",
            "Mean Reward in Episode 11: -1.0887021180089733\n",
            "Mean Reward in Episode 12: -2.616126765035254\n",
            "Mean Reward in Episode 13: -2.3707462858228445\n",
            "Mean Reward in Episode 14: -1.6968415012582225\n",
            "Mean Reward in Episode 15: -3.632245165983247\n",
            "Mean Reward in Episode 16: -2.147752169290466\n",
            "Mean Reward in Episode 17: -3.661494475492098\n",
            "Mean Reward in Episode 18: -2.634971718809009\n",
            "Mean Reward in Episode 19: -0.7638708133606265\n",
            "Mean Reward in Episode 20: -2.7340374888710373\n",
            "Mean Reward in Episode 21: -1.5662223500407546\n",
            "Mean Reward in Episode 22: -2.719331738506494\n",
            "Mean Reward in Episode 23: -2.919986399891126\n",
            "Mean Reward in Episode 24: -3.041333753577464\n",
            "Mean Reward in Episode 25: -2.22086417681642\n",
            "Mean Reward in Episode 26: -2.6955673164505507\n",
            "Mean Reward in Episode 27: -3.849376725632627\n",
            "Mean Reward in Episode 28: -3.314947589762424\n",
            "Mean Reward in Episode 29: -1.425815421946317\n",
            "Mean Reward in Episode 30: -0.9278376935667924\n",
            "Mean Reward in Episode 31: -1.6654971234584555\n",
            "Mean Reward in Episode 32: -1.311606238889373\n",
            "Mean Reward in Episode 33: -5.691079813355852\n",
            "Mean Reward in Episode 34: -1.8393253233894724\n",
            "Mean Reward in Episode 35: -0.941517225739006\n",
            "Mean Reward in Episode 36: -1.2609371948905377\n",
            "Mean Reward in Episode 37: -1.3294911944110346\n",
            "Mean Reward in Episode 38: -1.5883090872150387\n",
            "Mean Reward in Episode 39: -2.088433489133866\n",
            "Mean Reward in Episode 40: -0.645560603143912\n",
            "Mean Reward in Episode 41: -3.007117936494357\n",
            "Mean Reward in Episode 42: -4.797552823957658\n",
            "Mean Reward in Episode 43: -0.29573721572730804\n",
            "Mean Reward in Episode 44: -1.8340106849547615\n",
            "Mean Reward in Episode 45: -1.4307311144864208\n",
            "Mean Reward in Episode 46: -2.871430999301637\n",
            "Mean Reward in Episode 47: -0.9562423011868304\n",
            "Mean Reward in Episode 48: -4.265537261805282\n",
            "Mean Reward in Episode 49: -0.8866951045946347\n",
            "Mean Reward in Episode 50: -1.1924235332373978\n",
            "Mean Reward in Episode 51: -1.5362198335781518\n",
            "Mean Reward in Episode 52: -1.1492983452750813\n",
            "Mean Reward in Episode 53: -1.4981828622285438\n",
            "Mean Reward in Episode 54: -3.102896165107702\n",
            "Mean Reward in Episode 55: -3.910636195477755\n",
            "Mean Reward in Episode 56: -2.928359608612975\n",
            "Mean Reward in Episode 57: -0.9386365637573263\n",
            "Mean Reward in Episode 58: -2.3828240097874605\n",
            "Mean Reward in Episode 59: -4.010228227822174\n",
            "Mean Reward in Episode 60: -2.0766863807624993\n",
            "Mean Reward in Episode 61: -0.7345790507712423\n",
            "Mean Reward in Episode 62: -2.6303207346764053\n",
            "Mean Reward in Episode 63: -0.8556706525198085\n",
            "Mean Reward in Episode 64: -1.1256286751839113\n",
            "Mean Reward in Episode 65: -1.286659358676752\n",
            "Mean Reward in Episode 66: -1.4560351038156254\n",
            "Mean Reward in Episode 67: -3.975109129846341\n",
            "Mean Reward in Episode 68: -3.122067772688351\n",
            "Mean Reward in Episode 69: -2.8081611284966796\n",
            "Mean Reward in Episode 70: -0.4314896445396398\n",
            "Mean Reward in Episode 71: -1.2043757577694878\n",
            "Mean Reward in Episode 72: -1.372677807429604\n",
            "Mean Reward in Episode 73: -1.5809237995364394\n",
            "Mean Reward in Episode 74: -2.6169916247640708\n",
            "Mean Reward in Episode 75: 0.44032570156783435\n",
            "Mean Reward in Episode 76: -3.0847696252462673\n",
            "Mean Reward in Episode 77: -4.3926873235023445\n",
            "Mean Reward in Episode 78: -1.7740837123515998\n",
            "Mean Reward in Episode 79: -1.9762367546000983\n",
            "Mean Reward in Episode 80: -3.785451016177347\n",
            "Mean Reward in Episode 81: -1.8067468267368472\n",
            "Mean Reward in Episode 82: -0.8075321421752473\n",
            "Mean Reward in Episode 83: -4.799931036913284\n",
            "Mean Reward in Episode 84: -1.007847531630169\n",
            "Mean Reward in Episode 85: -1.194641808967187\n",
            "Mean Reward in Episode 86: -1.5318209231544486\n",
            "Mean Reward in Episode 87: -2.699234057828929\n",
            "Mean Reward in Episode 88: -1.3229243219207927\n",
            "Mean Reward in Episode 89: -1.1205467372251092\n",
            "Mean Reward in Episode 90: -4.035873428969909\n",
            "Mean Reward in Episode 91: -1.3275120878427797\n",
            "Mean Reward in Episode 92: -3.9349320795770244\n",
            "Mean Reward in Episode 93: -3.140699977287907\n",
            "Mean Reward in Episode 94: -1.556065871016715\n",
            "Mean Reward in Episode 95: -1.9746961825397613\n",
            "Mean Reward in Episode 96: -3.0630613871016887\n",
            "Mean Reward in Episode 97: -1.365375306062986\n",
            "Mean Reward in Episode 98: -1.110686738856354\n",
            "Mean Reward in Episode 99: -1.2221545100151567\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3058, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3071, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3131, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3059, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3076, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2210, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2375, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2462, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2223, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2348, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3033, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3120, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3057, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3157, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2516, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3091, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2113, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2142, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2180, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3281, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.6119382264104227\n",
            "Mean Reward in Episode 1: -1.184151764866862\n",
            "Mean Reward in Episode 2: -2.954806945264563\n",
            "Mean Reward in Episode 3: -1.6364270780818417\n",
            "Mean Reward in Episode 4: -1.5011547847185855\n",
            "Mean Reward in Episode 5: -1.2373439151219263\n",
            "Mean Reward in Episode 6: -3.137792520954782\n",
            "Mean Reward in Episode 7: -1.8726214922087747\n",
            "Mean Reward in Episode 8: -0.19165348783581845\n",
            "Mean Reward in Episode 9: -0.9272036494540492\n",
            "Mean Reward in Episode 10: -2.9970367580900725\n",
            "Mean Reward in Episode 11: -1.5443159567648646\n",
            "Mean Reward in Episode 12: -2.927333086065471\n",
            "Mean Reward in Episode 13: -5.301549262767365\n",
            "Mean Reward in Episode 14: -2.388874450128575\n",
            "Mean Reward in Episode 15: -1.2411366550911025\n",
            "Mean Reward in Episode 16: -2.009735778885824\n",
            "Mean Reward in Episode 17: -1.8361730737084123\n",
            "Mean Reward in Episode 18: -2.0274271741251018\n",
            "Mean Reward in Episode 19: -1.3747760314215252\n",
            "Mean Reward in Episode 20: -4.376348750343433\n",
            "Mean Reward in Episode 21: -1.1723647504394912\n",
            "Mean Reward in Episode 22: -1.3485828918938427\n",
            "Mean Reward in Episode 23: -0.4413866553926651\n",
            "Mean Reward in Episode 24: -4.944972302372725\n",
            "Mean Reward in Episode 25: -2.4079426465206213\n",
            "Mean Reward in Episode 26: -1.1403949107420361\n",
            "Mean Reward in Episode 27: -1.3242726034859564\n",
            "Mean Reward in Episode 28: -0.21901428272152074\n",
            "Mean Reward in Episode 29: -4.769802831871639\n",
            "Mean Reward in Episode 30: -1.555442092524532\n",
            "Mean Reward in Episode 31: -1.7276291226548004\n",
            "Mean Reward in Episode 32: -0.8254084320997479\n",
            "Mean Reward in Episode 33: -1.7543872123256694\n",
            "Mean Reward in Episode 34: -2.415989572262953\n",
            "Mean Reward in Episode 35: -2.254371281639945\n",
            "Mean Reward in Episode 36: -1.3849504374456423\n",
            "Mean Reward in Episode 37: -5.394607311784421\n",
            "Mean Reward in Episode 38: -5.2490502939577945\n",
            "Mean Reward in Episode 39: -0.9368707791045512\n",
            "Mean Reward in Episode 40: -0.5006841115248087\n",
            "Mean Reward in Episode 41: -4.609141210597822\n",
            "Mean Reward in Episode 42: -1.4912483499479767\n",
            "Mean Reward in Episode 43: -3.3468269654611116\n",
            "Mean Reward in Episode 44: -4.700681946942906\n",
            "Mean Reward in Episode 45: -4.97390007678054\n",
            "Mean Reward in Episode 46: -3.353040281151659\n",
            "Mean Reward in Episode 47: -0.9870265676142079\n",
            "Mean Reward in Episode 48: -2.991516800093754\n",
            "Mean Reward in Episode 49: -2.36096708069123\n",
            "Mean Reward in Episode 50: -1.436010683758732\n",
            "Mean Reward in Episode 51: -1.4065080707239788\n",
            "Mean Reward in Episode 52: 0.24909457330286558\n",
            "Mean Reward in Episode 53: -2.3998072490082154\n",
            "Mean Reward in Episode 54: -2.7978109557529978\n",
            "Mean Reward in Episode 55: -2.0373629693433215\n",
            "Mean Reward in Episode 56: -0.8742172616029449\n",
            "Mean Reward in Episode 57: -2.067965695653163\n",
            "Mean Reward in Episode 58: -0.5898437424610811\n",
            "Mean Reward in Episode 59: -3.6941409013275246\n",
            "Mean Reward in Episode 60: -1.637395066173239\n",
            "Mean Reward in Episode 61: -2.477095672632466\n",
            "Mean Reward in Episode 62: -0.48231566271111304\n",
            "Mean Reward in Episode 63: -2.646766935650216\n",
            "Mean Reward in Episode 64: -1.3052319015035914\n",
            "Mean Reward in Episode 65: -0.5818936957613456\n",
            "Mean Reward in Episode 66: -1.168173559096965\n",
            "Mean Reward in Episode 67: -4.521895279991461\n",
            "Mean Reward in Episode 68: -2.1895653674815447\n",
            "Mean Reward in Episode 69: -0.8731916942442314\n",
            "Mean Reward in Episode 70: -5.194703671447693\n",
            "Mean Reward in Episode 71: 0.04948488352655772\n",
            "Mean Reward in Episode 72: -1.2361481978757778\n",
            "Mean Reward in Episode 73: -1.7478567556816769\n",
            "Mean Reward in Episode 74: -1.3425528926457975\n",
            "Mean Reward in Episode 75: -1.1752451357096712\n",
            "Mean Reward in Episode 76: -1.5112113120139772\n",
            "Mean Reward in Episode 77: -4.181396017433075\n",
            "Mean Reward in Episode 78: -1.237776859984811\n",
            "Mean Reward in Episode 79: -3.153989321250794\n",
            "Mean Reward in Episode 80: -2.5337244462323656\n",
            "Mean Reward in Episode 81: -2.536182396401198\n",
            "Mean Reward in Episode 82: -1.2828143939137393\n",
            "Mean Reward in Episode 83: -1.2565570623632123\n",
            "Mean Reward in Episode 84: -1.3298637757670466\n",
            "Mean Reward in Episode 85: -1.5806771893289115\n",
            "Mean Reward in Episode 86: -1.9109856683247781\n",
            "Mean Reward in Episode 87: -1.917386970813288\n",
            "Mean Reward in Episode 88: -3.786469477078859\n",
            "Mean Reward in Episode 89: -3.096705235273571\n",
            "Mean Reward in Episode 90: -1.1969872325603128\n",
            "Mean Reward in Episode 91: -1.1839269876200067\n",
            "Mean Reward in Episode 92: -0.4043182793455579\n",
            "Mean Reward in Episode 93: -1.106598718468961\n",
            "Mean Reward in Episode 94: -1.5535678332065102\n",
            "Mean Reward in Episode 95: -2.163607951742269\n",
            "Mean Reward in Episode 96: -1.1564920882731387\n",
            "Mean Reward in Episode 97: -3.8311005231664144\n",
            "Mean Reward in Episode 98: -3.3630323400620235\n",
            "Mean Reward in Episode 99: -1.9580112001271681\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3105, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2204, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3101, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3045, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2215, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3127, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3040, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2523, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3090, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2310, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2381, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3279, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2390, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3022, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3125, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3106, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2430, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2378, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3077, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3062, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -2.001385778323098\n",
            "Mean Reward in Episode 1: -2.787121030226874\n",
            "Mean Reward in Episode 2: -2.442968715743523\n",
            "Mean Reward in Episode 3: -1.1750581342459068\n",
            "Mean Reward in Episode 4: -1.450562024366886\n",
            "Mean Reward in Episode 5: -2.0488538717894005\n",
            "Mean Reward in Episode 6: -0.9902208713345217\n",
            "Mean Reward in Episode 7: -1.6083377923295075\n",
            "Mean Reward in Episode 8: -2.934256569958754\n",
            "Mean Reward in Episode 9: -1.8370095464741156\n",
            "Mean Reward in Episode 10: -1.5937663715926498\n",
            "Mean Reward in Episode 11: -3.476726233694562\n",
            "Mean Reward in Episode 12: -2.1105036254732688\n",
            "Mean Reward in Episode 13: -1.740933029318677\n",
            "Mean Reward in Episode 14: -2.0889910816709514\n",
            "Mean Reward in Episode 15: -0.19125738245269375\n",
            "Mean Reward in Episode 16: -2.2478706471872956\n",
            "Mean Reward in Episode 17: -1.2778711161468634\n",
            "Mean Reward in Episode 18: -1.9100419785050065\n",
            "Mean Reward in Episode 19: -3.4636747414910234\n",
            "Mean Reward in Episode 20: -1.8479583682794827\n",
            "Mean Reward in Episode 21: -2.0521717155446066\n",
            "Mean Reward in Episode 22: -3.037537786705094\n",
            "Mean Reward in Episode 23: -1.4399623208278114\n",
            "Mean Reward in Episode 24: -1.1861100819966655\n",
            "Mean Reward in Episode 25: -3.1086626463041283\n",
            "Mean Reward in Episode 26: -4.887469338421742\n",
            "Mean Reward in Episode 27: -1.017655273064728\n",
            "Mean Reward in Episode 28: -5.9558034943871725\n",
            "Mean Reward in Episode 29: -2.840938287547074\n",
            "Mean Reward in Episode 30: -2.8329396953997015\n",
            "Mean Reward in Episode 31: -5.1457719645317\n",
            "Mean Reward in Episode 32: -2.70140614659949\n",
            "Mean Reward in Episode 33: -2.6486312091287116\n",
            "Mean Reward in Episode 34: -4.983119796664585\n",
            "Mean Reward in Episode 35: -2.021314908200393\n",
            "Mean Reward in Episode 36: -1.6622241619399896\n",
            "Mean Reward in Episode 37: -1.6105298038995666\n",
            "Mean Reward in Episode 38: -1.7746689286980721\n",
            "Mean Reward in Episode 39: -2.8236271206477936\n",
            "Mean Reward in Episode 40: -1.866618786636066\n",
            "Mean Reward in Episode 41: -3.084935495536393\n",
            "Mean Reward in Episode 42: -2.9993507842716935\n",
            "Mean Reward in Episode 43: -3.825284821450277\n",
            "Mean Reward in Episode 44: -3.713448629626633\n",
            "Mean Reward in Episode 45: -1.7808459035025186\n",
            "Mean Reward in Episode 46: -2.8994367872886113\n",
            "Mean Reward in Episode 47: -3.4056617823022344\n",
            "Mean Reward in Episode 48: -1.5634182037357998\n",
            "Mean Reward in Episode 49: -2.045360722987984\n",
            "Mean Reward in Episode 50: -1.617617839559058\n",
            "Mean Reward in Episode 51: -1.2160915198883746\n",
            "Mean Reward in Episode 52: -1.3460107054863761\n",
            "Mean Reward in Episode 53: -3.1633516762038214\n",
            "Mean Reward in Episode 54: -0.7547514202095142\n",
            "Mean Reward in Episode 55: -1.4061233432342564\n",
            "Mean Reward in Episode 56: -1.3700879961780388\n",
            "Mean Reward in Episode 57: -1.3637879302401414\n",
            "Mean Reward in Episode 58: -1.500067023953321\n",
            "Mean Reward in Episode 59: -1.139613610309059\n",
            "Mean Reward in Episode 60: -4.836214503572569\n",
            "Mean Reward in Episode 61: -1.7933007292289875\n",
            "Mean Reward in Episode 62: -1.4163151833228609\n",
            "Mean Reward in Episode 63: -4.724955949309982\n",
            "Mean Reward in Episode 64: -0.519876224762043\n",
            "Mean Reward in Episode 65: -0.23239355590323232\n",
            "Mean Reward in Episode 66: -1.398476171490201\n",
            "Mean Reward in Episode 67: -1.3566377983066722\n",
            "Mean Reward in Episode 68: -0.8421887400303925\n",
            "Mean Reward in Episode 69: -1.4334577203775574\n",
            "Mean Reward in Episode 70: -4.903332873528627\n",
            "Mean Reward in Episode 71: -0.5918519147887017\n",
            "Mean Reward in Episode 72: -2.2320530837696473\n",
            "Mean Reward in Episode 73: -0.9899457883590385\n",
            "Mean Reward in Episode 74: -3.1511134534069067\n",
            "Mean Reward in Episode 75: -1.4679849376209793\n",
            "Mean Reward in Episode 76: -1.9409951427722718\n",
            "Mean Reward in Episode 77: -0.9632297141238415\n",
            "Mean Reward in Episode 78: -1.0594908289110896\n",
            "Mean Reward in Episode 79: -1.1634499105735499\n",
            "Mean Reward in Episode 80: -1.3478307189248773\n",
            "Mean Reward in Episode 81: -0.7922882739290018\n",
            "Mean Reward in Episode 82: -1.4468300728676489\n",
            "Mean Reward in Episode 83: -2.045789991053632\n",
            "Mean Reward in Episode 84: -2.506238420614103\n",
            "Mean Reward in Episode 85: -0.9133426226952244\n",
            "Mean Reward in Episode 86: -1.0687236295671267\n",
            "Mean Reward in Episode 87: -1.184945345440472\n",
            "Mean Reward in Episode 88: -1.655738104972592\n",
            "Mean Reward in Episode 89: -2.013242049641762\n",
            "Mean Reward in Episode 90: -0.8817007067123457\n",
            "Mean Reward in Episode 91: -2.526077764753928\n",
            "Mean Reward in Episode 92: -1.70761327848156\n",
            "Mean Reward in Episode 93: -1.7127982907547588\n",
            "Mean Reward in Episode 94: -3.3137514231200123\n",
            "Mean Reward in Episode 95: -3.8537864403153614\n",
            "Mean Reward in Episode 96: -3.658431468818767\n",
            "Mean Reward in Episode 97: -1.4931741399268708\n",
            "Mean Reward in Episode 98: -3.374101224815275\n",
            "Mean Reward in Episode 99: -1.5048483206297525\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2311, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2359, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3089, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3150, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2125, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2377, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2299, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2377, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2442, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2274, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2210, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2389, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2378, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2316, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3069, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2365, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2287, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2291, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3080, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2418, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -0.9657868954739538\n",
            "Mean Reward in Episode 1: -5.54905552080211\n",
            "Mean Reward in Episode 2: -1.406972222019664\n",
            "Mean Reward in Episode 3: -1.5500453109520196\n",
            "Mean Reward in Episode 4: -1.4614106295267357\n",
            "Mean Reward in Episode 5: -2.6862036255057116\n",
            "Mean Reward in Episode 6: -3.739217840475104\n",
            "Mean Reward in Episode 7: -1.3565506738996784\n",
            "Mean Reward in Episode 8: -1.4657335413535735\n",
            "Mean Reward in Episode 9: -1.5757573969911878\n",
            "Mean Reward in Episode 10: -0.9547710617295587\n",
            "Mean Reward in Episode 11: -3.0247263232107744\n",
            "Mean Reward in Episode 12: -3.9788590067472454\n",
            "Mean Reward in Episode 13: -0.39454419984624384\n",
            "Mean Reward in Episode 14: -1.3673190323183426\n",
            "Mean Reward in Episode 15: -2.4756583536291084\n",
            "Mean Reward in Episode 16: -1.3571376332030567\n",
            "Mean Reward in Episode 17: -3.1622945468197328\n",
            "Mean Reward in Episode 18: -2.8753825960384116\n",
            "Mean Reward in Episode 19: -2.142977176849957\n",
            "Mean Reward in Episode 20: -2.6202014480250275\n",
            "Mean Reward in Episode 21: -4.58061217491243\n",
            "Mean Reward in Episode 22: 0.8229741489741622\n",
            "Mean Reward in Episode 23: -0.8825646138512171\n",
            "Mean Reward in Episode 24: -2.723120205857605\n",
            "Mean Reward in Episode 25: -2.246051540157779\n",
            "Mean Reward in Episode 26: -2.3702325230739105\n",
            "Mean Reward in Episode 27: -1.3847327606575155\n",
            "Mean Reward in Episode 28: -1.9087433309826183\n",
            "Mean Reward in Episode 29: -2.797043495146686\n",
            "Mean Reward in Episode 30: -5.65205551651226\n",
            "Mean Reward in Episode 31: -2.154250954690738\n",
            "Mean Reward in Episode 32: -1.8316747068396675\n",
            "Mean Reward in Episode 33: -3.134116090781012\n",
            "Mean Reward in Episode 34: -1.1731657336333594\n",
            "Mean Reward in Episode 35: -3.138684678068212\n",
            "Mean Reward in Episode 36: -2.255086586897138\n",
            "Mean Reward in Episode 37: -0.09262612064053692\n",
            "Mean Reward in Episode 38: -4.603970619593682\n",
            "Mean Reward in Episode 39: -1.0869342026468536\n",
            "Mean Reward in Episode 40: -1.668895515915929\n",
            "Mean Reward in Episode 41: -3.2750309103455426\n",
            "Mean Reward in Episode 42: -2.160967379895211\n",
            "Mean Reward in Episode 43: -4.336750393182776\n",
            "Mean Reward in Episode 44: -4.211426185030786\n",
            "Mean Reward in Episode 45: -0.13224738729951072\n",
            "Mean Reward in Episode 46: -1.6165777724799324\n",
            "Mean Reward in Episode 47: -1.0676767684653097\n",
            "Mean Reward in Episode 48: -4.315446479426658\n",
            "Mean Reward in Episode 49: -1.8915812910778096\n",
            "Mean Reward in Episode 50: -3.927395960865111\n",
            "Mean Reward in Episode 51: -2.413429266976629\n",
            "Mean Reward in Episode 52: -1.9485551829551218\n",
            "Mean Reward in Episode 53: -2.84856278531759\n",
            "Mean Reward in Episode 54: -1.8495406911309633\n",
            "Mean Reward in Episode 55: -2.5371886786814994\n",
            "Mean Reward in Episode 56: -2.8027603932014555\n",
            "Mean Reward in Episode 57: -2.264452543471261\n",
            "Mean Reward in Episode 58: -4.178481098306403\n",
            "Mean Reward in Episode 59: -1.5078649157934922\n",
            "Mean Reward in Episode 60: -2.6953574213582043\n",
            "Mean Reward in Episode 61: -2.888594831220591\n",
            "Mean Reward in Episode 62: -0.9639133070335635\n",
            "Mean Reward in Episode 63: -1.0930851079528243\n",
            "Mean Reward in Episode 64: -1.7392001556575176\n",
            "Mean Reward in Episode 65: -1.8597437449409113\n",
            "Mean Reward in Episode 66: -2.298459465929715\n",
            "Mean Reward in Episode 67: -1.1577835208989684\n",
            "Mean Reward in Episode 68: -2.61360558336841\n",
            "Mean Reward in Episode 69: -1.3777042741668473\n",
            "Mean Reward in Episode 70: -3.9049158791799856\n",
            "Mean Reward in Episode 71: -2.372255219962415\n",
            "Mean Reward in Episode 72: -1.8607851851335624\n",
            "Mean Reward in Episode 73: -1.4019657427715944\n",
            "Mean Reward in Episode 74: -2.7352012659738416\n",
            "Mean Reward in Episode 75: -1.4465368972549522\n",
            "Mean Reward in Episode 76: -1.4471692594459928\n",
            "Mean Reward in Episode 77: -1.3440017936641877\n",
            "Mean Reward in Episode 78: -2.2328952205048123\n",
            "Mean Reward in Episode 79: -4.42177273800189\n",
            "Mean Reward in Episode 80: -4.646505041675263\n",
            "Mean Reward in Episode 81: -1.0525520953136915\n",
            "Mean Reward in Episode 82: -1.813061665642963\n",
            "Mean Reward in Episode 83: -3.9973120020537274\n",
            "Mean Reward in Episode 84: -1.255561627927809\n",
            "Mean Reward in Episode 85: -0.7515109369371877\n",
            "Mean Reward in Episode 86: -2.9952559697542243\n",
            "Mean Reward in Episode 87: -1.024678780033746\n",
            "Mean Reward in Episode 88: -1.925214557636191\n",
            "Mean Reward in Episode 89: -2.6619160210663733\n",
            "Mean Reward in Episode 90: -1.1140933720640414\n",
            "Mean Reward in Episode 91: -3.046479197446547\n",
            "Mean Reward in Episode 92: -2.061551097613203\n",
            "Mean Reward in Episode 93: -1.4153362092314643\n",
            "Mean Reward in Episode 94: -2.445886651452521\n",
            "Mean Reward in Episode 95: -1.4328295944683132\n",
            "Mean Reward in Episode 96: -0.6524085497233847\n",
            "Mean Reward in Episode 97: -3.054993853754438\n",
            "Mean Reward in Episode 98: -2.8427613727755574\n",
            "Mean Reward in Episode 99: -2.5195929839180597\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3053, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2299, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2167, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2303, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2135, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2220, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3081, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3044, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2386, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3028, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2217, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2167, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2448, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2342, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2169, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2447, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3195, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2168, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2165, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2205, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -2.7252544949300153\n",
            "Mean Reward in Episode 1: -1.3868349813644372\n",
            "Mean Reward in Episode 2: -0.3594669216006646\n",
            "Mean Reward in Episode 3: -2.3373680122717895\n",
            "Mean Reward in Episode 4: -3.171382205227977\n",
            "Mean Reward in Episode 5: -2.273174535230076\n",
            "Mean Reward in Episode 6: -1.602990503258771\n",
            "Mean Reward in Episode 7: -1.0308348943960801\n",
            "Mean Reward in Episode 8: -1.0912008101227983\n",
            "Mean Reward in Episode 9: 0.10470542084883598\n",
            "Mean Reward in Episode 10: -4.172415647880537\n",
            "Mean Reward in Episode 11: -1.1855081377755543\n",
            "Mean Reward in Episode 12: -2.226675130213288\n",
            "Mean Reward in Episode 13: -1.307750218563274\n",
            "Mean Reward in Episode 14: -2.8230313623534617\n",
            "Mean Reward in Episode 15: -2.3980161643696105\n",
            "Mean Reward in Episode 16: -2.5086197945929465\n",
            "Mean Reward in Episode 17: -1.5892040158674645\n",
            "Mean Reward in Episode 18: -1.6689873141718443\n",
            "Mean Reward in Episode 19: -0.9329614507268108\n",
            "Mean Reward in Episode 20: -2.688046536893399\n",
            "Mean Reward in Episode 21: -1.3136507927953653\n",
            "Mean Reward in Episode 22: -4.113363076982831\n",
            "Mean Reward in Episode 23: -4.017616036977591\n",
            "Mean Reward in Episode 24: -0.730469637683527\n",
            "Mean Reward in Episode 25: -1.4682219057169548\n",
            "Mean Reward in Episode 26: -3.534504023287667\n",
            "Mean Reward in Episode 27: -1.8454031509370132\n",
            "Mean Reward in Episode 28: -2.778084870301219\n",
            "Mean Reward in Episode 29: -1.3426707333481516\n",
            "Mean Reward in Episode 30: -3.69765679228449\n",
            "Mean Reward in Episode 31: -2.374386306851972\n",
            "Mean Reward in Episode 32: -1.1445214960583017\n",
            "Mean Reward in Episode 33: -0.9707897541808783\n",
            "Mean Reward in Episode 34: -1.3902448571538752\n",
            "Mean Reward in Episode 35: -1.1095726657480058\n",
            "Mean Reward in Episode 36: -0.8130599397937929\n",
            "Mean Reward in Episode 37: -1.3487287243746517\n",
            "Mean Reward in Episode 38: -2.13993355202335\n",
            "Mean Reward in Episode 39: -2.8571451534146517\n",
            "Mean Reward in Episode 40: -0.9325293566388625\n",
            "Mean Reward in Episode 41: -1.704387266893723\n",
            "Mean Reward in Episode 42: -1.0455344461493916\n",
            "Mean Reward in Episode 43: -4.096507514336092\n",
            "Mean Reward in Episode 44: -4.159895346225922\n",
            "Mean Reward in Episode 45: -4.385838938141774\n",
            "Mean Reward in Episode 46: -4.530122014281358\n",
            "Mean Reward in Episode 47: -4.464099193536923\n",
            "Mean Reward in Episode 48: -1.6019734811657174\n",
            "Mean Reward in Episode 49: -1.1522352062410026\n",
            "Mean Reward in Episode 50: -2.366066597647052\n",
            "Mean Reward in Episode 51: -0.8718071907732756\n",
            "Mean Reward in Episode 52: -1.7795315848159172\n",
            "Mean Reward in Episode 53: -1.7887594162716456\n",
            "Mean Reward in Episode 54: -2.0910313775953555\n",
            "Mean Reward in Episode 55: -1.5856392821821907\n",
            "Mean Reward in Episode 56: -1.8005235679091927\n",
            "Mean Reward in Episode 57: -0.9651212824333749\n",
            "Mean Reward in Episode 58: -1.0158963677265362\n",
            "Mean Reward in Episode 59: -3.5239787500430064\n",
            "Mean Reward in Episode 60: -1.8595790786486985\n",
            "Mean Reward in Episode 61: -1.715613788498921\n",
            "Mean Reward in Episode 62: -4.065836093746484\n",
            "Mean Reward in Episode 63: -1.5435681504678922\n",
            "Mean Reward in Episode 64: -2.2025956387890946\n",
            "Mean Reward in Episode 65: -1.1362175119874462\n",
            "Mean Reward in Episode 66: -3.5078028302131887\n",
            "Mean Reward in Episode 67: -1.7519334035537049\n",
            "Mean Reward in Episode 68: -3.010964760190463\n",
            "Mean Reward in Episode 69: -3.5994890465349236\n",
            "Mean Reward in Episode 70: -0.8521782859201773\n",
            "Mean Reward in Episode 71: -4.305306609949344\n",
            "Mean Reward in Episode 72: -1.9761379123995033\n",
            "Mean Reward in Episode 73: -1.8191174187883097\n",
            "Mean Reward in Episode 74: -1.5886778550690532\n",
            "Mean Reward in Episode 75: -1.312147462316569\n",
            "Mean Reward in Episode 76: -0.9778641894400082\n",
            "Mean Reward in Episode 77: -1.5111108608270993\n",
            "Mean Reward in Episode 78: -0.5869952341360082\n",
            "Mean Reward in Episode 79: -0.9031978047137258\n",
            "Mean Reward in Episode 80: -2.2953929739812664\n",
            "Mean Reward in Episode 81: -1.9477450693929799\n",
            "Mean Reward in Episode 82: -1.1319322909056833\n",
            "Mean Reward in Episode 83: -2.235020394951921\n",
            "Mean Reward in Episode 84: -3.2021353455287698\n",
            "Mean Reward in Episode 85: -3.443253446192046\n",
            "Mean Reward in Episode 86: -1.1086955193272112\n",
            "Mean Reward in Episode 87: -2.421017511976651\n",
            "Mean Reward in Episode 88: -3.692339643674108\n",
            "Mean Reward in Episode 89: -4.386517736332609\n",
            "Mean Reward in Episode 90: -1.299745454596173\n",
            "Mean Reward in Episode 91: -1.5409451610514966\n",
            "Mean Reward in Episode 92: -1.5948828896383065\n",
            "Mean Reward in Episode 93: -2.8852339022915348\n",
            "Mean Reward in Episode 94: -1.2156402856426578\n",
            "Mean Reward in Episode 95: -1.1709453357106687\n",
            "Mean Reward in Episode 96: -1.6473970192778908\n",
            "Mean Reward in Episode 97: -2.5198657908916586\n",
            "Mean Reward in Episode 98: -2.576177046754666\n",
            "Mean Reward in Episode 99: -3.3231454922172374\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3070, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2428, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2351, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2137, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2216, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2309, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3100, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2487, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2299, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3248, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3097, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2384, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3032, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2187, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2376, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2311, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2322, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2275, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2303, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2420, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.5937876418445587\n",
            "Mean Reward in Episode 1: -1.5696608660923543\n",
            "Mean Reward in Episode 2: -1.989261689010446\n",
            "Mean Reward in Episode 3: -0.8877163798403712\n",
            "Mean Reward in Episode 4: -1.6884997675566213\n",
            "Mean Reward in Episode 5: -1.0274930896140477\n",
            "Mean Reward in Episode 6: -1.9401000422449541\n",
            "Mean Reward in Episode 7: -3.0255977903492446\n",
            "Mean Reward in Episode 8: -3.004662660044258\n",
            "Mean Reward in Episode 9: -0.8785121345835547\n",
            "Mean Reward in Episode 10: -3.0270568259392197\n",
            "Mean Reward in Episode 11: -1.6716254644518729\n",
            "Mean Reward in Episode 12: -2.090259180655375\n",
            "Mean Reward in Episode 13: -2.8433207162978746\n",
            "Mean Reward in Episode 14: -1.91886749792965\n",
            "Mean Reward in Episode 15: -1.5455227789864812\n",
            "Mean Reward in Episode 16: -0.8855461859789102\n",
            "Mean Reward in Episode 17: -2.984339889010095\n",
            "Mean Reward in Episode 18: -4.41566046333777\n",
            "Mean Reward in Episode 19: -4.377727157667109\n",
            "Mean Reward in Episode 20: -3.9544798260751644\n",
            "Mean Reward in Episode 21: -2.519264958120618\n",
            "Mean Reward in Episode 22: -2.144796363830819\n",
            "Mean Reward in Episode 23: -3.331232663015696\n",
            "Mean Reward in Episode 24: -2.9694021080564355\n",
            "Mean Reward in Episode 25: -3.671058724370137\n",
            "Mean Reward in Episode 26: -2.568753750620039\n",
            "Mean Reward in Episode 27: -1.358535816390323\n",
            "Mean Reward in Episode 28: -4.490036183758243\n",
            "Mean Reward in Episode 29: -1.5135803650316972\n",
            "Mean Reward in Episode 30: -0.9621534406743205\n",
            "Mean Reward in Episode 31: -4.208860109758842\n",
            "Mean Reward in Episode 32: -1.584180196857447\n",
            "Mean Reward in Episode 33: -4.7086603669808\n",
            "Mean Reward in Episode 34: -3.4232945258757637\n",
            "Mean Reward in Episode 35: -2.921039235354833\n",
            "Mean Reward in Episode 36: -1.526828974151968\n",
            "Mean Reward in Episode 37: -3.6057046170830627\n",
            "Mean Reward in Episode 38: -4.385299845945613\n",
            "Mean Reward in Episode 39: -0.5430826921682353\n",
            "Mean Reward in Episode 40: -3.3391084319548554\n",
            "Mean Reward in Episode 41: -4.349417892761302\n",
            "Mean Reward in Episode 42: -3.660037449514208\n",
            "Mean Reward in Episode 43: -1.3255099335685963\n",
            "Mean Reward in Episode 44: -2.1805392801302492\n",
            "Mean Reward in Episode 45: -1.3696501841678546\n",
            "Mean Reward in Episode 46: -3.957794612789938\n",
            "Mean Reward in Episode 47: -3.2880947688745223\n",
            "Mean Reward in Episode 48: -1.204158733393899\n",
            "Mean Reward in Episode 49: -4.402559109369714\n",
            "Mean Reward in Episode 50: -1.1054929611531565\n",
            "Mean Reward in Episode 51: -1.8195576738420958\n",
            "Mean Reward in Episode 52: -0.20429638456091975\n",
            "Mean Reward in Episode 53: -1.1227815927347071\n",
            "Mean Reward in Episode 54: -2.1516904413304534\n",
            "Mean Reward in Episode 55: -4.718090914114451\n",
            "Mean Reward in Episode 56: -3.232530363096914\n",
            "Mean Reward in Episode 57: -1.1758417944529458\n",
            "Mean Reward in Episode 58: -0.8240257754936632\n",
            "Mean Reward in Episode 59: -3.6694783557328297\n",
            "Mean Reward in Episode 60: -1.4170289956339748\n",
            "Mean Reward in Episode 61: 0.07324769202188972\n",
            "Mean Reward in Episode 62: -1.8874718633822076\n",
            "Mean Reward in Episode 63: -0.20839237285010992\n",
            "Mean Reward in Episode 64: -1.6620130501540615\n",
            "Mean Reward in Episode 65: -1.4014983939211723\n",
            "Mean Reward in Episode 66: -4.093357511103281\n",
            "Mean Reward in Episode 67: -1.116372578091184\n",
            "Mean Reward in Episode 68: -1.1286829088053938\n",
            "Mean Reward in Episode 69: -0.9647373722053405\n",
            "Mean Reward in Episode 70: -4.147287501810277\n",
            "Mean Reward in Episode 71: -0.9779198379987025\n",
            "Mean Reward in Episode 72: -4.924506835907665\n",
            "Mean Reward in Episode 73: -4.199892085765926\n",
            "Mean Reward in Episode 74: -2.472685022508614\n",
            "Mean Reward in Episode 75: -0.09009789775774002\n",
            "Mean Reward in Episode 76: -1.53823282703597\n",
            "Mean Reward in Episode 77: -2.5093517100356033\n",
            "Mean Reward in Episode 78: -4.582363286162996\n",
            "Mean Reward in Episode 79: -1.3405810564623029\n",
            "Mean Reward in Episode 80: -1.8667770056184023\n",
            "Mean Reward in Episode 81: -1.3471746131039852\n",
            "Mean Reward in Episode 82: -0.9798333751046603\n",
            "Mean Reward in Episode 83: -1.7330117160153276\n",
            "Mean Reward in Episode 84: -1.468124324539857\n",
            "Mean Reward in Episode 85: -1.72309861968318\n",
            "Mean Reward in Episode 86: -0.9349370585019289\n",
            "Mean Reward in Episode 87: -1.7762590523666248\n",
            "Mean Reward in Episode 88: -1.375619944530368\n",
            "Mean Reward in Episode 89: -4.4338355219844585\n",
            "Mean Reward in Episode 90: -2.4419527895790427\n",
            "Mean Reward in Episode 91: -2.207015854422622\n",
            "Mean Reward in Episode 92: -1.6620932097908208\n",
            "Mean Reward in Episode 93: -1.3123013579262783\n",
            "Mean Reward in Episode 94: -3.1237689781482842\n",
            "Mean Reward in Episode 95: -2.1477149806629297\n",
            "Mean Reward in Episode 96: -1.40816017911821\n",
            "Mean Reward in Episode 97: -2.0555984224760757\n",
            "Mean Reward in Episode 98: -2.5970518665907814\n",
            "Mean Reward in Episode 99: -3.197344271105932\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2452, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3084, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2284, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2423, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2198, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2144, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2399, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2181, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2245, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2177, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2229, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2276, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2358, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2169, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2224, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2123, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3106, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3125, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3046, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2215, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.666064065306846\n",
            "Mean Reward in Episode 1: -1.1097543898505906\n",
            "Mean Reward in Episode 2: -2.8152069856125133\n",
            "Mean Reward in Episode 3: -2.308820514850838\n",
            "Mean Reward in Episode 4: -1.5721975889553412\n",
            "Mean Reward in Episode 5: -1.423255160073305\n",
            "Mean Reward in Episode 6: -1.4639661192988072\n",
            "Mean Reward in Episode 7: -1.0868132279958782\n",
            "Mean Reward in Episode 8: -2.8666211905604717\n",
            "Mean Reward in Episode 9: -1.968086648028352\n",
            "Mean Reward in Episode 10: -1.4876153871250035\n",
            "Mean Reward in Episode 11: -3.12016296726958\n",
            "Mean Reward in Episode 12: -0.9626668156255894\n",
            "Mean Reward in Episode 13: -6.13250252381353\n",
            "Mean Reward in Episode 14: -1.9639213192461962\n",
            "Mean Reward in Episode 15: -3.8003154468875637\n",
            "Mean Reward in Episode 16: -1.040868343345531\n",
            "Mean Reward in Episode 17: -3.694814812498854\n",
            "Mean Reward in Episode 18: -1.1097668719533074\n",
            "Mean Reward in Episode 19: -1.9822139757293034\n",
            "Mean Reward in Episode 20: -2.652953743833826\n",
            "Mean Reward in Episode 21: -1.2820455360800813\n",
            "Mean Reward in Episode 22: -2.042127800189787\n",
            "Mean Reward in Episode 23: -1.326622974158183\n",
            "Mean Reward in Episode 24: -1.6036255298238427\n",
            "Mean Reward in Episode 25: -2.372307097884514\n",
            "Mean Reward in Episode 26: -0.9815058695037352\n",
            "Mean Reward in Episode 27: -1.4507812809203275\n",
            "Mean Reward in Episode 28: -0.799652974527654\n",
            "Mean Reward in Episode 29: -0.7814458539543668\n",
            "Mean Reward in Episode 30: -1.6394481031352872\n",
            "Mean Reward in Episode 31: -1.2038905822279673\n",
            "Mean Reward in Episode 32: -1.3000314665571486\n",
            "Mean Reward in Episode 33: -1.5225205543929963\n",
            "Mean Reward in Episode 34: -2.184013638978872\n",
            "Mean Reward in Episode 35: -1.41409705428776\n",
            "Mean Reward in Episode 36: -1.4913577462998804\n",
            "Mean Reward in Episode 37: 0.5008664497609091\n",
            "Mean Reward in Episode 38: -2.251811203423049\n",
            "Mean Reward in Episode 39: -1.90872615815657\n",
            "Mean Reward in Episode 40: -1.7887942485672965\n",
            "Mean Reward in Episode 41: -1.909173738679354\n",
            "Mean Reward in Episode 42: -2.4348754560603245\n",
            "Mean Reward in Episode 43: -2.298531664425949\n",
            "Mean Reward in Episode 44: -0.9373014610792717\n",
            "Mean Reward in Episode 45: -2.8402106209905202\n",
            "Mean Reward in Episode 46: -0.833789310163066\n",
            "Mean Reward in Episode 47: -1.1456034338094547\n",
            "Mean Reward in Episode 48: -2.3890701726523367\n",
            "Mean Reward in Episode 49: -4.696633741066982\n",
            "Mean Reward in Episode 50: -1.7160050969413498\n",
            "Mean Reward in Episode 51: -3.635658158621782\n",
            "Mean Reward in Episode 52: -2.546892372539392\n",
            "Mean Reward in Episode 53: -1.4408653040368589\n",
            "Mean Reward in Episode 54: -1.445251302652559\n",
            "Mean Reward in Episode 55: -2.832707913590675\n",
            "Mean Reward in Episode 56: -2.5201762536042143\n",
            "Mean Reward in Episode 57: -1.5175340295365587\n",
            "Mean Reward in Episode 58: -3.9198565872637015\n",
            "Mean Reward in Episode 59: -0.9913756834184284\n",
            "Mean Reward in Episode 60: -4.451166586545023\n",
            "Mean Reward in Episode 61: -1.282246106980816\n",
            "Mean Reward in Episode 62: -2.692848587523542\n",
            "Mean Reward in Episode 63: -1.8949723556637417\n",
            "Mean Reward in Episode 64: -1.847099193958134\n",
            "Mean Reward in Episode 65: -6.54422416100484\n",
            "Mean Reward in Episode 66: -1.5165312740050283\n",
            "Mean Reward in Episode 67: -0.8486048159945547\n",
            "Mean Reward in Episode 68: -0.5462667757970201\n",
            "Mean Reward in Episode 69: -1.7753746349653512\n",
            "Mean Reward in Episode 70: -2.9278559757616005\n",
            "Mean Reward in Episode 71: -3.9124787523735227\n",
            "Mean Reward in Episode 72: -3.864019585066538\n",
            "Mean Reward in Episode 73: -3.980563820967182\n",
            "Mean Reward in Episode 74: -1.1455911317112089\n",
            "Mean Reward in Episode 75: -1.9029291865628675\n",
            "Mean Reward in Episode 76: -3.7460120799592684\n",
            "Mean Reward in Episode 77: 0.39792908632422586\n",
            "Mean Reward in Episode 78: -1.9380832867663915\n",
            "Mean Reward in Episode 79: -2.1679205496132234\n",
            "Mean Reward in Episode 80: -4.674438632671819\n",
            "Mean Reward in Episode 81: -1.047378546689084\n",
            "Mean Reward in Episode 82: -0.9745679975864809\n",
            "Mean Reward in Episode 83: -3.895583578950399\n",
            "Mean Reward in Episode 84: -0.9524289909841228\n",
            "Mean Reward in Episode 85: -0.900652760768056\n",
            "Mean Reward in Episode 86: -4.239530360115567\n",
            "Mean Reward in Episode 87: -1.6674889474324157\n",
            "Mean Reward in Episode 88: -2.7641656616548\n",
            "Mean Reward in Episode 89: -2.844934685073865\n",
            "Mean Reward in Episode 90: -1.3191537916718024\n",
            "Mean Reward in Episode 91: -3.471168412271894\n",
            "Mean Reward in Episode 92: -1.591682451855286\n",
            "Mean Reward in Episode 93: -2.3986098403317304\n",
            "Mean Reward in Episode 94: -3.595838296347137\n",
            "Mean Reward in Episode 95: -2.7186163389316524\n",
            "Mean Reward in Episode 96: -5.328430690722656\n",
            "Mean Reward in Episode 97: -2.009495578901378\n",
            "Mean Reward in Episode 98: -3.5100661296494726\n",
            "Mean Reward in Episode 99: -2.6975410682595937\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3202, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2172, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3081, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3042, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3134, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2319, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2290, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2178, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3137, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2113, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3046, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2185, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2081, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2473, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2191, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2434, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2248, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2131, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2404, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2224, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -1.4464611403377567\n",
            "Mean Reward in Episode 1: -1.9503302581047026\n",
            "Mean Reward in Episode 2: -2.492351314057249\n",
            "Mean Reward in Episode 3: -1.3883216599112966\n",
            "Mean Reward in Episode 4: -3.6932001346774843\n",
            "Mean Reward in Episode 5: -2.019231289446629\n",
            "Mean Reward in Episode 6: -2.1267992250611423\n",
            "Mean Reward in Episode 7: -1.2338543863302713\n",
            "Mean Reward in Episode 8: -0.26266245317184594\n",
            "Mean Reward in Episode 9: -3.169523342133358\n",
            "Mean Reward in Episode 10: -4.050126246065916\n",
            "Mean Reward in Episode 11: -1.1064698208158392\n",
            "Mean Reward in Episode 12: -3.633378332651256\n",
            "Mean Reward in Episode 13: -4.027454616665703\n",
            "Mean Reward in Episode 14: -1.39818738392015\n",
            "Mean Reward in Episode 15: -0.9516488404596651\n",
            "Mean Reward in Episode 16: -1.8497323738196196\n",
            "Mean Reward in Episode 17: -0.9546999251100224\n",
            "Mean Reward in Episode 18: -1.9756170009981968\n",
            "Mean Reward in Episode 19: -2.369222983726411\n",
            "Mean Reward in Episode 20: -1.2357809123169472\n",
            "Mean Reward in Episode 21: -1.8762698800736488\n",
            "Mean Reward in Episode 22: -2.421053283513894\n",
            "Mean Reward in Episode 23: -0.7979050788696411\n",
            "Mean Reward in Episode 24: -1.1247965731587495\n",
            "Mean Reward in Episode 25: -1.1591680803828424\n",
            "Mean Reward in Episode 26: -2.596438272179188\n",
            "Mean Reward in Episode 27: -1.325642300103508\n",
            "Mean Reward in Episode 28: -1.8164333028774204\n",
            "Mean Reward in Episode 29: -2.321352219515465\n",
            "Mean Reward in Episode 30: -2.609758526987747\n",
            "Mean Reward in Episode 31: -0.8913298164832959\n",
            "Mean Reward in Episode 32: -3.5187656902777533\n",
            "Mean Reward in Episode 33: -1.3818901142419309\n",
            "Mean Reward in Episode 34: -1.4176363328432446\n",
            "Mean Reward in Episode 35: -1.413760186374854\n",
            "Mean Reward in Episode 36: -1.3240301022994883\n",
            "Mean Reward in Episode 37: -0.4551823274040259\n",
            "Mean Reward in Episode 38: -0.9730230395527655\n",
            "Mean Reward in Episode 39: -1.599842268492093\n",
            "Mean Reward in Episode 40: -1.0448743871822526\n",
            "Mean Reward in Episode 41: -1.0430207864437306\n",
            "Mean Reward in Episode 42: -1.8441494055955716\n",
            "Mean Reward in Episode 43: -1.2675837305120152\n",
            "Mean Reward in Episode 44: -3.66539181262901\n",
            "Mean Reward in Episode 45: -4.192089407467658\n",
            "Mean Reward in Episode 46: -0.7890951457512265\n",
            "Mean Reward in Episode 47: -2.163059058624142\n",
            "Mean Reward in Episode 48: -1.2854233883356845\n",
            "Mean Reward in Episode 49: -3.2047260546219576\n",
            "Mean Reward in Episode 50: -3.494647561605288\n",
            "Mean Reward in Episode 51: -1.9218130423315074\n",
            "Mean Reward in Episode 52: -1.950976458946246\n",
            "Mean Reward in Episode 53: -1.211903460786577\n",
            "Mean Reward in Episode 54: -1.6066463989795687\n",
            "Mean Reward in Episode 55: -3.3973012347720974\n",
            "Mean Reward in Episode 56: -4.319738580811063\n",
            "Mean Reward in Episode 57: -1.4249175738636508\n",
            "Mean Reward in Episode 58: -4.149321329154359\n",
            "Mean Reward in Episode 59: -3.5525888388981257\n",
            "Mean Reward in Episode 60: -3.3000324349765657\n",
            "Mean Reward in Episode 61: -4.671530464409238\n",
            "Mean Reward in Episode 62: -1.8826259175937228\n",
            "Mean Reward in Episode 63: -4.567179808782601\n",
            "Mean Reward in Episode 64: -1.883690724080492\n",
            "Mean Reward in Episode 65: -4.647229053355199\n",
            "Mean Reward in Episode 66: -2.2205098688489957\n",
            "Mean Reward in Episode 67: -4.62358271257126\n",
            "Mean Reward in Episode 68: -0.6092163034975887\n",
            "Mean Reward in Episode 69: -1.7249688822561389\n",
            "Mean Reward in Episode 70: -2.0875045110910277\n",
            "Mean Reward in Episode 71: -1.0610435321763767\n",
            "Mean Reward in Episode 72: -2.9003632295849506\n",
            "Mean Reward in Episode 73: -2.815352355213143\n",
            "Mean Reward in Episode 74: -1.424860471320105\n",
            "Mean Reward in Episode 75: -1.6902475747186252\n",
            "Mean Reward in Episode 76: -1.0713529788870502\n",
            "Mean Reward in Episode 77: -1.6345300097000626\n",
            "Mean Reward in Episode 78: -1.6273164968055365\n",
            "Mean Reward in Episode 79: -1.1421012106666493\n",
            "Mean Reward in Episode 80: -3.8388362434104635\n",
            "Mean Reward in Episode 81: -2.3352068533537467\n",
            "Mean Reward in Episode 82: -1.2364820936294967\n",
            "Mean Reward in Episode 83: -0.9497478481968293\n",
            "Mean Reward in Episode 84: -2.6683015937787697\n",
            "Mean Reward in Episode 85: -0.9943923282263365\n",
            "Mean Reward in Episode 86: -2.181518222021137\n",
            "Mean Reward in Episode 87: -1.5957659279419474\n",
            "Mean Reward in Episode 88: -0.9051561359627416\n",
            "Mean Reward in Episode 89: -1.1285627375131582\n",
            "Mean Reward in Episode 90: -1.5861518181136414\n",
            "Mean Reward in Episode 91: -4.407564603358381\n",
            "Mean Reward in Episode 92: -2.8047866176095266\n",
            "Mean Reward in Episode 93: -3.9815645234928803\n",
            "Mean Reward in Episode 94: -1.081496844875448\n",
            "Mean Reward in Episode 95: -1.5509183909377746\n",
            "Mean Reward in Episode 96: -1.4649613569238022\n",
            "Mean Reward in Episode 97: -2.9682591081824543\n",
            "Mean Reward in Episode 98: -2.17427690040433\n",
            "Mean Reward in Episode 99: -5.8186715247387415\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2292, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2365, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2186, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3072, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3104, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2192, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2179, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2351, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2189, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2206, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3073, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3260, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2162, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3046, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3067, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3042, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3054, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.3222, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2382, grad_fn=<NllLossBackward>)\n",
            "TARGET ACTIOM torch.Size([1])\n",
            "TARGET ACTIOM torch.Size([1, 4])\n",
            "LOSSSSS tensor(-0.2347, grad_fn=<NllLossBackward>)\n",
            "Mean Reward in Episode 0: -3.452599414452935\n",
            "Mean Reward in Episode 1: -1.9464721496697572\n",
            "Mean Reward in Episode 2: -2.387296166469387\n",
            "Mean Reward in Episode 3: -3.1920358188758238\n",
            "Mean Reward in Episode 4: -1.8804726957769526\n",
            "Mean Reward in Episode 5: -1.9566198164045443\n",
            "Mean Reward in Episode 6: -1.1199470771322237\n",
            "Mean Reward in Episode 7: -2.962953218249921\n",
            "Mean Reward in Episode 8: -3.112272571769005\n",
            "Mean Reward in Episode 9: -2.0528519715564237\n",
            "Mean Reward in Episode 10: -1.839446535017382\n",
            "Mean Reward in Episode 11: -0.9598477408461935\n",
            "Mean Reward in Episode 12: -1.3806263549663442\n",
            "Mean Reward in Episode 13: -1.7325409847106479\n",
            "Mean Reward in Episode 14: -5.0566530019691385\n",
            "Mean Reward in Episode 15: -1.5252968496205042\n",
            "Mean Reward in Episode 16: -2.565355813629071\n",
            "Mean Reward in Episode 17: -2.3591753566362232\n",
            "Mean Reward in Episode 18: -1.4894397878249686\n",
            "Mean Reward in Episode 19: -2.8154955270123643\n",
            "Mean Reward in Episode 20: -1.6414409109957715\n",
            "Mean Reward in Episode 21: -4.306503464297051\n",
            "Mean Reward in Episode 22: -2.989379598563164\n",
            "Mean Reward in Episode 23: -1.2061773217151275\n",
            "Mean Reward in Episode 24: -1.894126973321295\n",
            "Mean Reward in Episode 25: -3.108546181048327\n",
            "Mean Reward in Episode 26: -1.7102687668834042\n",
            "Mean Reward in Episode 27: -1.6579092424231343\n",
            "Mean Reward in Episode 28: -1.6304483044580749\n",
            "Mean Reward in Episode 29: -2.116815312639508\n",
            "Mean Reward in Episode 30: -1.8062386711622225\n",
            "Mean Reward in Episode 31: -2.9354500742284464\n",
            "Mean Reward in Episode 32: -1.3556898635499195\n",
            "Mean Reward in Episode 33: -1.9874931162071854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-2caf020273c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mstate_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmean_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-657be76e5e0d>\u001b[0m in \u001b[0;36mplay_episodes\u001b[0;34m(max_steps, model)\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mstate_action_per_timestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mreward_per_timestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    300\u001b[0m                                            True)\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_UZ6mClvPzX",
        "outputId": "7fdab21f-d69d-4b3a-b9b2-b7b2a2a82563"
      },
      "source": [
        "test = []\r\n",
        "test.append(([0,2,3,],1))\r\n",
        "\r\n",
        "np.array(test)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[list([0, 2, 3]), 1]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47-QTbaQvUxs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}